---
output: pdf_document
#toc: yes
number_sections: yes
fig_caption: true
lang: es
editor_options: 
chunk_output_type: console
fontsize: 12pt

title: 
- \large Maestría en Estadística Aplicada

subtitle: "Tesis: Evaluación de métodos de pronóstico del precio de cierre diario y volatilidad de  Ether (ETH/USD)."

documentclass: report
classoption: a4paper
linestretch: 1.5
geometry: "left=2cm,right=1.5cm,top=2cm,bottom=2cm"
base_format: rticles::jss_article
always_allow_html: true
header-includes:



- \usepackage{titling} 




- \pretitle{\begin{center}
    \includegraphics[width=2 in, height=2 in]{logounr.png}\\
    
    \Large UNIVERSIDAD NACIONAL DE ROSARIO\\}









- \posttitle{\small Maestrando\protect{:} Aldo Sebastián Bellittieri\\
               Director\protect{:} Dr. Martín Masci\\  
               Co Directora\protect{:} Mag. Fernanda Mendez\end{center}}  
  
- \usepackage{caption}


- \usepackage{float}
- \floatplacement{figure}{H}
- \usepackage{subfig}
- \usepackage{fancyhdr}
- \pagestyle{plain}
- \usepackage{pdflscape}
- \usepackage{cancel}

- \captionsetup[figure]{name=Figura} 
- \captionsetup[table]{name=Tabla}
- \captionsetup{font=normalsize,width=.75\textwidth}


---




```{r setup, include=FALSE}
library(ggplot2)
library(MASS)
library(dplyr)
library(ggrepel)
library(readr)
library(tidyr)
library(here)
library(cowplot)
library(purrr)
library(GGally)
library(ggridges)
library(nortest)
library(digest)
library(gridExtra)
library(knitr)
library(tinytex)
library(rmarkdown)
library(robust)
library(broom)
library(tinytex)
library(pander)
library(gganimate)
library(ggforce)
library(rgl)
library(egg)
library(plotly)
library(farver)
library(tweenr)
library(sampling)
library(readxl)
library(xtable)
library(tseries)
  library(forecast)
  library(ggplot2)
  library(gridExtra)
 library(PASWR2)
  library(pastecs)
  library(psych)
 library(dplyr)
  library(astsa)
  library(tseries)
  library(xts)
  library(tidyverse)
  library(magrittr)
  library(ggfortify)
  library(forecast)
  library(dygraphs)
  library(fpp)   ## ausbeer
  library(fpp2)  ## goog200
  library(lessR)
  library(ggplot2)
  library(forecast)
  library(astsa)
  library(lmtest)
  library(fUnitRoots)
  library(FitARMA)
  library(strucchange)
  library(reshape)
  library(Rmisc)
  library(fBasics)
  library(vars)
  library(FitAR)
 library(keras)
  library(tensorflow)
  library(reticulate)
  library(quantmod)
  library(timetk)
  library(Quandl)
library(rmgarch)
library(fGarch)
library(rugarch)
library(skimr)
library(kableExtra)
library(fpp3)
library(Metrics)
library(RColorBrewer)
library(patchwork)
library(tidymodels)
library(modeltime)
library(modeltime.resample)
library(recipes)
library(rsample)
library(parsnip)
library(workflows)
library(future)
library(TSLSTM)
#library(glue)
#library(tidyquant)
#library(tibbletime)
#library(yardstick)
#library(keras)

```

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
knitr::opts_chunk$set(echo = FALSE)
```

\newpage


```{r,echo=F,inclu=F}

## FUNCIÃN PARA CONSTRUIR UNA MATRIZ CON LOS VALORES DE AIC
AIC_Matrix <- function(ts,p.order, q.order){
  require(forecast)
  aic_matrix = matrix(data = NA, nrow = p.order + 1, ncol = q.order + 1)
  max_i = p.order
  max_j = q.order
  for(i in 0:max_i){
    for(j in 0:max_j){
      aic = arima(ts,order = c(i,0,j))$aic
      #print(aic)
      a = i+1
      b = j+1
      aic_matrix[a,b] = aic
    }
  }
  rownames(aic_matrix) = c(0:max_i)
  colnames(aic_matrix) = c(0:max_j)
  return(aic_matrix)
}

## CREAMOS UNA FUNCI?N PARA TESTEAR NORMALIDAD ##
Normality_Test <- function(ts,type = c("JB", "AD", "SW")){
  require(tseries)
  require(nortest)
  if(type == "JB"){
    p_val = jarque.bera.test(ts)$p.value
    stat  = jarque.bera.test(ts)$statistic
  } else if(type == "AD"){
    p_val = ad.test(ts)$p.value
    stat  = ad.test(ts)$statistic
  } else {
    p_val = shapiro.test(ts)$p.value
    stat  = shapiro.test(ts)$statistic
  }
  
  table = data.frame(P_Value = p_val,
                     Statistic = stat)
  return(table)
}

## LO FORMALIZAMOS EN UNA FUNCI?N ##
Incorrelation <- function(ts, type = c("Ljung-Box","Box-Pierce"), fitdf = 0){
  p_ljung_box = NULL
  s_ljung_box = NULL
  for(i in 0:(length(ts)/4)){
    p_ljung_box[i] = Box.test(ts,lag = i,type = type,fitdf = fitdf)$p.value
    s_ljung_box[i] = Box.test(ts,lag = i,type = type,fitdf = fitdf)$statistic
  }
  table = data.frame(j = 1:(length(ts)/4),
                     P_Value = p_ljung_box,
                     Statistic = s_ljung_box)
  return(table)
}


## LE INCORPORAMOS UN IF ##
Incorrelation2 <- function(ts, type = c("Ljung-Box","Box-Pierce"), fitdf = 0){
  if(is.ts(ts)){
    p_ljung_box = NULL
    s_ljung_box = NULL
    for(i in 0:(length(ts)/4)){
      p_ljung_box[i] = Box.test(ts,lag = i,type = type,fitdf = fitdf)$p.value
      s_ljung_box[i] = Box.test(ts,lag = i,type = type,fitdf = fitdf)$statistic
    }
    table = data.frame(id = 1:(length(ts)/4),
                       P_Value = p_ljung_box,
                       Statistic = s_ljung_box)
    return(table)  
  } else {
    print("El objeto no es una serie de tiempo")  
  }
}
## FUNCIÃN PARA CONSTRUIR UNA MATRIZ CON LOS VALORES DE AIC
BIC_Matrix <- function(ts,p.order, q.order){
  require(forecast)
  bic_matrix = matrix(data = NA, nrow = p.order + 1, ncol = q.order + 1)
  max_i = p.order
  max_j = q.order
  for(i in 0:max_i){
    for(j in 0:max_j){
      bic = BIC(arima(ts,order = c(i,0,j)))
      #print(bic)
      a = i+1
      b = j+1
      bic_matrix[a,b] = bic
    }
  }
  rownames(bic_matrix) = c(0:max_i)
  colnames(bic_matrix) = c(0:max_j)
  return(bic_matrix)
}
lag_transform <- function(x, k = 1){
  lagged = c(rep(NA, k), x[1:(length(x)-k)])
  DF = as.data.frame(cbind(lagged, x))
  colnames(DF) <- c( paste0('x-', k), 'x')
  DF[is.na(DF)] <- 0
  return(DF)
}


scale_data = function(train, test, feature_range = c(0, 1)) {
  
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
}

invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

#Function to reverse scale data for prediction
reverse_scaling <- function(scaled, scaler, feature_range = c(0,1)) {
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for(i in 1:t) {
    X = (scaled[i] - mins) / (maxs - mins)
    rawValues = X * (max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

#Función Calibrate and Plot:

#' Helper function - Calibrate and Plot
#'
#' @author Steven P. Sanderson II, MPH
#'
#' @family Utility
#'
#' @description
#' This function is a helper function. It will take in a set of workflows and then
#' perform the [modeltime::modeltime_calibrate()] and [modeltime::plot_modeltime_forecast()].
#'
#' @details This function expects to take in workflows fitted with training data.
#'
#' @param ... The workflow(s) you want to add to the function.
#' @param .type Either the training(splits) or testing(splits) data.
#' @param .data The full data set.
#' @param .splits_obj The splits object.
#' @param .print_info The default is TRUE and will print out the calibration
#' accuracy tibble and the resulting plotly plot.
#' @param .interactive The defaults is FALSE. This controls if a forecast plot
#' is interactive or not via plotly.
#'
#' @examples
#' \dontrun{
#' suppressPackageStartupMessages(library(timetk))
#' suppressPackageStartupMessages(library(dplyr))
#' suppressPackageStartupMessages(library(recipes))
#' suppressPackageStartupMessages(library(rsample))
#' suppressPackageStartupMessages(library(parsnip))
#' suppressPackageStartupMessages(library(workflows))
#'
#' data <- ts_to_tbl(AirPassengers) %>%
#'   select(-index)
#'
#' splits <- timetk::time_series_split(
#'    data
#'   , date_col
#'   , assess = 12
#'   , skip = 3
#'   , cumulative = TRUE
#' )
#'
#' rec_obj <- recipe(value ~ ., data = training(splits))
#'
#' model_spec <- linear_reg(
#'    mode = "regression"
#'    , penalty = 0.1
#'    , mixture = 0.5
#' ) %>%
#'    set_engine("lm")
#'
#' wflw <- workflow() %>%
#'    add_recipe(rec_obj) %>%
#'    add_model(model_spec) %>%
#'    fit(training(splits))
#'
#' output <- calibrate_and_plot(
#'   wflw
#'   , .type = "training"
#'   , .splits_obj = splits
#'   , .data = data
#'   , .print_info = FALSE
#'   , .interactive = FALSE
#'  )
#' }
#'
#' @return
#' The original time series, the simulated values and a some plots
#'
#' @export calibrate_and_plot
#'
# *** PLOTTING UTILITY *** ----
# - Calibrate & Plot
calibrate_and_plot <- function(..., .type = "testing", .splits_obj
                               , .data, .print_info = TRUE
                               , .interactive = FALSE){

    # Tidyeval ----
    splits_obj <- .splits_obj

    # Checks ----
    if(.type == "testing"){
        new_data = rsample::testing(splits_obj)
    } else {
        new_data = rsample::training(splits_obj) %>%
            tidyr::drop_na()
    }

    if(!is.data.frame(.data)){
        stop(call. = FALSE, "(.data) is missing or is not a data.frame/tibble, please supply.")
    }

    if(!class(splits_obj)[[1]] == "ts_cv_split") {
        if(!class(splits_obj)[[2]] == "rsplit") {
            stop(call. = FALSE, ("(.splits) is missing or is not an rsplit or ts_cv_split. Please supply."))
        }
        stop(call. = FALSE, ("(.splits) is missing or is not a rsplit or ts_cv_split. Please supply."))
    }

    # Data
    data <- .data

    # Calibration Tibble
    calibration_tbl <- modeltime::modeltime_table(...) %>%
        modeltime::modeltime_calibrate(new_data)

    model_accuracy_tbl <- calibration_tbl %>%
        modeltime::modeltime_accuracy()

    plt <- calibration_tbl %>%
        modeltime::modeltime_forecast(
            new_data = new_data
            , actual_data = data
        ) %>%
        modeltime::plot_modeltime_forecast(
            .conf_interval_show = TRUE
            , .interactive = .interactive
        )

    output <- list(
        calibration_tbl = calibration_tbl,
        model_accuracy  = model_accuracy_tbl,
        plot            = plt
    )

    # Should we print?
    if(.print_info){
        print(model_accuracy_tbl)
        plt
    }

    return(output)
}

#### LSTM #####

predict_keras_lstm <- function(split, epochs = 300, ...) {
    
    lstm_prediction <- function(split, epochs, ...) {
        
        # 5.1.2 Data Setup
        df_trn <- training(split)
        df_tst <- testing(split)
        
        df <- bind_rows(
            df_trn %>% add_column(key = "training"),
            df_tst %>% add_column(key = "testing")) %>% 
            dplyr::rename(index=date)
        
        # 5.1.3 Preprocessing
        rec_obj <- recipe(value ~ ., df) %>%
          step_center(value) %>%   
          step_scale(value) %>%
            prep()
        
        
        df_processed_tbl <- bake(rec_obj, df)
        
        center_history <- rec_obj$steps[[1]]$means["value"]
        scale_history  <- rec_obj$steps[[2]]$sds["value"]
        
        # 5.1.4 LSTM Plan
        lag_setting  <- 30 # = nrow(df_tst)
        batch_size   <- 5
        train_length <- 300
        tsteps       <- 1
        epochs       <- epochs
        
        # 5.1.5 Train/Test Setup
        lag_train_tbl <- df_processed_tbl %>%
            mutate(value_lag = lag(value, n = lag_setting)) %>%
            filter(!is.na(value_lag)) %>%
            filter(key == "training") %>%
            tail(train_length)
        
        x_train_vec <- lag_train_tbl$value_lag
        x_train_arr <- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))
        
        y_train_vec <- lag_train_tbl$value
        y_train_arr <- array(data = y_train_vec, dim = c(length(y_train_vec), 1))
        
        lag_test_tbl <- df_processed_tbl %>%
            mutate(
                value_lag = lag(value, n = lag_setting)
            ) %>%
            filter(!is.na(value_lag)) %>%
            filter(key == "testing")
        
        x_test_vec <- lag_test_tbl$value_lag
        x_test_arr <- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))
        
        y_test_vec <- lag_test_tbl$value
        y_test_arr <- array(data = y_test_vec, dim = c(length(y_test_vec), 1))
                
        # 5.1.6 LSTM Model
        model <- keras_model_sequential()

        model %>%
            layer_lstm(units            = 50, 
                       input_shape      = c(tsteps, 1), 
                       batch_size       = batch_size,
                       return_sequences = TRUE, 
                       stateful         = TRUE) %>% 
            layer_lstm(units            = 50, 
                       return_sequences = FALSE, 
                       stateful         = TRUE) %>% 
            layer_dense(units = 1)
        
        model %>% 
            compile(loss = 'mae', optimizer = 'adam')
        
        # 5.1.7 Fitting LSTM
        for (i in 1:epochs) {
            model %>% fit(x          = x_train_arr, 
                          y          = y_train_arr, 
                          batch_size = batch_size,
                          epochs     = 1, 
                          verbose    = 1, 
                          shuffle    = FALSE)
            
            model %>% reset_states()
            cat("Epoch: ", i)
            
        }
        
       # 5.1.8 Predict and Return Tidy Data
        # Make Predictions
      pred_out <- model %>% 
          predict(x_test_arr, batch_size = batch_size) %>%
            .[,1] 

        # Retransform values
      pred_tbl <- tibble(
        index   = lag_test_tbl$index,
        value   = (pred_out * scale_history + center_history)
    ) 

      # Combine actual data with predictions
      tbl_1 <- df_trn %>%
          add_column(key = "actual")

      tbl_2 <- df_tst %>%
          add_column(key = "actual")

      tbl_3 <- pred_tbl %>%
          add_column(key = "predict")

      # Create time_bind_rows() to solve dplyr issue
      time_bind_rows <- function(data_1, data_2, index) {
            index_expr <- enquo(index)
            dplyr::bind_rows(data_1, data_2) %>%
               tibble(index = !! index_expr)
        }

            ret <- list(tbl_1, tbl_2, tbl_3) %>%
                purrr::reduce(time_bind_rows, index = index) %>%
                dplyr::arrange(key, index) %>%
                dplyr::mutate(key = as_factor(key))

        return(ret)
        
    }
    
    safe_lstm <- possibly(lstm_prediction, otherwise = NA)
    
    safe_lstm(split, epochs, ...)
    
}


```

```{r, include=F,eval=F}

library(fpp3)# recordar esta librería!!!
sunspots <- sunspot.year %>%  tsibble::as_tsibble()
fit <- sunspots %>% 
  model(NNETAR(sqrt(value)))
fit %>% 
  forecast(h = 30) %>% 
  autoplot(sunspots) +
  labs(x = "Year", y = "Counts", title = "Yearly sunspots")
```



\pagenumbering{roman}


\newpage

## Resumen: 

En el presente trabajo de tesis de Maestría en Estadística Aplicada se aplican métodos de pronóstico de series temporales a la serie de precios históricos de cierre de la criptomoneda Ether en dólares estadounidenses (en adelante ETH/USD), en el período de tiempo definido entre 01/01/2017 y 30/12/2021, generando pronósticos para 5, 15 y 30 días del mes de enero de 2022. Se trabaja con métodos paramétricos de suavizado exponencial (ETS), autorregresivos integrados de medias móviles (ARIMA), autorregresivos fraccionalmente integrados de medias móviles (ARFIMA), y con métodos no paramétricos de redes neuronales autorregresivas de alimentación hacia adelante (NNETAR) y redes neuronales recurrentes de memoria de corto y largo plazo (LSTM), comparando las métricas de raíz cuadrada del error cuadrático medio (RMSE), error absoluto medio (MAE) y  error porcentual absoluto medio (MAPE). Se utiliza un esquema de validación cruzada aplicado a series de tiempo para la calibración de los modelos. Se observa leptocurtosis en los residuos de los modelos paramétricos. Los resultados muestran comportamiento heterogéneo, donde en el horizonte de 5 días y 15 días se generan pronósticos con mejores métricas con los modelos ETS y ARFIMA respectivamente y para el plazo de 30 días los modelos no paramétricos obtienen mejor perfomance.  

Se estudia además la volatilidad de la serie de retornos logarítmicos del precio de ETH/USD y se generan pronósticos fuera de la muestra a un día con una muestra de tamaño 300 utilizando los modelos autorregresivos de varianza condicional heterocedástica (ARCH), generalizados autorregresivos de varianza condicional heterocedástica (GARCH) y exponenciales generalizados autorregresivos de varianza condicional heterocedástica (EGARCH). Para la comparación de modelos se utilizan las métricas RMSE, MAE y MAPE, el test basado en regresión de Mincer-Zarnowitz y el test de Diebold-Mariano. Se seleccionan los modelos considerando distintas distribuciones de densidad condicional para las innovaciones en los modelos propuestos. Se presenta leptocurtosis en los residuos de los modelos. Los resultados para comparación de modelos de volatilidad presentan la dificultad de la función de aproximación de la volatilidad real y las métricas de error deben ser consideradas con cautela. No se detectan resultados significativos en la comparación por test de Diebold-Mariano, tampoco en el test de regresión de Mincer-Zarnowitz. Los criterios de información indican el modelo ARCH(4) para la volatilidad de la serie mientras que en base a las métricas se selecciona el GARCH(1,1).

**Palabras clave**: Ether, series de tiempo, pronósticos. 

\newpage


## Abstract:

In the present Master's in Applied Statistics thesis, time series forecasting methods are used in a series of historical closing prices of the cryptocurrency Ether (ETH/USD), during the period of time between 01/01/2017 and 30/12/2021, generating forecasts for 5, 15 and 30 days of the month of January 2022. It is worked with exponential smoothing (ETS), autoregresive integrated moving averages (ARIMA) and autoregresive fractional integrated moving averages (ARFIMA), parametric methods and with feed-forward neural networks with a single hidden layer and lagged inputs (NNETAR) and long short time memory recurrent neural networks (LSTM) non-parametric methods, comparing the root mean squared error (RMSE), mean absolute error (MAE) and mean absolute percentage error (MAPE) metrics. It is used a cross-validation scheme applied to time series for the calibration of the models. Leptokurtosis is observed in the residuals of the parametric models. The results show heterogeneous behavior, where in the 5-day and 15-day horizon forecasts with better metrics are generated with the ETS and ARFIMA models respectively and for the 30-day term the non-parametric models obtain better performance.

The volatility of the series of logarithmic returns of the ETH/USD price is also studied and one-day out-of-sample forecasts are generated with a sample of size 300 using the autoregresive conditional heteroskedasticity (ARCH),  generalized autoregressive conditional heteroskedasticity (GARCH) and exponential  generalized autoregressive conditional heteroskedasticity (EGARCH). The RMSE, MAE and MAPE metrics, the Mincer-Zarnowitz regression-based test, and the Diebold-Mariano test are used to compare models. The models are selected considering different distributions of conditional density for the innovations in the proposed models. Leptokurtosis is present in the residuals of the models. The results for the comparison of volatility models present the difficulty of the approximation function of the real volatility and the error metrics must be considered with caution. No significant results were detected in the comparison by the Diebold-Mariano test, nor in the Mincer-Zarnowitz regression test. The information criteria indicate the ARCH(4) model for the volatility of the series, while the GARCH(1,1) model is selected based on the metrics.

**Key words**: Ether, time series, forecasts.

\newpage

## Agradecimientos:

Quiero mencionar mi especial agradecimiento a mi familia, a mi esposa Natalia y a mi hija Nina por impulsarme a terminar este trabajo.

A mis compañeros y compañeras de la carrera por permitirme compartir, aprender y trabajar en distintos grupos de manera totalmente enriquecedora. 

A todos los docentes de la carrera. Tanto a nivel humano como profesional he tenido una maravillosa experiencia durante el cursado y ampliamente recomendaría la Maestría.

A mi Director de tesis y Co Directora por la guía a través del proceso de confección del trabajo final.


\newpage


\newpage
\tableofcontents
\addcontentsline{toc}{section}{Índice de General}


\listoffigures
\addcontentsline{toc}{section}{Índice de Figuras}

\listoftables
<!-- \addcontentsline{toc}{section}{Índice de Cuadros} -->
\addcontentsline{toc}{section}{Índice de Tablas}






\newpage

\newpage
\pagenumbering{arabic}
\setcounter{page}{1}
\pagestyle{plain}



# 1-Introducción 

En la actualidad se debate con frecuencia sobre quién es el sostén de confianza de las monedas fiduciarias dando como respuesta a los roles gubernamentales y de los bancos centrales de los diferentes países. En la época del respaldo monetario en oro era el sistema Bretton Woods a escala global, en la actualidad varias instituciones influencian la dinámica del competitivo mundo financiero como el Fondo Monetario Internacional, el Banco Mundial, el Banco Central Europeo, El Banco Asiático de inversión en Infraestructura y otros dependiendo en la región y el punto de vista (Pichl et al., 2020). Sobre la confianza en el dinero fiduciario (Desai y Said, 2004) mencionan a las crisis cambiarias de Checoslovaquia de 1953 y la de Rusia en 1993 como generadoras de destrucción  del valor de los ahorros de la población general, con una combinación de restricciones cambiarias y alta tasa de inflación. En igual situación la economía argentina con las sucesivas crisis cambiarias y pérdida de confianza en la moneda nacional (Padinger, 2022).


El dinero fiduciario es la moneda que utilizada en las transacciones comerciales gracias a la confianza de pago de la entidad emisora ya que, al carecer valor intrínseco, el emisor debe liquidarla con los bienes de valor que la respaldan (R.A.E., 2024).
Cuando un sistema de dinero fiduciario se vuelve completamente digital, cosa que no ha ocurrido aún, la institución central gana la capacidad de intervenir a cada individuo del sistema financiero y la información sobre su comportamiento debido al flujo de información que las transacciones involucran. A medida que se sofistican los sistemas digitales de finanzas, los riesgos asociados con el mal funcionamiento de los mismos aumentan. "La relación asimétrica de un individuo y la institución financiera puede ser conveniente cuando existe la protección de la privacidad y las medidas de seguridad funcionen correctamente pero puede ser un desastre en casos de ciberataques exitosos” (Pich et al., 2020).
Una definición de activo digital es “instrumento digital emitido o representado mediante el uso de un registro distribuido o una tecnología similar. Se excluyen las representaciones digitales de monedas fiduciarias” (Fondo Monetario Internacional [FMI], 2022).

En relación al párrafo anterior, la separación conceptual sobre el dinero fiduciario digital y los activos digitales, continuando con los conceptos elementales relacionados con las criptomonedas, FMI define a la cadena de bloques como “un registro distribuido en el que los detalles de las transacciones se guardan en bloques de información. Un bloque nuevo se anexa a una cadena de bloques existente mediante un proceso computarizado que valida las transacciones” (FMI, 2022). Una criptomoneda es un activo digital del sector privado que depende primordialmente de la criptografía y de un registro distribuido (cadena de bloques) o tecnología similar (FMI, 2022).

En relación a los pros y contras de las criptomonedas, Pich et al (2020) mencionan el trabajo de Nakamoto (2008), el creador de Bitcoin (en adelante BTC), que resuelve el problema del doble gasto del dinero digital (cómo garantizar que el dinero digital no es gastado varias veces publicando el evento de transacciones de manera encriptada en la cadena de bloques). El incentivo para mantener la integridad de la cadena de bloques es dado por la recompensa al creador de un nuevo bloque con un monto fijo de BTC (proceso de minado) o por comisiones en las transacciones. A medida que más diversificado está el sistema más robusto se vuelve al ataque que intente alterar los registros de la cadena de bloques (Nakamoto, 2008). Las características distintivas de BTC como criptomoneda son la falta de una entidad financiera regulatoria, la cantidad fija de monedas y la disponibilidad de un completo registro de transacciones entre direcciones. Todas las criptomonedas tienen la propiedad de no tener un valor específicamente económico a diferencia del dinero fiduciario, la falta de confianza en el sistema financiero de monedas fiduciarias no necesariamente implica confiar en un activo digital. Las fluctuaciones de precio de los valores de BTC en el tiempo y de otras varias criptomonedas confirman el hecho de que el valor intrínseco de las criptomonedas no está en los bienes y servicios de la economía. Las criptomonedas también enfrentan un problema ambiental debido a la enorme cantidad de energía que requieren para los cálculos que inducen a una gran huella de carbono. Al mes de septiembre de 2020, existen más de 7100 criptomonedas diferentes según la plataforma Coinmarketcap.com. Consultado el 20/09/2020. (https://coinmarketcap.com/)

Ethereum es una plataforma de fuente abierta para contratos inteligentes basada en la plataforma de cadena de bloques. Tiene la ventaja de que puede ser aplicada a lo que se denomina contratos inteligentes como la verificación de votos en elecciones, la verificación de documentos, el combate a la piratería digital. Ethereum corre en aplicaciones que no tienen posibilidades de falta de tiempo, censura, fraude o interferencia de terceras partes. Es una plataforma blockchain con funcionalidad de contratos inteligentes creada en el año 2013 por Vitalik Buterin. Las aplicaciones corren de manera descentralizada entre una red de diferentes computadoras sumando el potencial de todas y pagadas por Ether (la criptomoneda asociada a la plataforma Ethereum). Una de las aplicaciones más populares de Ethereum como plataforma incluye la creación de tókenes transables que se pueden usar como monedas (Ether), donde Ether es un activo digital en el tope de la cadena de bloques de Ethereum (Zhang et.al, 2019). Al momento del presente trabajo Ether es la segunda criptomoneda con mayor capitalización de mercado. 

El proyecto Ethereum trajo innovaciones sustanciales a las criptomonedas como el de la habilidad de script completo de Turing en 15 segundos, comparado con BTC que tarda 10 minutos en este proceso, la creación de contratos para mantener la participación de cada miembro de un grupo de inversores grupales o la creación de lugares de votación seguros. Uno de las ventajas más importantes de Ethereum es que puede ayudar a empresas con la transformación hacía el internet de las cosas. Dado que la red no tiene falta de tiempo es extremadamente útil para aplicaciones básicas que tienen que estar continuamente funcionando como por ejemplo termostatos o sistemas de iluminación (Zhang et al., 2019). 

El proyecto de la presente tesis se centra en la aplicación de métodos de predicción del precio y la volatilidad de la criptomoneda Ether en relación al dólar estadounidense (ETH-USD), una de las aplicaciones de la plataforma Ethereum. No se considerarán las otras aplicaciones de Ethereum en lo referido al desarrollo de soluciones informáticas más allá de esta breve mención.

La serie de valores de cierre diario de ETH-USD (código definido para el precio de la  criptomoneda en dólares) presenta gran variabilidad a lo largo del tiempo. Se define a las burbujas como una desviación persistente del precio de un activo económico de sus valores fundamentales (Diba y Grossman, 1988). Se referencia a las burbujas en el precio para dar una referencia de la variabilidad existente en los precios de las criptomonedas.

Kroll et al.,(2013) proveen una descripción detallada del proceso de minado de criptomonedas. Los mineros agregan transacciones verificadas a un libro de contabilidad distribuido públicamente, o cadena de bloques, y son incentivados a hacerlo por la recompensa de tarifas de transacción y nuevas criptomonedas. El proceso de minado de criptomonedas no está en los alcances de este trabajo más allá de esta breve mención.

Corbet et al.,(2017) analizan los fundamentales del precio tanto de Bitcoin como de Ethereum, los precios y las fechas de potenciales burbujas de precio de las dos criptomonedas. Generan ratios que son económica y computacionalmente sensibles para detección de burbujas en los precios. Seleccionan 3 variables que representan los componentes clave en la estructura de precios de las criptomonedas: la primer medida se relaciona con la dificultad de minar en relación a la dificultad que se tenía en el pasado, la segunda medida se relaciona con la tasa de hash, que es la velocidad a la que una computadora completa una operación en el código Bitcoin. Una alta tasa de hash en el proceso de minado aumenta la oportunidad de encontrar el próximo bloque y de recibir pagos. En tercer lugar la relación entre los retornos de las criptomonedas, volatilidad y liquidez establecida por Donier y Bouchaud (2017).


La aleatoriedad y los tests de aleatoriedad (Ljung Box-Bartels) son utilizados porque en mercados eficientes los precios tienen un comportamiento de paseo aleatorio. Urquhart et al. (2015) encontraron evidencia para sugerir que los retornos de BTC son guiados por los compradores y vendedores internamente y no por factores económicos. Utilizando ratios sin tendencia los autores determinaron que BTC tiene retornos 26 veces más volátiles que el índice Standard and Poor (S&P), lo que sugiere que BTC es un vehículo de inversión especulativo. Cheung et al. (2015) utilizaron una investigación econométrica utilizando la metodología de Phillips et. al.(2015) que resulta robusta en la detección de burbujas. Se analizó en este caso, para ETH-USD, 826 observaciones desde el 30/6/2015 al 9/11/2017. En la metodología utilizan la técnica de Phillips et. al. (2015) que se basa en un proceso recursivo ex ante que detecta señales de burbuja de manera preventiva con procedimientos de testeo basados en análisis de raíces unitarias de Dickey Fuller aumentado (ADF) para testear la presencia de una burbuja con la inclusión de una secuencia de tests recursivos de raíces unitarias (de cola derecha). Se presenta luego una estrategia de fechas basada en la técnica de regresión hacía atrás para encontrar los puntos de inicio y final de la burbuja. Cuando se suceden gran cantidad de eventos en una serie de tiempo se utiliza el método del supremo generalizado de ADF (GSADF) para chequear la presencia de burbujas y la regresión recursiva para la identificación de las fechas de origen y final de las mismas. 

El trabajo de Corbet et al. (2017) indica que en el caso de Ethereum se dieron burbujas en el período de inicio del 2016 y la mitad de 2017 pero no con una marcada explosividad y concluye que no se detectan evidencias suficientes de que la serie de precios de Ethereum o BTC presente comportamiento de burbuja. Dados los vínculos teóricos entre los precios ETH y BTC con la posición de la cadena de bloques, la tasa de hash y la liquidez respectivamente, los autores afirman que en ciertos períodos cortos de tiempo cada fundamental influencia la dinámica del precio de las criptomonedas, pero éstos efectos desaparecen rápidamente. Se menciona el trabajo de Corbet et al. (2017) respecto a las burbujas en el precio para dar una referencia de la volatilidad existente en los precios de las criptomonedas. No se alcanza en el presente trabajo el análisis de los fundamentales del precio de Ethereum.

Dada la volatilidad en el nivel de los precios de BTC y Ether, Mensi et. al. (2018) plantean modelos autorregresivos condicionales heterocedásticos,  con modelado de la presencia de quiebres estructurales y larga memoria para el pronóstico de la serie de retornos de precios. Urquhart (2016) estudia la ineficiencia de los mercados con un conjunto de tests (Ljung–Box test, Runs, Bartels, AVR, BDS, and R/S Hurst) para probar la ineficiencia del mercado en el caso de BTC. Gregoriu (2015) examina la literatura de uso de pruebas de raíz unitaria para detectar formas débiles de eficiencia del mercado en los mercados financieros, menciona el bajo poder para rechazar la raíz unitaria nula en presencia de heteroscedasticidad, particularmente problemática en las finanzas de alta frecuencia de datos.

La hipótesis de mercados eficientes es importante para predecir si los inversores pueden generar ganancias a través del intercambio de acciones o criptomonedas en este caso. Si los mercados son eficientes, los precios reflejan toda la información presente en el mercado y por lo tanto no tiene sentido generar ganancias extraordinarias haciendo análisis técnicos o fundamentales en este tipo de mercados (Fama, 1970, 1991). Si los mercados no son eficientes existe el potencial para que se generen predicciones basados en cambios históricos de precio. La hipótesis de mercados eficientes también tiene implicancia en la asignación de recursos. Si los mercados son eficientes permite que el inversor asigne sus ahorros a inversión de manera eficaz, y, en caso contrario los precios no reflejan toda la información disponible en el mercado y la asignación eficiente de recursos se vuelve cuestionable.

Para testear la hipótesis de eficiencia de mercados se utilizan los tests de raíces unitarias. Si los precios del activo presentan raíces unitarias o exhiben comportamiento de paseo aleatorio se presenta una situación en la que los precios escapan a un equilibrio a largo plazo (serie no estacionaria) y por lo tanto no es posible predecir con precisión el precio futuro de la serie en base a información de precios pasados. Pero sin embargo, si la serie es estacionaria se produce una reversión a la media a largo plazo haciendo posibles los pronósticos. Gregoriu (2015) menciona que en los estudios originales de hipótesis de eficiencia de mercados se utilizan tests de raíces unitarias sin quiebres estructurales y, presentan el problema de que al aparecer en la serie quiebres estructurales, los tests convencionales tienen baja potencia para rechazar la presencia de raíces unitarias. Aplica test de raíces unitarias GARCH. La metodología comienza estableciendo un punto de comparación con el test de Dickey Fuller Aumentado y el de Phillips Perron (Kwiatkowski test) que son tests que no tienen elevada potencia para rechazar la hipótesis nula de raíz unitaria en presencia de uno o más quiebres estructurales. Gregoriu (2015) aplica el test de multiplicadores de Lagrange de raíces unitarias con quiebres estructurales. Narayán y Liu (2013) en el análisis de un conjunto de activos financieros relajan el supuesto de errores independientes e idénticamente distribuidos y proponen un modelo GARCH (1,1) de raíz unitaria que se acomoda a dos quiebres estructurales en el intercepto en presencia de errores heterocedásticos.

Se incluye en el análisis descriptivo la evaluación de existencia de quiebres estructurales y de larga memoria junto con tests para determinar la ineficiencia de mercado en términos de medias y varianzas en la serie histórica de precios de Ether en la presente tesis.

Por otro lado, se modela la serie con técnicas autorregresivas integradas de medias móviles (del inglés, ARIMA). Azari (2019) utiliza ARIMA para la predicción de BTC en horizontes de tiempo corto y largo comparando los resultados con la performance de redes neuronales recursivas (RNN). Bukhari et al. (2020) utilizan métodos autorregresivos fraccionalmente integrados (ARFIMA) para el pronóstico de acciones en donde el órden de integración es fraccionario, a diferencia del ARIMA de orden entero.  En este caso se estudia el orden de integración del proceso marcando la importancia de las constantes de los procesos integrados que representan los efectos deterministas (Peña, 2010). Se modela la serie de precios de ETH con método ARFIMA y ARIMA en el presente trabajo y se presentan los pronósticos en los horizontes de tiempo definidos con las correspondientes métricas de error.

En relación a métodos tradicionales de pronóstico de series de tiempo, Kurniawan y Madelan (2022) utilizan la técnica de Holt Winters para la predicción del precio de cierre de BTC, Ripple y Litecoin.  En el presente trabajo se plantea la técnica tradicional de suavizado de Holt-Winters (1960) , donde se construye el pronóstico utilizando suavizado exponencial que pondera las observaciones con un decaimiento exponencial a medida que las observaciones son más lejanas en el pasado (Hyndman, 2014).

Bush (2019) aplica la técnica de pronóstico con RNN y LSTM de periodo corto y largo de memoria con corrección de pérdida de gradiente para realizar la comparación del pronóstico de la serie de tiempo. Se construyen modelos RNN y LSTM con  calibrado de hiperparámetros para el pronóstico de la serie de cierre de precios de Ether en el período definido en estudio.

Se considera la introducción precedente como una primera aproximación al contenido de este trabajo. Se comparan las técnicas para la predicción RNN, LSTM, Holt-Winters, ARFIMA y ARIMA para la serie de tiempo de precios de cierre de Ether en dólares estadounidenses (ETH-USD), en el período de tiempo comprendido entre el 01/01/2017 y el 31/12/2021, en tres horizontes de pronóstico definidos en 5, 15 y 30 días, mediante los criterios de raíz cuadrada del error cuadrático medio (del inglés RMSE), error absoluto medio (del inglés MAE) y del error medio absoluto porcentual (del inglés, MAPE). Se busca entonces, construir un marco teórico acorde al fenómeno en estudio, realizar un análisis descriptivo y encontrar calibraciones adecuadas en los modelos mencionados para la serie de tiempo en estudio.

\newpage
# 2-Objetivos e Hipótesis

## Objetivos Generales:

Realizar un análisis comparativo de métodos de pronóstico paramétricos y no paramétricos aplicados a la predicción de la serie de precios de cierre de la criptomoneda ETH/USD para los horizontes de 5, 15 y 30 días y de la volatilidad del precio de ETH/USD con pronóstico fuera de la muestra a un día.

## Objetivos Específicos:

- Construir, en base a la selección y sistematización de la literatura existente, un marco teórico que sustente la aplicabilidad de los métodos de pronóstico en la serie de datos de cierre diario de Ethereum y de los métodos de pronóstico de la volatilidad de los retornos logarítmicos de ETH/USD. 
 
- Realizar un análisis descriptivo de las series de tiempo de valores de cierre de Ethereum y retornos logarítmicos de precio de ciere en el período de estudio.
 
- Desarrollar los pronósticos de la serie de valores de cierre y de la volatilidad de Ethereum (ETH/USD) para los horizontes de tiempo fijados con las técnicas seleccionadas. Presentar en forma ordenada y sistematizada los resultados de la aplicación de las técnicas de pronóstico.
 
- Comparar los resultados obtenidos en el marco empírico y determinar la/s técnica/s que presenten mayor precisión de pronóstico contra los datos reales registrados en el mes de enero de 2022 en la cotización de cierre diario de Ethereum. Para la volatilidad, los pronósticos se comparan con la volatilidad aproximada por los residuales de los retornos logarítmicos al cuadrado en el periodo de estudio.  
 
- Realizar un análisis crítico de los resultados en relación al cumplimiento de las hipótesis planteadas.
 
## Hipótesis:

Se plantea que las series de tiempo de precio de cierre de ETH/USD y de retornos logarítmicos de ETH/USD presentan características que cumplen los supuestos necesarios para la aplicación de las técnicas de pronóstico detalladas en el presente trabajo.

Se postula que al desarrollar las técnicas de pronóstico de series temporales y aplicadas al caso particular de la series de precios de cierre y retornos logarítmicos de ETH/USD se obtendrán resultados útiles para la aplicación práctica en la negociación con la criptomoneda estudiada.


\newpage
# 3 Metodología

La metodología que se emplea para el desarrollo de la presente tesis se centra en los aspectos de comparación estadística de modelos estadísticos utilizados para la predicción de una serie de tiempo diaria con alta volatilidad. Se utilizará para el pronóstico los datos de cierre diarios de la cotización de Ethereum recopilados en el período de tiempo entre 01/01/2017 y el 30/12/2021. Como período de comparación se utilizarán las cotizaciones de cierre diario de Ethereum correspondientes al mes de enero de 2022 y la aplicación de un esquema de validación cruzada a lo largo de la serie completa. Para la comparación de la volatilidad se utilizan los residuales de los retornos logarítmicos al cuadrado como aproximación a la volatilidad con pronósticos fuera de la muestra a un horizonte de un día y un tamaño muestral de 300 (del 5/4/2021 al 30/1/2022).
 
Se abarcan los fundamentos de los métodos de pronóstico a emplear y su justificación de aplicación al problema. Además, se eligen los criterios de comparación, definiendo así la estructura conceptual a emplear empíricamente.
 
En la aplicación empírica se comienza con el análisis descriptivo de las series de tiempo estudiadas, se modelan los pronósticos utilizando las distintas técnicas y posteriormente se realiza la comparación de los resultados con los criterios definidos previamente.
 
Se comunican los resultados junto con la  interpretación de los mismos. Se desarrollan las conclusiones del trabajo en relación a la crítica sobre los resultados obtenidos y a se plantean posibles investigaciones futuras.

Se desarrollan los conceptos téoricos necesarios para el posterior trabajo empírico. Se busca mantener un orden en la complejidad de las técnicas que se desarrollan. Se comienza por una descripción de los modelos paramétricos utilizados para el pronóstico del precio de cierre de ETH/USD. Entre ellos en la presente tesis se trabaja con el modelo de suavizado exponencial, luego por los modelos autorregresivos integrados de medias móviles, los fraccionalmente integrados. Entre lo modelos no paramétricos para pronosticar el precio de cierre de ETH/UDS se desarrolla la descripción de conceptos relacionados con las redes neuronales autorregresivas NNETAR y los modelos de aprendizaje profundo de memoria larga LSTM.

Por otro lado, se desarrollan los conceptos de los modelos ARCH, GARCH y EGARCH para el modelado de la heterocedasticidad condicional ya que generan un aporte necesario a la hora de la modelización y predicción de la volatilidad.

Además de los modelos se mencionan las métricas aplicadas para el tratamiento de los pronósticos, el esquema de validación cruzada empleado, los estimadores de volatilidad y las consideraciones a emplear al momento de la evaluación de la performance del los modelos.

\newpage
## Modelos Paramétricos:

### Suavizado exponencial (ETS):

En Hyndman y Athanasopoulos (2018) se introduce el tema de los métodos de suavizado exponencial con un primer contraste sobre lo que ocurre al aplicar el método naive (todos los pronósticos del futuro son iguales al último valor observado de la serie) y el método de promedio (en donde todos los futuros pronósticos son iguales al promedio simple de los datos observados). Naive da importancia justamente al último valor de la serie, mientras que el promedio da igual importancia a todos los valores de la serie sin importar la distancia al pasado remoto. 
Hyndman et. al (2008) diferencian el concepto de método de pronóstico y modelo estadístico de pronóstico. Un método de pronóstico es un algoritmo que provee una estimación puntual de un valor futuro mientras que un modelo estadístico provee un proceso de generación de datos estocásticos que puede ser utilizado para producir una distribución completa de probabilidades para un período futuro de tiempo. Un modelo de estadístico permite la predicción de intervalos de predicción a un determinado nivel de confianza. Dentro de la familia de métodos de pronóstico se encuentran los suavizados exponenciales.

Entre los primeros aportes a estos métodos está el trabajo de Holt (1957) donde se trabaja con suavizado estacional aditivo y multiplicativo. Winters (1960) hace uso empírico del trabajo de Holt y en adelante los métodos son generalmente conocidos como Holt-Winters.

En un modelo de espacio de estados, la variable observada en la serie de tiempo $y_t$ es suplementada por variables auxiliares no observables llamadas estados. Se representan a las variables auxiliares con un solo vector $x_t$ que es llamado vector de estados (Hyndman et al. 2003).

Considerando $Y_t$ a la observación de la variable en estudio en el tiempo $t$ y $X_t$ un vector de estados que contiene componentes no observados que describen el nivel, la tendencia y la estacionalidad de la serie.

\begin{equation}\large{\mathbf{Y_t=w'X_t+\epsilon_t}}\label{eq:1}\end{equation}

\begin{equation}\large{\mathbf{X_t=FX_{t-1}+g\epsilon_t}}\label{eq:2}\end{equation}

Donde ${\epsilon_t}$ es una serie de ruido blanco, F, g y w son coeficientes. La primera de las ecuaciones anteriores es la ecuación de observación y la segunda es conocida como ecuación de transición (o estado). El término $\epsilon_t$ hace que sea un modelo de espacio de estados en las innovaciones.

Varios modelos de pronósticos de suavizado exponencial son equivalentes a casos especiales del modelo de la ecuaciones 1 y 2.

Los modelos de espacio de estado como el anterior encajan correctamente con los de suavizado exponencial ya que el nivel, la tendencia y el comportamiento estacional se encuentran incluidos explícitamente en el modelo. Esto no ocurre en modelos como por ejemplo el ARIMA (Box et al., 1994). Una de las ventajas de los modelos de suavizado exponencial para pronósticos es su simplicidad para ser ejecutados de manera completamente automática (Hyndman et. al, 2008).

Una serie de tiempo puede plantearse como una combinación de componentes como la tendencia (T),  el ciclo ( C), el componente estacional (S) y el irregular o error (E). Los mismos componentes se definen a continuación.

*Tendencia*: dirección a largo plazo de la serie.

*Componente estacional (S)*: un patrón que se repite con una periodicidad conocida (7 días por semana o 12 meses al año).

*Ciclo*: un patrón que se repite con cierta regularidad pero con una desconocida periodicidad (un ciclo de crecimiento económico).

*Irregular o Error (E)*: es el componente impredecible de la serie.

Estos componentes pueden ser combinados de diferentes maneras, ya sea, de forma aditiva o multiplicativa para formar la serie temporal. También puede darse una combinación de modelo aditivo pero con el término del error de forma multiplicativa. En Hyndman et. al (2008) se menciona que los métodos de suavizado exponencial comienzan con la descripción de la tendencia, que resulta una combinación del nivel inicial $(l)$ y el término de crecimiento $(b)$. Si $T_h$ es la tendencia en los $h$ próximos períodos y $\phi$ es el parámetro de amortiguación que varía entre 0 y 1. Existen 5 tipos distintos de tendencias o patrones de crecimiento: 

Ninguno: $T_h=l$

Aditivo:$T_h=l+bh$  

Aditivo amortiguado: $T_h=l+(\phi+\phi^2+...+\phi^h)b$

Multiplicativo:$T_h=lb^h$

Multiplicativo Amortiguado:$T_h=l(\phi+\phi^2+...+\phi^h)$

Los métodos amortiguados mitigan la tendencia a medida que el horizonte de pronóstico se prolonga. Una vez que se selecciona el componente de la tendencia Hyndman et. al (2008) indican que debe seleccionarse el componente estacional ya sea aditivo o multiplicativo y el correspondiente error que también puede incluirse de manera aditiva o multiplicativa pero en el caso de estimaciones puntuales de métodos de pronóstico no genera diferencias. Todas estas diferentes formas de considerar la tendencia y la estacionalidad dan lugar a distintas combinaciones de métodos (Taylor et al. 2003).

El método de Holt-Winters está basado en tres ecuaciones de suavizado: una para el nivel, una para la tendencia y otra para la estacionalidad. Existen 2 variantes del método dependiendo de la manera de representación de la estacionalidad, ya sea, aditiva o multiplicativa.

Cuando la estacionalidad es multiplicativa:

Siendo $Y_t$ el valor de la serie en el tiempo t, $\hat{Y}_{t+h|t}$ la estimación puntual del pronóstico de la serie en el tiempo $t+h$, $l_t$ es el término del nivel en el momento t, $b_t$ el crecimiento en tiempo t, $S_t$ es el término estacional, $\alpha$, $\beta$ son constantes de suavizado con valores entre 0 y 1 y , $\gamma$ es una constante de suavizado variando entre 0 y $1-\alpha$.

Nivel:

\begin{equation}\large{\mathbf{l_t=\alpha\frac{Y_t}{S_{t-m}}+(1-\alpha)(l_{t-1}+b_{t-1})}}\label{eq:2}\end{equation}

Crecimiento:

\begin{equation}\large{\mathbf{b_t=\beta^{*}(l_t-l_{t-1}) +(1-\beta^{*}) b_{t-1}}}\label{eq:2}\end{equation}

Estacional:

\begin{equation}\large{\mathbf{s_t=\frac{\gamma y_t}{l_{t-1}+b_{t-1}}+(1-\gamma)s_{t-m}}}\label{eq:2}\end{equation}

Pronóstico:

\begin{equation}\large{\mathbf{\hat{Y}_{t+h|t}=(l_t+b_th)s_{t-m+h_{m'}^{+}}}}\label{eq:2}\end{equation}



Cuando la estacionalidad es aditiva:

Nivel:

\begin{equation}\large{\mathbf{l_{t}=\alpha(y_{t}-s_{t-m})+(1-\alpha)(l_{t-1}+b_{t-1})}\label{eq:2}\end{equation}


Crecimiento: 

\begin{equation}\large{\mathbf{b_t=\beta^*(l_t-l_{t-1})+(1-\beta^*)b_{t-1}}}\label{eq:2}\end{equation}


Estacionalidad: 

\begin{equation}\large{\mathbf{s_t=\gamma(Y_t-l_{t-1}-b{t-1})+(1-\gamma)s_{t-m}}}\label{eq:2}\end{equation}


Pronóstico: 

\begin{equation}\large{\mathbf{\hat{Y}_{t+h|t}=l_t+b_th+s_{t-m+h_{m'}^{+}}}}\label{eq:2}\end{equation}


Los modelos de espacio de estados fundamentan a los métodos de suavizado exponencial. Para cada método existen 2 modelos (uno con errores aditivos y otro con errores multiplicativos), la estimación puntual es la misma para los 2 modelos, pero los intervalos de predicción difieren. Se expresan los modelos de suavizado exponencial como ETS (error, tendencia, estacionalidad) y dentro de cada subclase cada componente ya sea aditivo o multiplicativo. Una vez que el modelo está especificado en términos de sus componentes se puede estudiar la distribución de probabilidades de los valores futuros de la serie o la media condicional de una futura observación teniendo en cuenta las observaciones pasadas.

\begin{equation}\large{\mathbf{\mu_{t+h}=E(y_{t+h}| x_t)}}\label{eq:3}\end{equation}

Donde $x_t$ contiene los componentes no observables como $l_t$, $b_t$ y $s_t$. Para la mayoría de los modelos ETS estas medias condicionales son idénticas a las estimaciones puntuales, sin embargo, para otros modelos con tendencia multiplicativa o estacionalidad multiplicativa las medias condicionales y las estimaciones puntuales pueden diferir para $h>=2$ (Hyndman et.al 2008).

Un modelo general de espacio de estados para todos los métodos de suavizado exponencial involucra un vector de estados (Ecuación 12) y ecuaciones de espacio de estado (Ecuaciones 13 y 14) de la siguiente forma:

\begin{equation}\large{\mathbf{x_t=(l_t,b_t,s_t,s_{t-1},..., s_{t-m})'}}\label{eq:4}\end{equation}

\begin{equation}\large{\mathbf{y_t=w(x_{t-1})+ r(x_{t-1})\epsilon_t}}\label{eq:5}\end{equation}

\begin{equation}\large{\mathbf{x_t=f(x_{t-1})+g(x_{t-1})\epsilon_t}}\label{eq:5}\end{equation}

Donde $\epsilon_t$ es un ruido blanco gaussiano con varianza $\sigma^2$ y media $\mu_t=w(x_{t-1})$, f , g y w son constantes. El modelo con errores aditivos tiene $r(x_{t-1})=1$ entonces $y_t=\mu_t+\epsilon_t$ y el modelo con errores multiplicativos tiene  $r(x_{t-1})=\mu_t$, entonces $y_t=\mu_t(1+\epsilon_t)$. Entonces $\epsilon_t=(y_t-\mu_t)/\mu_t$ es el error relativo del error multiplicativo. Los modelos de error multiplicativo son útiles cuando los datos son estrictamente positivos pero no son estables numéricamente con la presencia de ceros o valores negativos. Para la aplicación de estos modelos en pronósticos es necesaria una inicialización en la que se presentan componentes iniciales de estacionalidad, un nivel inicial y una tendencia inicial con técnica heurística propuesta por Hyndman et al, 2002. Luego se procede a una estimación de los parámetros mediante la minimización de la función de máxima verosimilitud de los errores:

\begin{equation}\large{\mathbf{L^*(\theta, x_0)= n log(\sum_{t=1}^{n}\epsilon_t^2)+2\sum_{t=1}^{n}log|r(x_{t-1})|}}\label{eq:5}\end{equation}

que es igual al doble del logaritmo negativo de la función de verosimilitud (sin los términos constantes) condicional en los parámetros $\theta=(\alpha, \beta, \gamma,\phi$ y en los estados iniciales $x_o=(l_0, b_0,s_0,s_1,...,s_{t-m+1})'$, donde n es el número de observaciones. Los parámetros $\theta$ y los estados iniciales $x_0$ son obtenidos minimizando la verosimilitud $L$. 
En un pronóstico de un intervalo hacia adelante $\epsilon_t$ es el error de predicción a un paso. $E(y_t|y_{t-1},...,y_1,x_0)=E(y_t|x_{t-1})=wx_{t-1}$ entonces la predicción de $y_t$ dados el valor inicial $x_0$ y las observaciones $y_1,..., y_{t-1}$ es $wx_{t-1}$ 
Entonces las innovaciones pueden ser calculadas recursivamente utilizando:
$\hat{y}_{t|t-1}=w'x_{t-1}$
$\epsilon_t=y_t-\hat{y}_{t|t-1}$
$x_t=Fx_{t-1}+g\epsilon_t$
$D=F-gw’$.
Esta transformación se llama suavizado exponencial general (Box et al. 1994). Los pronósticos obtenidos con esta transformación son funciones lineales de las observaciones pasadas.
Cuando los pronósticos $y_t$ no están afectados por observaciones del pasado distante se describe al modelo como “pronosticable”. Siendo $a_t=wD_{t-1}x_0$  y $c_j=wD_{j}^{-1}g$ un modelo pronosticable es el que:
$sum_{j=1}^{\infty}|c_j|<\infty$ 
$\lim_{t \to\infty}a_t=a$
Esto impide que el efecto de observaciones del pasado distante tengan efecto en este tipo de modelos. La condición de estabilidad del modelo corresponde que los autovalores de la matriz D estén dentro del círculo unitario.  Es importante mencionar que un modelo puede ser estable y pronosticable pero algunos modelos pueden ser pronosticables pero no estables (Hyndman et al, 2003). La condición de estabilidad de la matriz D está relacionada con la restricción de invertibilidad de los modelos ARIMA.
La estacionariedad proviene de la matriz de transición, al iterar:
$x_t=Fx_{t-1}+g\epsilon_t=F^2x_{t-2}+Fg\epsilon_{t-1}+g\epsilon_t=...=$
$=F^tx_0+\sum_{j=0}^{t-1}F^jg\epsilon_{t-j}$
Sustituyendo $y_t=d_t+\sum_{j=0}^{t-1}k_j\epsilon_{t-j}$ 
Donde $d_t=w’F^{t-1}x_0$, $k_0=1$ y $k_j=w’F^{j-1}g$ para j=1,...,2…
Entonces la observación es  una función lineal del estado original $x_0$ y errores pasados y presentes. El modelo se define estacionario si:
$\sum_{j=0}^{\infty}|k_j|<\infty$ 
$lim_{t\to\infty} d_t=d$. 
Siendo un ejemplo de descomposición de Wold (Brockwell y Davis, 1991).

Se ve entonce que la $E(y_t)=d$ y 
$V(y_t)=\sigma^2\sum_{j=0}^{\infty} k_j^2$ 


\begin{equation}\large{\mathbf{y_t=d+\sum_{j=0}^{\infty}k_j\epsilon_{t-j}}}\label{eq:2}\end{equation}

Generalmente se puede aplicar modelos de suavizado exponencial a series presentan raíces unitarias y no son estacionarias.
Para la selección de los modelos una ventaja que presentan los modelos de suavizado exponencial es la posibilidad de utilizar los criterios de información para determinar cual es el más apropiado para una determinada serie temporal.
Para los modelos de suavizado exponencial el criterio de información de Akaike (AIC) se define como:
$AIC=-2log(L)+2k$ 
Donde L es la función de verosimilitud y k es el número total de parámetros y estados iniciales (incluida la varianza residual).
El criterio de Akaike corregido para el sesgo en pequeñas muestras se define como:
$AIC_c=AIC+\frac{2k(k+1)}{T-k-1}$
El criterio de información Bayesiano BIC se define como:
$BIC=AIC+k[log(T)-2]$. 
Donde T es el tamaño de la muestra.
Se menciona además que la técnica de suavizado exponencial es utilizada habitualmente por automatismos de intercambio (comunmente llamado por su término inglés, *trading*) de criptomonedas, que sin buscar un objetivo de pronóstico utilizan los cruces de medias móviles para detectar cambios en el comportamiento del mercado y accionar operaciones de compra o venta en función de determinadas estrategias.
 

### ARIMA:

Hyndman y Athanasopoulos (2018) inician la descripción de los modelos ARIMA con la definición de estacionariedad mencionada en los párrafos anteriores y la diferenciación como una primera forma de hacer estacionaria una serie de tiempo. Las transformaciones como el logaritmo ayudan a estabilizar la varianza mientras que la diferenciación ayuda a estabilizar la media de la serie. También la estacionariedad se observa en los gráficos de autocorrelación. En una serie estacionaria los valores van a caer cercanos a cero en pocos intervalos, mientras que en una serie no estacionaria la autocorrelación decrece lentamente.
Cuando una serie se comporta como un paseo aleatorio la serie puede ser escrita de la siguiente manera:

\begin{equation}\large{\mathbf{y_t=y_{t-1}+\epsilon_t}}\label{eq:2}\end{equation}

 
Los pronósticos de los modelos de paseo aleatorio son iguales a la última observación disponible ya que los movimientos futuros son impredecibles e igualmente probables de ir hacia arriba o hacia abajo, entonces muestran el mismo comportamiento que un modelo naive. Hyndman y Athanasopoulos (2018) mencionan también a las series cuyas diferencias entre períodos sucesivos no tienen media igual a cero como caminos aleatorios con deriva.

En el caso de que la primera diferencia no presente comportamiento estacionario es necesaria una segunda diferenciación. Otra forma de diferenciación que puede conducir a un pronóstico naive en caso de que el modelo de los datos ajuste correctamente se da en la diferenciación estacional, o sea, la diferenciación con respecto a determinado período en la misma estación. La diferenciación estacional da un modelo para los datos originales de la forma:

\begin{equation}\large{\mathbf{y_t=y_{t-m}+\epsilon_t}}\label{eq:2}\end{equation}

Donde m es el número de estaciones.

Es importante que al diferenciar los datos se pueda mantener una interpretabilidad de la información que se genera. 
Los tests de raíces unitarias son generalmente efectivos para determinar si una serie es o no estacionaria. Como se mencionó en la introducción los tests de raíces unitarias se emplean en este caso para probar la eficiencia de mercado. Entre los tests habitualmente utilizados se encuentran el contraste de Dickey Fuller (1979), Dickey Fuller Aumentado (1996) y el de KPSS (Kwiatkowsky et al, 1992), que son tests que no tienen elevada potencia para rechazar la hipótesis nula de raíz unitaria en presencia de uno o más quiebres estructurales. Gregoriu (2015) aplica el test de multiplicadores de Lagrange de raíces unitarias con quiebres estructurales. La presencia de quiebres estructurales en la serie afecta la potencia de los tests de raíces unitarias (Enders, 2015).
Hyndman y Athanasopoulos (2018) utilizan la notación de la letra B para indicar los retardos, por ejemplo, $By_t=y_{t-1}$. Una indicación del tipo $B^2y_t=y_{t-2}$ o si se refiere a un retraso anual $B^12y_t=y_{t-12}$. También se utiliza el operador de retardo para identificar el proceso de diferenciación, donde una primera diferencia puede ser escrita como:

\begin{equation}\large{\mathbf{y_t’=y_t-y_{t-1}=y_t-By_t=(1-B)y_t}}\label{eq:2}\end{equation}

En general la diferencia de orden d se escribe como $(1-B)^dy_t$.

Continuando la línea de descripción de los componentes de los modelos ARIMA se detalla el componente autorregresivo de los mismos. Un modelo autorregresivo genera un pronóstico de la variable de interés utilizando una combinación lineal de valores pasados de la variable. Un modelo autorregresivo (AR) tiene la forma:

\begin{equation}\large{\mathbf{y_t=c+\phi_1y_{t-1}+\phi_2y_{t-2}+...+\phi_py_{t-p}+\epsilon_t}}\label{eq:2}\end{equation}


Donde $\epsilon_t$ es ruido blanco. Este modelo utiliza p retardos para estimar el valor de $y_t$ con lo que se denomina AR(p). La varianza del término del error solamente afecta la escala de la serie, no los patrones de comportamiento de la misma. Los valores de los parámetros $\phi$ se pueden restringir para el modelado de series estacionarias.

Los modelos de promedios móviles (MA) pueden ser interpretados como modelos que realizan un promedio ponderado de los últimos errores de pronóstico. Un modelo de promédios móviles entonces es de la forma:

\begin{equation}\large{\mathbf{y_t=c+\epsilon_t+\theta_1\epsilon_{t-1}+\theta_2\epsilon_{t-2}+...+\theta_q\epsilon_{t-q}}}\label{eq:2}\end{equation}

Donde  $\epsilon_t$ es ruido blanco. 
Es posible escribir un proceso estacionario AR(p) como un MA($\infty$). El proceso inverso lleva de un MA(q) a un AR($\infty$). Cuando $|\theta|>1$ los pesos de los errores aumentan a medida que se vuelven más distantes en el tiempo, cuando $|\theta|=1$ los pesos de los errores tienen igual importancia sin distinguir el paso del tiempo. La situación que mayor sentido matemático tiene se da cuando los efectos del pasado son de menor importancia $|\theta<1|$ que justamente es cuando el proceso es invertible.

Al combinar un proceso integrado (inverso a la diferenciación) con los modelos autorregresivos y los modelos de medias móviles se obtiene un modelo ARIMA (Hyndman y  Athanasopoulos (2018)). El modelo puede escribirse en notación como se describe a continuación:

\begin{equation}\large{\mathbf{y’_t=c+\phi_1y’_{t-1}+...+\phi_py’_{t-p}+\theta_1\epsilon_{t-1}+...+\theta_q\epsilon_{t-q}+\epsilon_t}}\label{eq:2}\end{equation}


Donde $y’_t$ es la serie diferenciada (puede estar diferenciada más de una vez). Este modelo se denomina ARIMA(p,d,q) donde p es el orden de la parte autorregresiva, d es el grado de diferenciación involucrado y q es el orden de la parte de medias móviles. Con esta notación un modelo de ruido blanco corresponde a un ARIMA(0,0,0), un paseo aleatorio a un ARIMA(0,1,0), un paseo aleatorio con deriva es un ARIMA (0,1,0) con una constante, un modelo autorregresivo AR(p) es un ARIMA (p,0,0) y un modelo de medias móviles MA(q) es un ARIMA(0,0,q). 
Utilizando la notación del componente de retardo el modelo ARIMA(p,d,q) se escribe:

\begin{equation}\large{\mathbf{(1-\phi_1B-...-\phi_pB^p)(1-B)^dy_t = c+(1+\theta_1B+...+\theta_qB^q)\epsilon_t}}\label{eq:2}\end{equation}

Tal como se describe previamente la identificación tanto de raíces unitarias como de quiebres estructurales en la serie de tiempo en estudio permite encontrar intervalos temporales en los que no se manifieste el comportamiento de mercado eficiente que se evidencian como paseos aleatorios y, por lo tanto, no permiten en teoría mejorar la predicción obtenida mediante el método naive.

Mediante los gráficos de autocorrelación y autocorrelación parcial es posible identificar los valores de p y q apropiados para el modelado en una serie estacionaria o transformada en estacionaria. La función de autocorrelación (ACF, sus siglas en inglés) muestra la correlación entre valores sucesivos de la serie de tiempo. El valor de $r_n$ mide la correlación entre $y_t$ y $y_{t-k}$.
Se define entonces la autocorrelación de una serie de tiempo mediante la siguiente ecuación:
\begin{equation}\large{\mathbf{r_k=\frac{\sum_{t=k+1}^{T} (y_t-\bar{y})(y_{t-k}-\bar{y})}{\sum_{t=1}^{T}(y_t-\bar{y})}}}\label{none}\end{equation}

Cuando una serie no tiene autocorrelación se llama ruido blanco. Para series de ruido blanco se espera  que el 95\% de los picos de la función de autocorrelación se encuentren entre $\pm \frac{1.96}{\sqrt{T}}$ donde T es la longitud de la serie de tiempo. Estas bandas en general se grafican en la función de autocorrelación (ACF). Si uno o más picos se encuentran fuera de las bandas, entonces la serie probablemente no es un ruido blanco . Si ACF muestra la relación entre $y_t$ e $y_{t-k}$ para diferentes valores de k, cuando $y_t$ e $y_{t-1}$ están correlacionados entonces $y_{t-1}$ e $y_{t-2}$ deben estar correlacionados, sin embargo $y_t$ e $y_{t-2}$ pueden estar correlacionados simplemente porque ambos están conectados a $y_{t-1}$ en lugar de que algo de la información de $y_{t-2}$ esté relacionada con $y_t$ (Hyndman, 2013). Para solucionar este problema se utilizan las autocorrelaciones parciales que miden la relación entre $y_t$ e $y_{t-k}$ luego de remover los efectos de los retardos $1,2,3,...,k-1$. Las autocorrelaciones parciales tienen los mismos valores críticos de $\pm1.96/\sqrt{T}$. Los gráficos de ACF y PACF pueden ayudar a identificar los coeficientes de un modelo ARIMA cuando el modelo sea de la forma ARIMA(o,d,q) o ARIMA(p,d,0). Si ambos p y q son positivos los gráficos no permiten identificar de manera simple los valores de los coeficientes.

Una vez que el orden del modelo está identificado se estiman los parámetros del mismo. Dependiendo del tipo de software y del algoritmo utilizado puede haber alguna diferencia en los valores de los coeficientes. El software R utiliza la estimación de máxima verosimilitud. También se utilizan los criterios de información. El criterio de información AIC para un modelo ARIMA se    
escribe como:

\begin{equation}\large{\mathbf{AIC=-2Log(L)+2(p+q+k+1)}}\label{eq:2}\end{equation}
 
Donde L es la verosimilitud de los datos, $k=1$ si $c\neq0$ y $k=0$ si $c=0$
El criterio de información de Akaike corregido para los modelos ARIMA se define como:

\begin{equation}\large{\mathbf{AIC_c= AIC+\frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2}}}\label{eq:2}\end{equation}


El criterio de información Bayesiano:

\begin{equation}\large{\mathbf{BIC=AIC+[log(T)-2](p+q+k+1)}}\label{eq:2}\end{equation}

Hyndman(2013) recomienda utilizar un método para encontrar el orden de integración d del modelo (correspondiente al término integral) y luego utilizar los criterios de información para la determinación de los órdenes p y q.
Si se escribe el modelo ARIMA como:

\begin{equation}\large{\mathbf{\phi(B)(1-B)^d y_t=c+\theta(B)+\epsilon_t}}\label{eq:2}\end{equation}



Donde $\phi(B)=(1-\phi_1B-+...+\phi_pB^p)$ es un polinomio de orden p en B y 
$\theta(B)=(1+\theta_1B+...+\theta_qB^q)$ es un polinomio de orden q en B, las condiciones de estacionariedad del modelo son que las raíces complejas de $\phi(B)$ caigan fuera del círculo unitario y las condiciones de invertibilidad corresponden a que las raíces del polinomio $\theta(B)$ caigan fuera del círculo unitario. Hyndman y Khandakar (2008) utilizan una función en el software R que grafica la inversa de las raíces dentro del círculo unitario para probar estacionariedad e invertibilidad del modelo.

Los modelos ARIMA también pueden representar la estacionalidad de los datos. En ese caso se modela además la parte estacional de los datos $ARIMA (p,d,q) (P,D,Q)_m$ donde m es el número de observaciones al año. El componente estacional del modelo involucra a los retardos de la parte estacional del modelo. Esto se ve en los componentes estacionales de las funciones de autocorrelación y autocorrelación parcial.
Se deben analizar los residuos que deja el modelo seleccionado, los mismos deben parecer ruido blanco. Se debe complementar el análisis de los residuos utilizando el test de Ljung-Box (1978) para comprobar si los residuos presentan autocorrelación.

## ARFIMA:

La propiedad de memoria larga de las series de tiempo está relacionada con el lento decrecimiento de las autocorrelaciones muestrales hasta que finalmente convergen a cero. En las series de tiempo que presentan memoria larga las innovaciones tienen efectos transitorios pero estos perduran durante largo tiempo. En el modelo de estas series el trabajar con diferenciación es excesivo pero no diferenciar tampoco genera un comportamiento adecuado. Granger (1980), Granger y Joyeux(1980) y Hosking (1981) proponen una serie de modelos con  orden de integración fraccionario. Son procesos ARMA fraccionalmente integrados denominados ARFIMA (p,d,q) donde d es un número real. Baillie y Bollerslev (1994) encuentran la presencia de memoria larga en series de activos financieros.

Un proceso $y_t$ es integrado de orden d, y se denota por I(d), si se verifica que $(1-B)^d$, $y_t=u_t$, donde B es el operador retardo y $u_t$ es un proceso estacionario con densidad espectral acotada en la frecuencia cero. Si d=0, $y_t$ es estacionario; si $d=1$, $y_t$ tiene una raíz unitaria; si d es otro número entero, $y_t$ tiene d raíces unitarias; y si d es un número real no entero, $y_t$ es un proceso fraccionalmente integrado. Si la estructura de la media es además ARMA se dice que el proceso fraccionalmente integrado es un ARFIMA. El caso más sencillo, donde $p=0$ y $q=0$ se llama ruido blanco fraccionalmente integrado o ARFIMA(0,d,0).

Hosking(1981) demuestra que si $-0.5<d<0.5$ el proceso $y_t$ es invertible y estacionario y, además admite la representación $AR(\infty)$. Brockwell y Davies (1991) demuestran que los coeficientes de la representación de Wold de un ARFIMA(0,d,0) convergen hiperbólicamente hacía cero a diferencia del decrecimiento exponencial típico de los procesos ARMA. Los coeficientes safisfacen la condición de estacionariedad pero no son absolutamente sumables. 

La propiedad más característica de los procesos ARFIMA estacionarios $(d< \frac{1}{2})$  es el comportamiento asintótico de su función de autocorrelación. Robinson (1994) y Baillie (1996) muestran que cuando el orden de diferenciación está entre 0.5 y 1 el proceso puede modelizar el comportamiento de series no estacionarias pero que en el largo plazo tienen reversión a la media, algo que no es posible de modelar con procesos que tienen raíces unitarias.
La ecuación del proceso ARFIMA (p,d,q) resulta entonces:

\begin{equation}\large{\mathbf{\phi(B)(1-B)^dy_t=\theta(B)\epsilon_t}}\label{eq:2}\end{equation}

Donde $\phi(B)$ y $\theta(B)$ son polinomios de grado p y q respectivamente cuyas raíces están fuera del círculo unitario.

El comportamiento a largo plazo del proceso ARFIMA(p,d,q) es similar al del proceso ARFIMA(0,d,0), ya que para observaciones muy distantes, los efectos de los parámetros ARMA son casi despreciables. Hosking (1981) prueba que si $d<\frac{1}{2}$ y todas las raíces de $\phi(B)=0$ están fuera del círculo unidad, $y_t$ es estacionario, y si $d>\frac{1}{2}$ y todas las raíces de $\theta(B)=0$ están fuera del círculo unidad, $y_t$ es invertible.

Perez y Ruiz (2001) afirman que las diferencias entre los modelos ARFIMA, los modelos ARMA de memoria corta y los modelos integrados ARIMA, pueden establecerse analizando la forma en que la serie $y_t$ responde a un cambio unitario en la perturbación del modelo. Mientras en un modelo ARIMA, el efecto sobre el nivel futuro de la serie de un shock sobre la perturbación es permanente y no hay reversión a la media, en un ARFIMA dicho efecto finalmente desaparece, aunque a un ritmo más lento que el exponencial de los procesos ARMA.

Para contrastar la presencia de memoria larga en una serie temporal existen distintos contrastes. El contraste de Geweke y Porter-Hudak (1983) se deriva de la estimación MCO del modelo de regresión en el logaritmo del periodograma. La pendiente de este modelo es precisamente d, y por tanto, para contrastar la hipótesis $H_0:d=0$ (memoria corta) frente a la alternativa $H_1:d\neq0$, se realiza el contraste habitual de significación sobre el parámetro d con una distribución t-Student. Haubrich y Lo (1989) y Lo (1991) proponen un contraste de memoria corta basado en una modificación del estadístico R/S (rango reescalado) introducido por Hurst (1951). El estadístico R/S es el rango de las sumas parciales de las desviaciones de una serie a su media, normalizado por su desviación típica muestral.
Robinson (1991) desarrolla un estadístico basado en los multiplicadores de Lagrange para contrastar la hipótesis de ruido blanco frente a una familia muy general de alternativas que incluye a los procesos ARFIMA(0,d,0). En este caso, el estadístico admite una expresión muy sencilla de la forma $\Lambda_1= 2 \lambda_1$ , donde $\lambda_1=\sqrt{(\frac{6T}{\pi^2}}\sum_{j=1}^{T-1}\frac{r(j)}{j}$,siendo $r(j)=c(j)/c(0)$ la autocorrelación muestral de orden j. Bajo la hipótesis $H_0:d=0$, $\lambda_1$ tiene una distribución asintótica normal N(0,1), y por tanto, $\Lambda_1$ se distribuye asintóticamente como una $\chi^2$. Sowell (1990) deriva la distribución asintótica del estadístico del contraste de Dickey-Fuller para series fraccionalmente integradas I(d), con $1/2<d<3/2$, y demuestra que ésta depende del orden de integración d.

Además de los contrastes mencionados, una herramienta muy utilizada para detectar memoria larga en una serie temporal es la función de autocorrelación muestral. Las autocorrelaciones de un proceso con memoria larga decaen muy lentamente hacia cero, dicho comportamiento en las autocorrelaciones muestrales suele considerarse como indicio de integración fraccional.

## Modelos No Paramétricos utlizados en predicción de precio de cierre:

## NNETAR:

Las redes neuronales artificiales son métodos de pronóstico que se basan en modelos matemáticos simples del cerebro. Permiten modelizar relaciones complejas no lineales entre la variable de respuesta y sus predictores. Se puede pensar en una red neuronal como una red de neuronas que están organizadas en capas. Los predictores (o entradas) forman la capa inferior y los pronósticos (o salidas) forman la capa superior. También puede haber capas intermedias que contengan “neuronas ocultas” (Hyndman y Athanasopoulos, 2018).

Masci y Del Rosso (2020): Una red neuronal (RNN) se ocupa de los problemas de secuencia porque sus conexiones forman un ciclo dirigido. En otras palabras, pueden retener el estado de una iteración a la siguiente utilizando su propia salida como entrada para el siguiente paso.

Las redes más simples no contienen capas ocultas y son equivalentes a regresiones lineales.


```{r,graffoto16,fig.align='center', fig.height=4,fig.width=5,fig.cap="Red neuronal simple. Fuente: Masci,Del Rosso, Unidades 9 y 10, página 6, 2020)", fig.scap="Red neuronal simple.", echo=F, message=F}

library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\Grafico NNR-1.jpg")


```



La figura precedente muestra una versión de red neuronal de una regresión lineal con cuatro predictores. Los coeficientes adjuntos a estos predictores se denominan pesos. Los pronósticos se obtienen mediante una combinación lineal de las entradas. Los pesos se seleccionan en el marco de la red neuronal utilizando un "algoritmo de aprendizaje" que minimiza una función de costo como el ECM. 

Si se incorporan capas intermedias con neuronas ocultas, la red neuronal se vuelve no lineal. Esto se conoce como red de múltiples capas de alimentación hacia adelante, donde cada capa de nodos recibe entradas de las capas anteriores. Las salidas de los nodos en una capa son entradas para la siguiente capa. Las entradas de cada nodo se combinan mediante una combinación lineal ponderada. Luego, el resultado es modificado por una función no lineal antes de ser emitido.


```{r,graffoto17, fig.align='center', fig.height=4,fig.width=5,fig.cap="Red neuronal multicapa. Fuente: Hyndman y Athanosopoulos (2018), Unidad 12, Figura 12.15", fig.scap="Red neuronal multicapa.", echo=F, message=F}

library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\Captura_NNETAR_multicapa.jpg")


```


Por ejemplo las entradas en la neurona oculta j se combinan linealmente para dar la ecuación (30):

\begin{equation}\large{\mathbf{z_j=b_j+\sum_{k=1}^{x}w_{kj}y_{k}}}\label{eq:19}\end{equation}

Donde $z_j$ son los valores calculados de la capa oculta, $w_{i,j}$ es la matriz de pesos, $b$ es la ganancia escalar.

Cuando se usa una red neuronal de retroalimentación para pronosticar una serie de tiempo, las entradas pueden ser valores observados previamente de la serie de tiempo $y_t$. Esto se denomina red neuronal de autorregresión. Por ejemplo, podrían utilizarse los últimos cuatro valores previos para predecir el próximo valor de la serie temporal. Los nodos ocultos estarían dados por la ecuación (31):  

\begin{equation}\large{\mathbf{z_j=b_j+\sum_{k=1}^{4}w_{kj}y_{t-k}}}\label{eq:20}\end{equation}

En la capa oculta se aplica una función de activación a los nodos ocultos la cual es una función no lineal, como una función sigmoidea-logística:

\begin{equation}\large{\mathbf{\phi(z)=\frac{1}{1+e^{-z}}}}\label{eq:20}\end{equation}

Esta ecuación se aplica para dar la entrada a la siguiente capa de la red neuronal. La aplicación de esta función tiende a reducir el efecto de valores de entrada extremos, lo que hace que toda la red sea más robusta a los efectos de valores atípicos. El parámetro que restringe los pesos se conoce como parámetro de decaimiento.

Los pesos “aprenden” eligiendo primero valores aleatorios para ellos inicialmente,y estos pesos elegidos al azar se actualizan luego utilizando los datos observados o series temporales. Debido a que los pesos se eligen inicialmente al azar, existe un elemento de aleatoriedad en las predicciones producidas por una red neuronal. Por lo tanto, la red generalmente se entrena varias veces utilizando diferentes puntos de partida aleatorios, y luego se promedian los resultados. El número de capas ocultas y el número de nodos en cada capa oculta deben especificarse de antemano. 

Con datos de series de tiempo, los valores rezagados se pueden usar como entradas a una red neuronal, al igual que en un modelo AR(p). A esto se denomina un modelo autorregresivo de red neuronal (modelo $NNAR(p, k)$), el cual tiene dos componentes p y k, Solamente se considerará red de múltiples capas de alimentación hacia adelante con una capa oculta, y se utilizará la notación NNAR(p, k), donde, p denota el número de valores rezagados que se utilizan como entradas y k denota el número de nodos ocultos que están presentes.

Si el conjunto de datos es estacional, se expresa como $NNAR(p, P, k)$, donde P denota la cantidad de lags estacionales. 

El valor de p se elige en función de los criterios de información.
Las redes neuronales tienen un componente aleatorio inherente.

Luego, el pronóstico se presenta como media o mediana. También se sabe que las redes neuronales no funcionan bien con tendencias presentes en los datos. Por lo tanto, debemos eliminar la tendencia o diferenciar los datos antes de ejecutar el modelo de red neuronal.

Hyndman y Athanasopoulos (2018) mencionan que al realizar pronósticos a un período se utilizan los datos históricos de la serie temporal en estudio y al pronosticar más de un período es necesario utilizar la información historica junto con el/los pronóstico/s anterior/es realizados. 

En relación a los intervalos de pronóstico para las redes neuronales es necesario realizar simulaciones de los residuos. Si la red neuronal se escribe:

\begin{equation}\large{\mathbf{y_t=f(y_{t-1})+\epsilon_t}}\label{eq:20}\end{equation}

Donde $\mathbf{y_{t-1}}=(y_{t-1},y_{t-2},...,y_{t-r})'$ es un vector que contiene valores previos de la serie y $f$ es una red neuronal con 1 capa oculta. Si se supone que los errores $\epsilon_t$ son homocedásticos y posiblemente esten distribuidos normalmente, se puede generar una distribución de los errores de una normal o sino con remuestreo de los valores históricos. Al repetir cientos o miles de veces este proceso se generan los intervalos de confianza para los modelos de redes neuronales autorregresivas (NNETAR, sus siglas en inglés).

## LSTM:

Masci y Del Rosso (2020) explican en su apunte de clases que el proceso de memoria de corto y largo plazo fue introducido por Hochreiter y Schmidhuber (1997) como una red neuronal recurrente (RNN) que se entrena mediante la retropropagación a través del tiempo y supera el problema del gradiente de desaparición (vanishing gradient problem).
Que sea recurrente significa que mediante bucles permiten que la información persista.

Una red neuronal recurrente se puede considerar como múltiples copias de la misma red, cada una de las cuales pasa un mensaje a un sucesor. Uno de los atractivos de los RNN es la idea de que podrían conectar la información anterior a la tarea actual. Donde la brecha entre la información relevante y el lugar donde se necesita es pequeña, los RNN pueden aprender a usar la información pasada.También hay casos en los que se necesita más contexto y es muy posible que la brecha entre la información relevante y el punto donde se necesita se vuelva muy grande. En la Figura 3:


```{r,graffoto18,fig.align='center', fig.height=4,fig.width=5,fig.cap="Red neuronal recurrente LSTM. Fuente: Masci y Del Rosso, (unidades 9 y 10, página 16, 2020)", fig.scap="Red neuronal recurrente LSTM.", echo=F, message=F}

library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\LSTM-grafico-1.jpg")


```


Una de las mayores diferencias entre RNN y LSTM es que los RNN son incapaces de aprender a conectar la información de largo plazo mientras que los LSTM pueden cumplir con este objetivo.

Los LSTM también tienen esta estructura similar a una cadena, pero el módulo repetido tiene una estructura diferente. 


```{r,graffoto21,fig.align='center', fig.height=4,fig.width=5,fig.cap="Comportamiento dentro de cada módulo de cadena LSTM. Fuente: Masci y Del Rosso, (unidades 9 y 10, página 18, 2020)", fig.scap="Comportamiento dentro de cada módulo de cadena LSTM.", echo=F, message=F}

library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\LSRM-grafico-2.jpg")


```



* Cada línea lleva un vector completo, desde la salida de un nodo hasta las entradas de otros.

* Los círculos rosas representan operaciones puntuales, como la suma de vectores, mientras que los cuadros amarillos son capas de redes neuronales aprendidas.

* Las líneas que se fusionan denotan concatenación, mientras que una bifurcación de líneas denota que su contenido se está copiando y las copias van a diferentes ubicaciones.

El estado de la celda es como una cinta transportadora. Corre directamente a lo largo de toda la cadena, con solo algunas interacciones lineales menores.

El LSTM tiene la capacidad de eliminar o agregar información al estado de la celda, regulada cuidadosamente por estructuras llamadas puertas. Las puertas son una forma de dejar pasar información opcionalmente. Están compuestos por una capa de red neuronal sigmoidea y una operación de multiplicación puntual.

La capa sigmoidea genera números entre cero y uno, que describen cuánto de cada componente debe dejarse pasar. Un valor de cero significa “no dejar pasar nada”, mientras que un valor de uno significa “dejar pasar todo”. Un LSTM tiene tres de estas puertas para proteger y controlar el estado de la celda.

En lugar de neuronas, las redes LSTM tienen bloques de memoria que están conectados a través de capas.

* Un bloque tiene componentes que lo hacen más inteligente que una neurona clásica y una memoria para secuencias recientes.

* Un bloque contiene puertas que administran el estado y la salida del bloque.

* Un bloque opera sobre una secuencia de entrada y cada puerta dentro de un bloque utiliza las unidades de activación sigmoidea para controlar si se activan o no, lo que condiciona el cambio de estado y la adición de información que fluye a través del bloque.

Hay tres tipos de puertas dentro de una unidad:

* Puerta de olvido: decide condicionalmente qué información desechar del bloque.

* Puerta de entrada: decide condicionalmente qué valores de la entrada actualizar el estado de la memoria.

* Puerta de salida: decide condicionalmente qué generar en función de la entrada y la memoria del bloque.

## Modelos empleados para el pronóstico de la volatilidad:

La volatiliad es frecuentemente medida como la desviación estándard muestral de la serie de retornos de un determinado activo financiero (Poon, 2005). 

En este caso la ecuación:

\begin{equation}\large{\mathbf{\hat{\sigma}=\sqrt{\frac{1}{T-1}\sum_{t=1}^{T}(r_t-\mu)^2}}}\label{eq:2}\end{equation}

Donde $r_t$ es el retorno en el día t y $\mu$ es el retorno medio sobre el período de T días.

A veces se utiliza la varianza como una medida de la volatilidad. Sin embargo la varianza resulta menos estable que el desvío estándar para los cálculos computacionales y la evaluación de los pronósticos de volatilidad. La desviación estandar tiene además la misma unidad que la media lo que resulta ventajoso. La volatilidad está relacionada con el riesgo pero no es exactamente lo mismo ya que la volatilidad es una medida que puede ser relacionada con un resultado positivo y en general el riesgo se asocia con un resultado no deseado. La volatilidad no es una medida perfecta para medir el riesgo porque la desviación estandar es una medida de variabilidad de una distribución pero no brinda información sobre la forma de la distribución (a excepción de la distribución normal o lognormal donde la media y el desvío son estadísticos suficientes).

Los hechos que se manifiestan empíricamente en los mercados financieros de manera regular se denominan hechos estilizados. En el caso de la volatilidad, Poon (2005) menciona que los retornos de los activos financieros tienen largas colas comparadas con la distribución normal, lo que implica que gran parte del tiempo los retornos de los activos financieros fluctúan en un rango menor que una distribución normal. Otra de las características más importantes para el presente trabajo es la naturaleza de variabilidad temporal de las fluctuaciones en los retornos que justamente fue lo que originó los modelos de heterocedasticidad condicional detallados más adelante. Otro hecho estilizado es la presencia de clústers de volatilidad donde un día de alta volatilidad en los mercados tiende a estar seguido por otro día de alta volatilidad. 

Poon (2005) además menciona los siguiente hechos estilizados: los retornos de los activos financieros ($r_t$) no están generalmente autocorrelacionados a excepción de la autocorrelación a un retardo. La función de autocorrelación del valor absoluto de los retornos $|r_t|$ y la de $r_t^2$ decae lentamente y la correlación $corr(|r_t|,|r_{t-1})>corr(r_t^2,r_{t-1}^2)$. La autocorrelación para las potencias $corr(|r_t|,|r_{t-1})>corr(r_t^d,r_{t-1}^d)$ con $d\neq1$. Los retornos al cuadrado y el valor absoluto de los retornos son aproximaciones de la volatilidad diaria. La volatilidad presenta asimetría, situación que se observa en general luego de un día de retornos negativos lo cual se conoce como efecto Leverage. Otro hecho estilizado mencionado por Poon(2005) es que la volatilidad de diferentes activos financieros y diferentes mercados tienden a moverse en la misma dirección. 

La estimación de la volatilidad: Si se considera una serie de tiempo de los retornos $r_t$ con t variando de 1...T la desviación estandard $\sigma$ corresponde a la volatilidad incondicional en el período T. Dado que la volatilidad no se mantiene constante en el tiempo es necesario utilizar la volatilidad condicional $\sigma_{t,T}$. La estimación de la volatilidad depende de la información disponible, al haber información intradiaria para las transacciones la estimación de la volatilidad se vuelve más precisa. El uso del retorno diario para aproximar la volatilidad resultó ser un estimador muy ruidoso. Mientras la ecuación (34) al cuadrado resulta ser un estimador insesgado de la varianza no condicional $\sigma^2$, la raíz cuadrada de $\sigma^2$ resulta ser un estimador sesgado de $\sigma$ debido a la desigualdad de Jensen que establece que si el retorno $r_t\sim N(0,\sigma^2)$ entonces $E(|r_t|)=\sigma_t\sqrt{2/\pi}$, por lo tanto $\hat{\sigma_t}=|r_t|/\sqrt{2/\pi}$ tiene una distribución condicional normal. Ding, Granger y Engle (1993) sugieren medir la volatilidad directamente desde el valor absoluto de los retornos. Hay trabajos empíricos de Taylor (1986), Ederington y Guan, (2000) y McKenzie (1999) que muestran que los valores absolutos de los retornos permiten mejores pronósticos que los retornos al cuadrado. Sin embargo los modelos de la familia ARCH son modelos de retornos al cuadrado.

Lopez(2001) demostró que los retornos al cuadrado calculados con los precios de cierre de los activos financieros son estimadores insesgados pero muy imprecisos de $\sigma_t^2$ debido a la asimetría de su distribución (demostración en Poon, 2005).

El uso de la información del precio máximo y mínimo del día para la aproximación de la volatilidad. El estimador de la volatilidad que trabaja con precios mínimos, máximos, de apertura y de cierre fue estudiado por Parkinson (1980), Garman y Klass (1980), Beckers (1993), Rogers y Satchel (1991), Wiggins (1992), Rogers, Satchel y Yoon (1994), Alizadeth, Brandt y Diebold (2002). Se basa en el supuesto de que el retorno está normalmente distribuido con volatilidad condicional $\sigma_t$. Sean $H_t$ y $L_t$ los precios máximos y mínimos en el día t. El estimador de Parkinson (1980) H-L presenta un proceso de movimiento Browniano que resulta en el siguiente estimador desarrollado por Bollen e Inder (2002) con la siguiente ecuación:

\begin{equation}\large{\mathbf{\hat{\sigma_t}^2=\frac{(ln H_t-ln L_t)^2}{4ln2}}}\label{eq:2}\end{equation}

El estimador de Garman y Klass (1980) es una extensión del estimador de Parkinson (1980) con información del precio de la apertura $p_{t-1}$ y precio de cierre $p_t$ con la siguiente ecuación:

  \begin{equation}\large{\mathbf{\hat{\sigma_t}^2=0.5\left (\ln\frac{H_t}{L_t}\right ) ^2-0.39\left ( \ln\frac{p_t}{p_{t-1}} \right )^2}}\label{eq:2}\end{equation}

Como se mencionó anteriormente existe el hecho estilizado de que los retornos en los mercados financieros no presentan una distribución normal y tienen largas colas en general. El estimador H-L es sensible a la presencia de outliers y por esto Poon (2005) recomienda aplicar truncamiento. Siempre que no haya valores atípicos desestabilizadores el estimador H-L es muy eficiente y está menos afectado por la microestructura del mercado. Poon (2005) menciona que como característica de los mercados financieros, el uso de datos intradiarios de frecuencia menor a 5 minutos tienen correlación serial espúria causado por la microestructura del mercado como el intercambio no sincrónico, observaciones de precio discretas, patrones de volatilidad intradiaria y el rebote de precios por oferta y demanda. 

Poon (2005) menciona que el pronóstico de la volatilidad a través de múltiples períodos $\sigma_{T,T+j}$ para un período j se toma como la suma de pronósticos de múltiples pasos $\sum_{j=1}^s h_{T+j|T}$. Estos pronósticos se realizan por sustituciones recursivas utilizando el hecho de que $\epsilon^2_{T+i|T}=h_{T+i|T}$ para i mayores que cero y $\epsilon^2_{T+i|T}=\epsilon^2_{T+i}$ para $T+i\leq0$. Como la volatilidad de una serie de tiempo financiera tiene una estructura compleja, Diebold, Hickman, Inoue y Schuermann (1998) advierten que los pronósticos pueden diferir dependiendo el nivel de volatilidad actual de la serie, la estructura de volatilidad de la misma y el horizonte de tiempo empleado. Andersen, Bollerslev y Lange(1999) mencionan que en general la precisión de un pronóstico de volatilidad mejora cuando la frecuencia del muestreo de datos para la volatilidad aumenta en relación al horizonte de pronóstico. 

El segundo capítulo del libro de Poon (2005) desarrolla criterios para la evaluación de pronósticos de volatilidad. Llamando $\hat{X_t}$ la volatilidad predicha y a $X_t$ el valor estimado de la volatilidad define el error de pronóstico como $\epsilon_t=\hat{X_t}-X_t$. El autor menciona cinco puntos importantes a considerar a la hora de la evaluación de un pronóstico de volatilidad.

1- Si la forma de $X_t$ tiene que ser $\sigma_t$ o $\sigma_t^2$. Poon (2005) justifica el uso de $\sigma_t$ porque indica que $\sigma_t$ tiene mejor capacidad para capturar los hechos estilizados. Además el cuadrado del error de la varianza es la cuarta potencia del mismo error medido con la desviación estandar. Esto genera complicaciones de estimación porque deben existir los cuartos momentos de las distribuciones y además los intervalos de confianza resultantes son muy amplios y se genera dificultad a la hora de comparar métodos de pronóstico.

2- Al ser la volatilidad una variable latente, cual es el impacto de utilizar determinado estimador para la volatilidad. Selección del estimador de la volatilidad.

3- La forma del $\epsilon_t$ es más importante para la selección de un modelo de volatilidad y qué métricas utilizar en relación a penalizar el sub pronóstico $\hat{X_t}<X_t$ o sobre pronóstico
$\hat{X_t}>X_t$. Entre las métricas que Poon (2005) menciona se encuentran:

Error cuadrático medio (RMSE):

\begin{equation}\large{\mathbf{\sqrt{\frac{1}{n}\sum_{t=1}^N \epsilon_t^2}=\sqrt{\frac{1}{N}\sum_{t=1}^{N}(\hat\sigma_t^2-\sigma_t^2) }}}\label{eq:2}\end{equation}

Que presenta una medida global del error pero no indica la dirección y es aplica la raíz cuadrada para disminuir el efecto de observaciones extremas y el nivel de volatilidad de los retornos. Esto se interpreta en la misma unidad que la variable en estudio. 

Error Absoluto Medio (MAE):

\begin{equation}\large{\mathbf{MAE=\frac{1}{N}\sum_{t=1}^{N}|\hat{\sigma_t}-\sigma_t |}}\label{eq:2}\end{equation}

Los valores ideales de esta métrica aproximan a cero. No brinda una magnitud de la dirección del error porque las diferencias de signo contrario no se cancelan, es una medida de la magnitud del error global de pronóstico. Penaliza los errores en proporción a su magnitud. 

El error porcentual absuluto medio (MAPE):

\begin{equation}\large{\mathbf{MAPE=\frac{1}{N}\sum_{t=1}^N\frac{|\hat{\sigma_t}-\sigma_t|}{\sigma_t}}}\label{eq:2}\end{equation}

No permite dar información sobre la dirección del error y penaliza las diferencias en forma proporcional. 

4- La comparación del error de diferentes modelos. Poon (2005) menciona que la mayoría de las veces las comparaciones entre pronósticos se realizan con métricas estadísticas como las mencionadas en el punto anterior, pero aclara que las métricas están sujetas a ruido y a error. Con esto menciona que si una métrica de error para el modelo A es mayor que una métrica de error para un modelo B no es posible concluir que B es más preciso que A sin realizar tests de significación. West (1996), West y Cho (1995) y West y Mc Craken (1998) muestran como los errores estandard que se utilizan en métricas de comparación pueden haber sido obtenidos con pronosticos con correlación serial o incertidumbre en la estimación de los parámetros de volatilidad. 

Si hay T observaciones en la muestra y T es suficientemente grande, hay 2 maneras de realizar un pronóstico fuera de la muestra. Si se asumen n observaciones para la estimación y se realizan T-n pronósticos. El esquema recursivo de pronósticos comienza con una muestra ${1,...,n}$ y realiza el primer pronóstico en n+1, el segundo pronóstico incluye a la ultima información y forma el conjunto de entrenamiento con ${1,...,n+1}$, y así sucesivamente hasta que el pronóstico para T incluya a todas las observaciones excepto a la última utilizando como conjunto de información ${1,...,T-1}$. Poon (2005) menciona que en la práctica el esquema de pronóstico por fuera de la muestra con barrido de la información es el más utilizado ya que se utiliza un número fijo de observaciones en la estimación. De esta forma el pronóstico para n+2 se hace con la información de conjunto ${2,...,n+1}$ y el último pronóstico se hace basándose en ${T-n,..., T-1}$ lo cual omite utilizar información sobre el pasado distante. También se menciona que este esquema por barrido resulta más eficiente computacionalmente hablando cuando el número de elementos de T es grande. Diebold y Mariano (1995) propusieron 3 tests para comparación de modelos de pronóstico. Los tests incluyen un test asintótico que corrige por correlación serial y dos tests exactos para muestras finitas basados en un test de signos y el test de ranking de signos de Wilcoxon. Los resultados de los mismos por simulación probaron ser robustos contra pronósticos de distribución no normal, contra media distinta de cero y contra correlación serial. Se presentan a continuación los tests.

Primer Test: sea ${\hat{X_{i,t}}_{t=1}^{t}}$ y ${\hat{X_{j,t}}_{t=1}^{t}}$ dos conjuntos de pronósticos de ${X_t}_{t=1}^{T}$ de dos modelos i y j respectivamente. Si los errores asociados a los pronósticos son ${e_{i,t}}_{t=1}^{T}$ y ${e_{j,t}}_{t=1}^{T}$, sea $g(.)$ una función de pérdida (cualquiera de las métricas mencionadas arriba) tal que $g(X_t,\hat{X_{i,t}})=g(e_{i,t})$.
Se define un diferencial de pérdida como $d_t\equiv g(e_{i,t})-g(e_{j,t})$. Entonces, la hipótesis nula es que los 2 pronósticos tienen la misma precisión y la función de diferencial de pérdida es cero, o sea $E(d_t)=0$.

El primer test es para la media:

\begin{equation}\large{\mathbf{\bar{d}=\frac{1}{T}\sum_{t=1}^{T}|g(e_{i,t}-g(e_{j,t})|}}\label{eq:2}\end{equation}

con el estadístico:

\begin{equation}\large{\mathbf{S_1=\frac{\bar{d}}{\sqrt{\frac{1}{T}2\pi\hat{f_d}(0)} }}}\label{eq:2}\end{equation}

con $S1\sim N(0,1)$

\begin{equation}\large{\mathbf{2\pi\hat{f_d}(0)=\sum_{\tau=-(T-1)}^{T-1}1 (\frac{\tau}{S(T)}\hat{\gamma_d}(\tau))}}\label{eq:2}\end{equation}

\begin{equation}\large{\mathbf{\hat{\gamma_d}(\tau)=\frac{1}{T}\sum_{t=|\tau|+1}^{T}(d_t-\bar{d})(d_{t-|\tau|-\bar{d}})}}\label{eq:2}\end{equation}

El operador $1(\tau/S(T))$ es la ventana de retardo y $S(T)$ es el retardo de truncamiento con:

\begin{equation}\large{\mathbf{1(\frac{\tau}{S(T)})=\begin{cases}
 & \text{ 1 si } |\frac{\tau}{S(T)}|\leq1 \\
 & \text{ 0 de cualquier otra forma }  
\end{cases}}}\label{eq:2}\end{equation}

Asumiendo que los pronósticos de $k$ pasos hacia adelante tienen dependencia en a lo sumo $k-1$ se recomienda que $S(T)=k-1$. No es probable que $\hat{f}_d(0)$ sea negativa pero si eso sucede se debe rechazar la hipotesis nula de que ambos pronósticos tienen la misma precisión inmediatamente.

Segundo test: el segundo test de Diebold-Mariano (1995) es el test de signos. El test de signos trabaja sobre la mediana con la hipótesis nula de que $Med(d)=Med(g(e_{i,t})-g(e_{j,t})=0$

Asumiendo que $d_t$ es independiente e idénticamente distribuida el estadístico del test resulta 

\begin{equation}\large{\mathbf{\hat{S2=\sum_{t=1}^{T}I_+(d_t)}}}\label{eq:2}\end{equation}

donde 

\begin{equation}\large{\mathbf{I_+(d_t)=\begin{cases}
 & \text{ 1 si }d_t>0 \\
 & \text{ 0 de cualquier otra forma }\end{cases}
}}\label{eq:2}\end{equation}

Para una muestra pequeña S2 se debe buscar en las tablas de distribución binomial acumulada. En una gran muestra la versión studentizada de S2 es asintóticamente normal. 

\begin{equation}\large{\mathbf{S_{2a}=\frac{S_2-0.5T}{\sqrt{0.25T}~N(0,1)}}}\label{eq:2}\end{equation}

Tercer test: test de rangos de signos de Wilcoxon. Este test está basado en el signo y el rango de la función de pérdida diferencial con estadístico de prueba:

\begin{equation}\large{\mathbf{S3=\sum_{t=1}^{T}I_+(d_t)rank(|d_t|)}}\label{eq:2}\end{equation}

que representa la suma de los rangos de los valores absolutos para las observaciones positivas. Los valores críticos de S3 han sido tabulados para muestras pequeñas. Para muestras más grandes la versión studentizada de S3 es asíntóticamente normal.  

5-Como tener en consideración cuando $X_t$ y $X_{t-1}$ y de manera similar $\epsilon_t$ y $\hat{X_t}$ son solapados y están seriamente correlacionados: el test de eficiencia y ortogonalidad basado en la regresión es, según Poon (2005), el más popular en la evaluación de los pronósticos de volatilidad.

Se basa en la regresión de la volatilidad estimada $X_t$ con la volatilidad pronosticada $\hat{X_t}$

\begin{equation}\large{\mathbf{X_t=\alpha+\beta\hat{X}_{t}+v_{t}}}\label{eq:2}\end{equation}

Con lo que la predicción sería insesgada sólo si $\alpha=0$ y $\beta=1$.

Dado que el término del error $v_t$ es heterocedástico y está correlacionado de manera serial cuando pronósticos superpuestos son evaluados los errores estándar de los parámetros de regresión se computan bajo la teoría de Hansen y Hodrick (1980).

Sea $Y$ la matriz fila de regresores incluyendo el término constante $Y_t=(1 \hat{X}_t)$ es una matrix de 1x2. Entonces, el error estandard se estima con la siguiente expresión: 

\begin{equation}\large{\mathbf{\hat{\Phi}=T^{-1}\sum_{i=1}^{T}v_{t}^2Y'_{t}Y_t+T^{-1}\sum_{k=1}^{T}\sum_{t=k+1}^{T} Q(k,t)v_kv_t (Y'_tY_k+Y'_kY_t)}}\label{eq:2}\end{equation}

donde $v_k$ y $v_t$ son los residuos de la observación k y t de la regresión. El operador $Q(k,t)$ es una función indicadora que toma el valor 1 si hay solapamiento de información entre $Y_k$ e $Y_t$. La matriz de covarianza ajustada entonces para los coeficientes de regresión resulta entonces:

\begin{equation}\large{\mathbf{\hat{\omega}=(Y'Y)^{-1}\hat{\Phi}(Y'Y)^{-1}}}\label{eq:2}\end{equation}








## ARCH:

En los procesos lineales ARIMA tanto la varianza marginal como la varianza condicional del error son constantes. En los procesos financieros pueden darse casos en los que la varianza condicional no es constante y representa la incertidumbre en las predicciones. Estos procesos van a reflejar un riesgo variable (Peña, 2010). Son los modelos de varianza condicional heterocedástica. Engle (1982) introdujo los modelos autorregresivos de varianza condicional heterocedástica (ARCH) donde la varianza condicional depende del pasado con estructura autorregresiva. Poon(2005) destaca que a diferencia de los modelos de volatilidad histórica, los modelos ARCH no utilizan las desviaciones estandar pasadas sino que formulan la varianza condicional de los retornos de un activo financiero con procedimientos de máxima verosimilitud. Los procesos ARCH relajan la hipótesis de normalidad de residuos y permiten procesos de ruido blanco formados por variables dependientes en los residuos. Una clase de modelos con esta propiedad es $e_t=\sigma_t\epsilon_t$, donde $\epsilon_t$ y $\sigma_t$ son procesos estacionarios independientes entre sí. Tsay(2010) menciona que en la práctica $\epsilon_t$ puede asumir una distribución normal, o una t-student estandarizada o una distribución de error generalizada. El proceso $e_t$ es un ruido blanco normal estandarizado. El proceso $\sigma_t$ es estacionario con estructura dinámica siendo su valor en el tiempo función de $e_t-i$ los valores de la serie previos a t. Al ser independientes $\sigma$ y $\epsilon$ la serie de $e_t$ tiene media marginal y media condicional igual a cero.

\begin{equation}\large{\mathbf{E(e_t)=E(\sigma_t)E(\epsilon_t)=0}}\label{eq:2}\end{equation}


\begin{equation}\large{\mathbf{E(e_t|e_{t-1})=E(\sigma_t|e_{t-1})E(\epsilon_t)=0}}\label{eq:2}\end{equation}

El proceso tiene una varianza marginal constante $\sigma^2$ y una varianza condicional que varía con el tiempo. 


\begin{equation}\large{\mathbf{Var(e_t^2|e_{t-1})=E(\sigma_t^2|e_{t-1})E(\epsilon_t^2)=\sigma_t^2}}\label{eq:2}\end{equation}

Donde $\sigma_t^2$ representa la varianza condicionada de la serie en cada instantante que varía con una estructura estacionaria (Peña, 2010). La independencia de los procesos $\sigma_t$ y $\epsilon_t$ garantizan que la serie $e_t$ forma un ruido blanco y no tiene autocorrelación. Como el proceso $\epsilon_t$ es independiente de su pasado y de los valores previos y actuales del proceso $\sigma_t$,
el modelo ARCH(1) tiene una varianza condicional $\sigma_t^2$ que depende sólo del último valor observado mediante la ecuación:

\begin{equation}\large{\mathbf{E(e_t^2|e_{t-1})=\sigma_t^2=\alpha_0+\alpha_1e_{t-1}^2}}\label{eq:2}\end{equation}

Este caso se puede generalizar al ARCH(q) donde la varianza del error depende de q rezagos de los errores al cuadrado, en este caso:

\begin{equation}\large{\mathbf{\sigma_t^2=\alpha_0+\alpha_1e_{t-1}^2+\alpha_2e_{t-2}^2+...+\alpha_qe_{t-q}^2}}\label{eq:2}\end{equation}

Peña(2010) demuestra que el modelo ARCH(1) conduce a distribuciones con colas pesadas por las rachas de valores altos en la varianza de los datos aunque globalmente sea un proceso estacionario. 
Se utiliza la letra $\eta$ para la varianza condicional $\sigma_t^2$. Si se considera un modelo ARMA(1,0) y la ecuación del ARCH(1) se puede escribir el conjunto como:


\begin{equation}\large{\mathbf{y_t=\beta_0+\beta_1y_{t-1}+u_t}}\label{eq:2}\end{equation}

Donde $u_t~N(0,\eta)$ y $\eta_t=w+\alpha_1u_{t-1}^2$

Los modelos ARCH deben complir que la varianza condicional sea siempre positiva. Se debe satisfacer que $w>0$ y que los $\alpha_i>0$ $\forall i=1…q$. Se debe confirmar que existe el comportamiento autorregresivo de la serie identificando si alguno de los $q$ rezagos de los errores al cuadrado resulta significativo. Por último la suma de los parámetros del modelo ARCH no puede ser mayor a 1 porque en ese caso se genera un comportamiento explosivo de la volatilidad y el modelo entonces se vuelve inestable. 

Entre las limitaciones de los modelos ARCH se encuentra que no hay una forma óptima de encontrar el número de rezagos $q$ para el modelo ARCH, existe una relación entre lo parsimonioso que puede resultar el modelo y la cantidad de términos que tenga el modelo en relación justamente a la cantidad de rezagos. Un modelo con mayor cantidad de parámetros resulta menos parsimonioso.

## GARCH:

Los modelos generalizados autorregresivos condicionales heterocedásticos GARCH son una extensión de los modelos ARCH con la diferencia de que $\sigma_t^2$ se vuelve recursivo. Son un desarrollo de Bollerslev (1986) y Taylor (1986), en estos modelos se permite que la varianza condicional sea dependiente de sus propios rezagos. Esto se usa para aproximar un modelo ARCH de orden alto con una media móvil en las varianzas.
El modelo GARCH más simple es el GARCH(1,1) con ecuación:

\begin{equation}\large{\mathbf{\sigma_t^2=\alpha_0+\alpha_1e_{t-1}^2+\beta_1\sigma_{t-1}^2}}\label{eq:2}\end{equation}

Donde $\sigma_t^2$ es la varianza condicional que es una estimación anticipada de la varianza calculada. Este modelo permite interpretar la varianza ajustada $(\eta_t=\sigma_t^2)$,  como una función ponderada de un proceso de largo plazo $\alpha_0$, la información de la volatilidad previa representada por $\alpha_1e_{t-1}^2$ y la varianza ajustada del modelo del período anterior $\beta_1\sigma_{t-1}^2$ (por esto el caracter recursivo mencionado previamente).
Si se escribe el modelo en la representación de los cuadrados de las variables, introduciendo el proceso de ruido blanco $v_t=e_t^2-\sigma_t^2$ que es por construcción un proceso estacionario de media cero, de la misma forma que en los modelos ARCH las variables $v_t$ a pesar de no estar correlacionadas no son independientes. Si se sustituye $\sigma_t^2=e_t^2-v_t$ en la ecuación del modelo GARCH (1,1) se obtiene:

\begin{equation}\large{\mathbf{e_t^2=\alpha_0+(\alpha_1+\beta_1)e_{t-1}^2+v_t+\beta_1v_{t-1}}}\label{eq:2}\end{equation}

Ecuación que muestra que los cuadrados de las observaciones siguen una estructura de dependencia similar a un ARMA(1,1]). El coeficiente $\lambda=(\alpha_1+\beta_1)$ se llama persistencia y en series financieras es próximo a la unidad (Peña, 2010).
La generalización de el modelo GARCH se puede generalizar con una estructura autorregresiva de orden r en la dependencia de la varianza de los cuadrados de la serie y una estructura de medias móviles de orden s en la varianza condicional obteniéndose el modelo:

\begin{equation}\large{\mathbf{\sigma_t^2=\alpha_0+\sum_{i=1}^r\alpha_ie_{t-i}^2+\sum_{j=1}^s\beta_j\sigma_{t-j}^2}}\label{eq:2}\end{equation}


Los parámetros de la ecuación deben verificar ciertas restricciones para que la varianza de la ecuación sea positiva y existan los momentos de órden superior, $\alpha_0>0$, $\alpha_i>0$, $\beta_i>0$ y $\sum_{i=1}^{max r,s} (\alpha_i+\beta_i) <1$ (Peña, 2010).

Para la construcción de modelos ARCH o GARCH Peña (2010) recomienda seguir tres pasos fundamentales: reconocer la estructura, estimar los parámetros y comprobar su adecuación a los datos. Primero se ajusta un modelo ARIMA para eliminar la dependencia en la media, si los residuos no son independientes se observa ese comportamiento en la función de autocorrelación de los residuos al cuadrado del modelo ARIMA. La cantidad de residuos significativos en la FACP de los residuos al cuadrado permite aproximar la cantidad de términos del modelo ARCH. Peña (2010) recomienda utilizar un tratamiento a los valores atípicos previo al modelado de la heterocedasticidad condicional para evitar la confusión entre los efectos de los valores atípicos y la volatilidad que se intenta explicar con los ARCH o GARCH.

## EGARCH:

Nelson (1991) propone el modelo exponencial GARCH con el objetivo de evitar una de las debilidades de los modelos ARCH y GARCH que es permitir los efectos asimétricos de los retornos positivos y negativos en la modelización de la volatilidad.

Considerando una innovación ponderada:

\begin{equation}\large{\mathbf{g(\epsilon_t)=\theta\epsilon_t+\gamma[|\epsilon_t|-E(\epsilon_t)]}}\label{eq:2}\end{equation}

Donde $\theta$ y $\gamma$ son constantes reales. Tanto $\epsilon_t$ como $|\epsilon_t|-E(\epsilon_t)$ tienen media cero y son independientes e identicamente distribuidas con distribuciones continuas. Con esto $E[g(\epsilon_t)]=0$. Si se reescribe la ecuación de una innovación ponderada del siguiente modo se puede ver la asimetría que describe:

\begin{equation}\large{\mathbf{g(\epsilon_t) \left\{\begin{matrix}
(\theta+\gamma)\epsilon_t-\gamma E(|\epsilon_t)|)\text{ } si\text{ } \epsilon_t\geq0 \\ (\theta-\gamma)\epsilon_t-\gamma E(|\epsilon_t)|)\text{ } si\text{ } \epsilon_t<0
\end{matrix}\right.}}\label{eq:2}\end{equation}

Se define un modelo EGARCH(p,q) con las ecuaciones a continuación:

\begin{equation}\large{\mathbf{a_t=\sigma_t\epsilon_t}}\label{eq:2}\end{equation}

\begin{equation}\large{\mathbf{ln(\sigma_t^2=\omega+\frac{1+\beta_1B+...+\beta_{p-1}B^{p-1}}{1-\alpha_1B-...-\alpha_qB^q}g(\epsilon_{t-1}))}}\label{eq:2}\end{equation}

Donde $\omega$ es constante, B es el operador de retardo, y $1+\beta_1B+...+\beta_{p-1}B^{p-1}$ y $1-\alpha_1B-...-\alpha_qB^q$ son polinomios con raíces fuera del círculo unitario y no tienen factores comunes. La ecuación utliza parametrización ARMA para describir la varianza condicional de la innovación $a_t$. La media de la varianza no condicional es $\omega$.

Las diferencias entre este modelo y el GARCH son que utiliza el logaritmo de la varianza condicional para relajar la restricción de positividad de los coeficientes del modelo y el uso de la función $g(\epsilon_t)$ para responder de manera asimétrica a valores positivos y negativos de las innovaciones.  


## Métricas de comparación:

La idea general de todas las métricas utilizadas en este presente trabajo es comparar la situación observada en el fenómeno que se desarrolla en la serie de tiempo contra lo que los distintos modelos estiman y calcular alguna medida de error sobre esa comparación.

*Raíz del error cuadrático medio*:

Toma las diferencias al cuadrado entre los valores observados y predichos, a través de todos los datos y los promedia:

\begin{equation}\large{\mathbf{MSE=\sum_{i=1}^n\frac{(\hat{y}_i-y_i)^2}{n}}}\label{eq:20}\end{equation}

En el presente trabajo se utiliza para comparación la raíz cuadrada o RMSE:

\begin{equation}\large{\mathbf{RMSE=\sqrt{\sum_{i=1}^n\frac{(\hat{y}_i-y_i)^2}{n}}}}\label{eq:20}\end{equation}

*Desviación media absoluta:*

Se define entonces la desviación media absoluta:

\begin{equation}\large{\mathbf{MAE=\sum_{i=1}^n\frac{|\hat{y}_i-y_i|}{n}}}\label{eq:20}\end{equation}

*Error porcentual medio absoluto:*

El MAPE es un indicador que mide el tamaño del error absoluto de fácil interpretación porque es una medida relativa.

\begin{equation}\large{\mathbf{MAPE=\frac{100\%}{n}\sum_{i=1}^n|\frac{y_i-\hat{y}_i}{y_i}|}}\label{eq:20}\end{equation}

En las 3 métricas definidas arriba los valores de $\hat{y_i}$ corresponden al valor pronosticado para el período $i$ y $y_i$ es el valor observado (real) de la variable en el período $i$.

## Validación Cruzada:

Para los pronósticos del presente trabajo se aplica además un esquema de validación cruzada con el objetivo de obtener una comparación más robusta entre modelos. Se elige un esquema de validación cruzada por bloques, en donde se selecciona distintos intervalos de tiempo dentro de la historia de la serie, se pronostica en los plazos identificados en los objetivos y se promedian las métricas obtenidas para cada bloque en cada modelo aplicado.

Gráficamente, la validación cruzada por bloques se muestra a continuación:


```{r,graffoto19,fig.align='center', fig.height=4,fig.width=5,fig.cap="Esquema de valización Cruzada. Fuente: Elaboración propia.", fig.scap="Esquema de valización cruzada.", echo=F, message=F}

library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\graficocvporbloques.jpg")


```


En la figura se observa como se configuran diferentes intervalos temporales (bloques) en los que se entrenan los modelos (zona marcada en azul) y luego se realizan pronósticos (zonas marcadas en rojo). Luego las métricas son calculadas contra los valores reales de la serie y se realizan los promedios.

En las pruebas que se realizan en el marco empírico se confeccionan pronósticos a 5, 15 y 30 días para la serie de precios de cierre de los primeros días de 2022 y luego se informan los resultados obtenidos con el esquema de validación cruzada para tener otro elemento de comparación.

\newpage

# 6-Marco Empírico: 

En el estudio de los métodos de pronóstico para la serie de precios de cierre diario de ETH/USD y la volatilidad de ETH/USD se trabaja con la aplicación empírica de lo desarrollado en el marco teórico para generar las predicciones que luego serán comparadas.

Como primer punto se desarrolla un análisis descriptivo de la serie de precios de cierre diario de ETH/USD. En el presente trabajo de tesis se desarrollan modelos sobre series de tiempo univariantes, todos los modelos trabajan sobre el comportamiento histórico de la misma serie, aunque en el caso de ARIMA, ARCH, GARCH y EGARCH se aplica la transformación de la serie de precios de cierre diario a retornos logarítmicos.  

Luego se construyen los modelos ETS, ARIMA, ARFIMA,  NNETAR y LSTM, se generan los pronósticos para los precios de cierre en los horizontes fijados  y los modelos ARCH, GARCH, EGARCH para el pronóstico de la volatilidad de la serie de retornos logarítmicos diarios. Se registran los resultados de las métricas obtenidas.

\newpage
## Análisis Exploratorio:

Se presenta el gráfico de la serie de precios de cierre diario de Ether en la Figura 6 : 

```{r grafCuanti, fig.height=5, fig.width=7.5, fig.scap="Precios de cierre diario de ETH/USD (2017-2021)",fig.cap="Precios de cierre diario de ETH/USD (2017-2021). Fuente: Elaboracion Propia ",echo=F}

eth<-read.csv('C:/MEA/Curso 10-Series de Tiempo/dataETH.csv',header=TRUE,sep=',')
ethTS<-ts(eth$close, start=c(2017,01,01),end=c(2022,01,01),frequency = 365)

ethTEST<-read.csv('C:/MEA/Tesis/data.csv',header=TRUE,sep=',')
ethTSTEST<-ts(ethTEST$close, start=c(2017,01,01),frequency = 365)


## ESTADÃSTICAS BÃSICAS - MEDIDAS
#summary(ethTS)

## VISUALIZACIÃN DE LA SERIE

autoplot(ethTS) + xlab("Tiempo")+
  ylab("USD") + theme_bw()

```

En la Figura anterior se muestran los precios de cierre Ether (ETH/USD) entre el 01/01/2017 y el 01/01/2022. 

Se observa un comportamiento no estacionario de la serie, con una marcada tendencia ascendente a partir del 2021, un período de precios de cierre entre los 250 y 800 dólares que se extiende entre los años 2019 y 2021. También se aprecia otra etapa de fuerte crecimiento desde 2017 hasta el fin del primer cuarto del 2018 para luego una caída del precio hasta comienzos del 2019. Se observan períodos de baja del precio durante los meses del segundo cuarto de 2018 y en el segundo cuarto de 2021, donde las bajas fueron superiores al $100\%$. Por otro lado en el período que comienza en el último cuarto de 2019 hasta el comienzo del año 2021 la serie presenta menor variabilidad y los precios se ubican entre los 200 y los 500 dólares.

A continuación se estudia el gráfico de la función de autocorrelación de la serie de precios de cierre diario. En la FAC se observa el comportamiento de disminución lenta de los valores de autocorrelación típico de las series no estacionarias. 

```{r grafCuanti4, fig.height=4, fig.width=5.5, fig.align='center',fig.scap="Gráfico de FAC de precios de cierre de ETH/USD",fig.cap="Gráfico de FAC de precios de cierre de ETH/USD. Fuente: Elaboración Propia.",echo=F}
library(forecast)
g1 <- ggAcf(ethTS, type = "correlation",lag.max = 200) + 
  ggtitle("")+labs(x="Retardo[días]",y="FAC")
g1

```


Para testear la estacionariedad de la serie de precios de cierre se realizan las pruebas de raíz unitaria Aumentada de Dickey-Fuller y KPSS (Kwiatkowski,Phillips,Schmidt,Shin). El valor p del la prueba de Dickey Fuller aumentada es $p=1.809.10^{-14}$ con lo cual se rechaza la hipótesis nula de estacionariedad de la serie. Del mismo modo el test de KPSS rechaza la hipótesis nula de estacionariedad a un nivel de significación menor al $1\%$.

```{r,include=F,eval=F}
## TEST DE RAÃZ UNITARIA
library(urca)
urdftest_lag = floor(12*(length(ethTS)/100)^0.25)

#?ur.df
ur.df(ethTS, lags = urdftest_lag, type = "trend")
a<-summary(ur.df(ethTS, lags = urdftest_lag, type = "trend"))

a<-tibble::tibble(a)
## la prueba de Dickey Fuller estÃ¡ sesgada hacia el no rechazo de la hipÃ³tesis nula

## la prueba KPSS tendrÃ¡ la hipÃ³tesis de estacionariedad de tendencia como nula 
## es decir, tendencia determinista con residuos estacionarios

ur.kpss(ethTS, lags = "long", type = "tau")
summary(ur.kpss(ethTS, lags = "long", type = "tau"))


#Serie de log-retornos:
ur.kpss(diff(log(ethTS)), lags = "long", type = "tau")
summary(ur.kpss(diff(log(ethTS)), lags = "long", type = "tau"))

ur.df(diff(log(ethTS)), lags = urdftest_lag, type = "trend")
a2<-summary(ur.df(diff(log(ethTS)), lags = urdftest_lag, type = "trend"))


#Log Retornos diferenciada:
autoplot(diff(diff(log(ethTS))))

ur.kpss(diff(diff(log(ethTS))), lags = "long", type = "tau")
summary(ur.kpss(diff(diff(log(ethTS))), lags = "long", type = "tau"))


```


Las medidas básicas de resumen de los datos de la serie de precios de cierre diario en estudio son las siguientes:

```{r, echo=F,warning=F}

# aux_5<-as.data.frame(t(eda(ethTS)))
# 
# colnames(aux_5)<-c("Tamaño (n)", "Faltantes", "Mínimo", "1erQ","Media","Mediana","Media Truncada","3erQ","Máximo","Desvío Std","Varianza","Error Estandar Media","Rango Intercuartílico","Rango","Kurtosis","Asimetría", "Valor p SW")

# aux_5<-read_xlsx("~/resumen_estadisticos_precio_cierre.xlsx")

aux_5<- data.frame(
  Medidas = c("Tamaño (n)", "Faltantes", "Mínimo", "1erQ", "Media", "Mediana", "3erQ", "Valor p SW"),
  Valores = c("1826", "0", "7.9800", "177.0060", "794.5610", "292.8530", "719.8280", "0.0000"),
  `Medidas (cont.)` = c("Máximo", "Desvío Std", "Varianza", "Error Estandar Medio", "Rango Intercuartílico", "Rango", "Kurtosis", "Asimetría"),
  `Valores (cont.)` = c("4891.7000", "1110.2900", "1232743.0570", "25.9830", "542.8220"," 4803.9150", "2.7900", "1.9700")
)

knitr::kable(aux_5,caption = "Resumen de estadísticos descriptivos para precio de cierre de ETH/USD. Elaboración Propia",caption.short="Resumen de estadísticos descriptivos para precio de cierre de ETH/USD."
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)


```

En esta tabla se observa la variabilidad que presenta el precio de cierre de ETH, donde el mínimo valor registrado es $7.98$ USD y el precio más alto registrado en el momento del estudio es $4891.70$ USD. La serie cuenta con 1826 observaciones sin valores faltantes. 

La distribución de los valores de cotización de cierre de Eth/USD en el período en estudio (01-01-2017 a 01-01-2022) tiene una distribución asimétrica a la derecha como se muestra en la figura:

```{r grafCuanti2, fig.align='center',fig.height=4, fig.width=5.5, fig.cap="Gráfico de distribución de densidad. Precios de cierre de ETH/USD. Elaboración Propia.",fig.scap="Gráfico de distribución de densidad. Precios de cierre de ETH/USD.",echo=F, message=FALSE,warning=FALSE}
#tt <- 1:length(ethTS)
#fit <- ts(loess(ethTS ~ tt, span = 0.2)$fitted, start = 1880, frequency = 1)
#plot(ethTS, type='l')
#lines(fit, col = 4)
#grid()


qr<-as.data.frame(ethTS)
ggplot(qr, aes(x = x)) + geom_density()+labs(x="N=1826 Ancho de Banda=81.1900", y="Densidad")


```

En la tabla a continuación se muestran los valores máximos y mínimos de cotización para cada año en estudio junto con el porcentaje de variación anual.

```{r, echo=F,warning=F,include=F}
a<-as.data.frame(read.csv("C:\\Users\\Sebastian\\Documents\\datos_ETH_anuales.csv", sep=" "))


a$open<-round(a$open,4)
a$close<-round(a$close,4)
a$low<-round(a$low,4)
a$high<-round(a$high,4)
a$year_change<-round(a$year_change,4)

b<-dplyr::rename(a,Año=X,Apertura=open,Cierre=close,Mínimo=low,Máximo=high,"Cambio %"=year_change)

b$Apertura<-round(b$Apertura,2)
b$Cierre<-round(b$Cierre,2)
b$Mínimo<-round(b$Mínimo,2)
b$Máximo<-round(b$Máximo,2)

```

```{r,echo=F,warning=F}
knitr::kable(b,caption = "Resumen de precios anuales ETH/USD. Elaboración Propia.",caption.short="Resumen de precios anuales ETH/USD. Elaboración Propia."
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

\newpage
Se observa en la tabla el crecimiento explosivo del primer año de estudio, con una corrección a la baja marcada en el segundo año. Durante los años 2020 y 2021 otra fase de crecimiento explosivo.

Para el trabajo empirico con el modelo ARIMA y los modelos ARCH, GARCH y EGARCH se utiliza la serie de retornos logarítmicos de precios de ETH/USD. Se define la misma como:

\begin{equation}\large{\mathbf{Z_t=log(P_t)-log(P_{t-1})}}\end{equation}

Donde $Z_t$ es el retorno logarítmico en el tiempo t, $P_t$ y $P_{t-1}$ son los precios de cierre en el tiempo $t$ y el día anterior $(t-1)$.

La serie del retorno logarítmico para los precios de cierre de ETH/USD:

```{r grafCuanti255, fig.height=4, fig.width=8, fig.cap="Gráfico de Retornos logarítmicos de ETH/USD. Fuente: Elaboración Propia",fig.scap="Gráfico de Retornos logarítmicos de ETH/USD.",echo=F, message=FALSE,warning=FALSE }

g76<-readRDS("~/g70.RDS")
g76
```

La Figura 4 muestra el comportamiento de los retornos logarítmicos del precio de ETH/USD. En general la serie aparenta presentar el comportamiento estacionario esperable en los hechos estilizados de retornos de activos financieros, aunque se observa un valor atípico en el año 2020 y los tests de KPSS y Dickey Fuller Aumentado rechazan la hipotesis de estacionariedad. No se observa comportamiento cíclico ni estacional en esta serie.

```{r, grafCuanti256, fig.height=4,fig.width=6, fig.cap="Distribución de densidad de retornos logarítmicos de ETH/USD vs normal. Diagrama QQ para retornos logarítmicos ETH/USD. Fuente: Elaboración Propia.",echo=F,message=F,warning=F}

#mean(diff(log(ethTS)))
#sd(diff(log(ethTS)))

qr2<-as.data.frame(diff(log(ethTS)))
g77<-ggplot(qr2, aes(x = x)) + geom_density()+stat_function(
    fun = dnorm,
    args = list(mean = mean(diff(log(ethTS))), sd = sd(diff(log(ethTS)))),
    color = "blue",
    linetype = "dashed"
  )+labs(x="N=1825  Ancho de Banda=0.0075", y="Densidad")

g78<-ggplot(qr2, aes(sample=x)) +
  stat_qq() +
  stat_qq_line()+ylab ("Valores Observados")+xlab("Valores Teóricos")

(g77|g78)
```

En las Figuras precedentes se puede observar el comportamiento leptocúrtico de los retornos logarítmicos del precio de ETH/USD, lo que concuerda con lo mencionado en el marco teórico como hecho estilizado.

```{r}

aux_6<-read_xlsx("~/resumen_estadisticos_retornos.xlsx")

aux_6<- data.frame(
  Medidas = c("Tamaño (n)", "Faltantes", "Mínimo", "1erQ", "Media", "Mediana", "3erQ", "Valor p SW"),
  Valores = c("1825", "0", "-0.5510", "-0.0210", "0.0030", "0.0020", "0.0290", "0"),
  `Medidas(cont.)` = c("Máximo", "Desvío Std", "Varianza", "Error Estandar Media", "Rango Intercuartílico", "Rango", "Kurtosis", "Asimetría"),
  `Valores(cont.)` = c("0.2788", "0.0560", "0.0030", "0.0010", "0.0500", "0.8410", "8.6130", "-0.4420")
)


knitr::kable(aux_6,caption = "Resumen de estadísticos descriptivos para retorno logarítmico ETH/USD. Elaboración Propia",caption.short="Resumen de estadísticos descriptivos para retorno logarítmico ETH/USD."
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)

```

La serie de retornos logaritmicos de ETH/USD contiene 1825 valores (por utilizar una diferenciación tiene un valor menos), no presenta faltantes, se rechaza con un valor p de 0 el supuesto de normalidad en la distribución mediante la prueba de Sapiro Wilk. El valor medio de 0.0030 muy cercano a cero es característico de los retornos logarítmicos de series financieras. 

\newpage

## Modelo ETS:

A continuación se realiza el modelado y pronóstico de la serie de precios de cierre de ETH/USD con el método ETS y el paquete "Forecast" en R.

En primer lugar se produce la selección de un modelo apropiado para la serie de tiempo. En base a los criterios desarrollados en el marco teórico el paquete Forecast (versión 8.18) en el software R determina cual es mejor modelo dentro de la familia de  suavizado exponencial para realizar el pronóstico.

```{r,echo=F,include=F}
# create a tsibble w/o a key

ethTS2<-as.tibble(ethTS)

prueba<-tsibble(
  date = as.Date("2017-01-01") + 0:1825,
  value = ethTS2$x
)
prueba2<-prueba %>% filter (date>"2021-06-01")
fit<-prueba %>% model(ETS(value)) %>% report(fit)

```

Se selecciona para toda la serie un modelo que contempla error multiplicativo, tendencia aditiva y no contempla componente estacional, ETS(M,A,N).

Las ecuaciones del modelo son:

\begin{equation}\large{\mathbf{l_t=\alpha y_t+(1-\alpha)(l_{t-1}+b_{t-1})}}\end{equation}

\begin{equation}\large{\mathbf{b_t=\beta(l_t-l_{t-1})+(1-\beta) b_{t-1}}}\end{equation}

\begin{equation}\large{\mathbf{\hat{y}_{t+h|t}=l_t+hb_t}}\end{equation}

Donde en este caso $\alpha=0.9546$, $\beta=0.0023$, los estados iniciales $l_0=7.8559$, $b_0=0.2812$. 

Los criterios de información AIC=24683.5000, AICc=24683.5300 y BIC=24711.0500.

La representación de los componentes que identifica el modelo se observa en el gráfico a continuación:

```{r,grafCuanti6, fig.align='center',fig.height=7.5,fig.width=5.5, fig.cap="Componentes del modelo ajustado ETS(M,A,N). Fuente: elaboración propia.",fig.scap="Componentes del modelo ajustado ETS(M,A,N).",echo=F}

g82<-components(fit) %>% autoplot() 

g82$data<-g82$data %>% mutate(data,.var=case_when(
  .var=="value"~"Valor",
  .var=="level"~"Nivel",
  .var=="slope"~"Tendencia",
  .var=="remainder"~"Error",
  TRUE~.var)) 
g82$labels$title<-"Descomposición ETS(M,A,N)"
g82$labels$subtitle<-""


g82$layers[[2]]<-NULL
g82$layers

g82 + labs(x="Año")





```

La descomposición precedente muestra que el modelo seleccionado tiene una combinación de tendencia y nivel aditivos junto con un componente de error multiplicativo.

A continuación se realiza el pronóstico para 5, 15 y 30 días para los primeros días del mes de enero de 2022. 

Las gráficas a continuación muestran los pronósticos calculados.

```{r, grafCuanti7, fig.height=4, fig.width=6.5, fig.cap="Pronóstico de precio de cierre de ETH/USD con horizonte de 5 días con modelo ETS(M,A,N). Fuente: elaboración propia.",fig.scap="Pronóstico de precio de cierre de ETH/USD con horizonte de 5 días con modelo ETS(M,A,N)",echo=F}
g3<-fit %>%
  forecast(h = "5 days") %>%
  autoplot(prueba2)+
  labs(title="Precio de cierre de ETH/USD (h=5 días)",
      x="2021-2022", y="USD",level="Nivel")
g3


forecast_ets_5_d<-fit %>%
  forecast(h = "5 days")

forecast_ets_15_d<-fit %>%
  forecast(h = "15 days")

forecast_ets_30_d<-fit %>%
  forecast(h = "30 days")


predicted_ets<-forecast_ets_30_d$.mean
actual_ets<-ethTEST$close[1827:1856]

#rmse(actual_ets, predicted_ets)
#mae(actual_ets, predicted_ets)
#mape(actual_ets, predicted_ets)

```

```{r, grafCuanti8, fig.height=4, fig.width=6.5, fig.cap="Pronóstico de precio de cierre de ETH/USD con horizonte de 15 días con modelo ETS(M,A,N).Fuente: elaboración propia.",fig.scap="Pronóstico de precio de cierre de ETH/USD con horizonte de 15 días con modelo ETS(M,A,N).",echo=F}
g4<-fit %>%
  forecast(h = "15 days") %>%
  autoplot(prueba2)+
  labs(title="Precio de cierre de ETH/USD (h=15 días)",
      x="2021-2022", y="USD",level="Nivel")
g4

pronostico_ets_5<-fit %>% forecast(h="5 days")
pronostico_ets_15<-fit %>% forecast(h="15 days")
pronostico_ets_30<-fit %>% forecast(h="30 days")

actual_ets_30<-ethTEST$close[1827:1856]
actual_ets_15<-ethTEST$close[1827:1841]
actual_ets_5<-ethTEST$close[1827:1831]

#Mincer-Zarnowsky
# mz_ets_30<-lm(pronostico_ets_30$.mean~actual_ets_30)
# car::linearHypothesis(mz_ets_30, c("(Intercept) = 0", "actual_ets_30 = 1")) 
# summary(mz_ets_30)
# 
# mz_ets_15<-lm(pronostico_ets_15$.mean~actual_ets_15)
# car::linearHypothesis(mz_ets_15, c("(Intercept) = 0", "actual_ets_15 = 1")) 
# summary(mz_ets_15)
# 
# mz_ets_5<-lm(pronostico_ets_5$.mean~actual_ets_5)
# car::linearHypothesis(mz_ets_5, c("(Intercept) = 0", "actual_ets_5 = 1")) 
# summary(mz_ets_5)




```

```{r, grafCuanti9, fig.height=4, fig.width=6.5, fig.cap="Pronóstico de precio de cierre de ETH/USD con horizonte de 30 días con modelo ETS(M,A,N).Fuente: elaboración propia.",fig.scap="Pronóstico de precio de cierre de ETH/USD con horizonte de 30 días con modelo ETS(M,A,N).",echo=F}
g4<-fit %>%
  forecast(h = "30 days") %>%
  autoplot(prueba2)+
  labs(title="Precio de cierre de ETH/USD (h=30 días)",
      x="2021-2022", y="USD",level="Nivel")
g4


```




Los residuos del modelo ETS ajustado se muestran en la gráfica a continuación:

```{r,include=F, echo=F, message=FALSE}
aux_ets<-fit %>%  gg_tsresiduals()


g79<-aux_ets[[1]]+labs(x="Fecha",y="Residuos de las innovaciones")
g80<-aux_ets[[2]]+labs(x = "Rezagos [1 día]",y="FAC")
g81<-aux_ets[[3]]+labs(x="Residuos",y ="Cuenta")

readr::write_rds(g79, here::here('g79.RDS'))
readr::write_rds(g80, here::here('g80.RDS'))
readr::write_rds(g81, here::here('g81.RDS'))
```



```{r,grafCuan, fig.height=7.5, fig.width=6.5, fig.cap="Residuos modelo ETS(M,A,N).Fuente Elaboración propia.",fig.scap="Residuos modelo ETS(M,A,N).",echo=F,message=F,warning=F}



(g79/
 g80/
 g81)

```

Se observa que los residuos presentan un comportamiento de ruido blanco, la función de autocorrelación de residuos presenta valores inferiores a 0.0500, lo cual es indicio de que existe leve autocorrelación de residuos y la distribución de los residuos es leptocúrtica. 


Las métricas para el modelo ETS(M,A,N) en el periodo y los horizontes seleccionados son:

*Horizonte de 5 días*: pronóstico desde el 1/1/2022 al 5/1/2022: 

```{r, echo=F,warning=F}
tabla_fcst_ets_5_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c(111.4981,106.0277,2.8501)
                                      )

knitr::kable(tabla_fcst_ets_5_d,
        caption = "Resumen de métricas pronóstico a 5 días ETH/USD (cierre diario). Fuente: Elaboración propia.", caption.short="Resumen de métricas pronóstico a 5 días ETH/USD (cierre diario) modelo ETS"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 10)
```

*Horizonte de 15 días*: pronóstico desde el 1/1/2022 al 15/1/2022:

```{r, echo=F,warning=F}
tabla_fcst_ets_5_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c(410.9421,359.1125,11.0130)
                                      )

knitr::kable(tabla_fcst_ets_5_d,
        caption = "Resumen de métricas pronóstico a 15 días ETH/USD (cierre diario). Fuente: Elaboración propia", caption.short="Resumen de métricas pronóstico a 15 días ETH/USD (cierre diario) modelo ETS"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 10)
```

*Horizonte de 30 días*: pronóstico desde el 1/1/2022 al 30/1/2022:

```{r, echo=F,warning=F}
tabla_fcst_ets_5_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c(839.4285,709.9851,25.9284)
                                      )

knitr::kable(tabla_fcst_ets_5_d,
        caption = "Resumen de metricas pronóstico a 30 días ETH/USD (cierre diario). Fuente: Elaboración propia", caption.short="Resumen de metricas pronóstico a 30 días ETH/USD (cierre diario) metodo ETS."
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 10)
```


```{r,eval=F, include=F}
a<-prueba %>% 
  slice(1:(n()-1)) %>% 
  stretch_tsibble(.init = 300,.step=200)

prueba %>% 
  slice(1:(n()-1)) %>% 
  stretch_tsibble(.init = 300,.step=200) %>% 
  model(ETS(value)) %>% 
  forecast(h = 30) %>% 
  fabletools::accuracy(prueba) %>% 
  select(.model, RMSE:MAPE)
```

Al trabajar con el modelo de validación cruzada propuesto en el marco teórico se subdivide la serie en tramos de 300 observaciones consecutivas, separadas entre sí por 200 observaciones y en cada una de los tramos definidos se calcula el pronóstico en los horizontes fijados, las métricas con respecto a la serie original y finalmente se promedian las métricas para tener los valores que se muestran a continuación. 

En el gráfico a continuación se muestra el esquema de validación cruzada aplicado para el horizonte de 30 días. En los otros horizontes de pronóstico los intervalos seleccionados son los mismos, con la variación obvia del intervalo de testeo.

```{r,grafCuanti11, fig.height=12, fig.width=9, fig.cap="Esquema de validación cruzada (h=30 días). Fuente: Elaboración propia", fig.scap="Esquema de validación cruzada (h=30 días)",echo=F,message=FALSE,erro=F,warning=FALSE}


# DATA ----
prueba3 <- prueba %>% mutate(id == "1")

prueba3<-as.tibble(prueba3)

# RESAMPLE SPEC ----
resample_spec <- time_series_cv(data = prueba3,
                                initial     = 300,
                                assess      = 15,
                                skip        = 200,
                                cumulative  = FALSE,
                                slice_limit = 8)



g94<-resample_spec %>% 
  tk_time_series_cv_plan() %>% mutate(.key = case_when(
    .key == "training" ~ "Entrenamiento",
    .key == "testing" ~ "Testeo",
    TRUE ~ .key
  ),.id = case_when(
    .id=="Slice1"~"Conjunto 1",
    .id=="Slice2"~"Conjunto 2",
    .id=="Slice3"~"Conjunto 3",
    .id=="Slice4"~"Conjunto 4",
    .id=="Slice5"~"Conjunto 5",
    .id=="Slice6"~"Conjunto 6",
    .id=="Slice7"~"Conjunto 7",
    .id=="Slice8"~"Conjunto 8",
    TRUE~.id
    
   )) %>% 
  timetk::plot_time_series_cv_plan(date,value,.interactive=FALSE,.title="")

g94$labels$colour<-"Leyenda"
g94





#resample_spec

# VISUALIZE CV PLAN ----

# Select date and value columns from the tscv diagnostic tool
#resample_spec %>% tk_time_series_cv_plan()

# Plot the date and value columns to see the CV Plan
#resample_spec %>%
#    timetk::plot_time_series_cv_plan(date, value, .interactive = #FALSE,.title = "")

```


Obteniéndose los siguientes promedios 

Para un pronóstico de precio de cierre con horizonte de 5 días:

```{r, echo=F,warning=F}
tabla_fcst_ets_5_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("92.8000","35.8000","4.8600")
                                      )

knitr::kable(tabla_fcst_ets_5_d,
        caption = "Resumen de metricas de CV en pronóstico a 5 días ETH/USD (cierre diario). Fuente: Elaboración propia", caption.short="Resumen de metricas de CV en pronóstico a 5 días ETH/USD (cierre diario) método ETS"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

Para un pronóstico a 15 días:

```{r, echo=F,warning=F}
tabla_fcst_ets_5_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("148.0100","72.8100","9.4900")
                                      )

knitr::kable(tabla_fcst_ets_5_d,
        caption = "Resumen de metricas de CV en pronóstico a 15 días ETH/USD (cierre diario). Fuente: Elaboración propia", caption.short="Resumen de metricas de CV en pronóstico a 15 días ETH/USD (cierre diario) método ETS"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

Para un pronóstico a 30 días:

```{r, echo=F,warning=F}
tabla_fcst_ets_5_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("158.0100","96.3100","15.8100")
                                      )

knitr::kable(tabla_fcst_ets_5_d,
        caption = "Resumen de metricas de CV en pronóstico a 30 días ETH/USD (cierre diario). Fuente: Elaboración propia", caption.short="Resumen de metricas de CV en pronóstico a 30 días ETH/USD (cierre diario) método ETS."
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```



\newpage
## Modelo ARIMA:

Retomando la gráfica de la serie de precios de cierre de ETH/USD para el período en estudio se identifico en el análisis exploratorio la presencia de raíz unitaria y la necesidad de diferenciar la serie. 

Dada la naturaleza no estacionaria de la serie, se procede de acuerdo a la metodología planteada por Hyndman y Athanasopoulos (2018) y se estudian las propiedades del la primer diferencia del logaritmo de la serie, o retorno logarítmico, definida como 
ya se mencionó anteriormente como $Z_t=log(P_t)-log(P_{t-1})$.

Donde $P_t$ es el precio de cierre en el día $t$. Se interpreta en base a la ecuación precedente que la serie diferenciada tiene una observación menos (porque en el momento inicial no se puede calcular). Se grafica a continuación la serie diferenciada del logaritmo del precio cierre de ETH en dólares estadounidenses partiendo desde el 02/01/2017 al 01/01/2022, conocida como retorno logarítmico. Se aplica la transformación logaritmo a los datos, además de la diferenciación para estabilizar la varianza.

```{r,include=FALSE,eval=TRUE}
log_returns<- c(NA,diff(log(prueba$value)))



prueba$LogRetornos<-log_returns

aux<-prueba %>% 
  gg_tsdisplay(LogRetornos,plot_type = "partial")

g70<-aux[[1]]+labs(x = "Año",y="Retornos Logaritmicos ETH/USD")

g71<-aux[[2]]+labs(x = "Rezagos [1 día]",y="FAC")

g72<-aux[[3]]+labs(x = "Rezagos [1 día]",y="FACP")


readr::write_rds(g70, here::here('g70.RDS'))
readr::write_rds(g71, here::here('g71.RDS'))
readr::write_rds(g72, here::here('g72.RDS'))



```


```{r, grafCuanti12, fig.height=4, fig.width=7,fig.cap="Gráfico de serie diferenciada de logaritmo de precios de cierre de ETH/USD (2017-2022). Fuente: Elaboración Propia.",fig.scap="Gráfico de serie diferenciada de logaritmo de precios de cierre de ETH/USD (2017-2022)",echo=F}

g70<-readRDS("~/g70.RDS")
g71<-readRDS("~/g71.RDS")
g72<-readRDS("~/g72.RDS")


g70

# Se vuelve a testear raíces unitarias con test de Dickey Fuller Aumentado:
#urdftest_lag2 = floor(12*(length(diff(ethTS))/100)^0.25)
#ur.df(diff(ethTS), lags = urdftest_lag2, type = "none")
#summary(ur.df(diff(ethTS), lags = urdftest_lag2, type = "none"))
#Mediante KPSS:
#ur.kpss(diff(log(ethTS)), lags = "long", type = "tau")
#summary(ur.kpss(diff(log(ethTS)), lags = "long", type = "tau"))

#ggAcf(ret_eth)
```

\newpage

```{r, grafCuanti121, fig.height=6, fig.width=7,fig.cap="Función de Autocorrelacion y Autocorrelación parcial del retorno logarítmico de precios de cierre de ETH/USD (2017-2022). Fuente: Elaboración Propia.",fig.scap="FAC y FACP retorno logarítmico de precios de cierre de ETH/USD (2017-2022)",echo=F}

(g71/
g72)

```


Se testea a continuación la estacionariedad de la serie diferenciada de logaritmos de precios diarios de ETH/USD. Del mismo modo que con la serie original se aplican Dickey Fuller Aumentado y KPSS. Ambas pruebas indican la estacionariedad de la serie modificada a un nivel $\alpha=0.05$

```{r, include=F, eval=F}
library(urca)
urdftest_lag = floor(12*(length(ret_eth)/100)^0.25)

#?ur.df
ur.df(ret_eth, lags = urdftest_lag, type = "trend")
summary(ur.df(ret_eth, lags = urdftest_lag, type = "trend"))


## la prueba de Dickey Fuller estÃ¡ sesgada hacia el no rechazo de la hipÃ³tesis nula

## la prueba KPSS tendrÃ¡ la hipÃ³tesis de estacionariedad de tendencia como nula 
## es decir, tendencia determinista con residuos estacionarios

ndiffs(diff(log(ethTS)),alpha=0.05,test = "kpss")

# Gráficos de autocorrelación y autocorrelación parcial 




```


El gráfico de autocorrelación parcial presenta autocorrelaciones muy bajas para ser consideradas en modelización. Por otro lado la función de autocorrelación presenta un comportamiento similar con autocorrelaciones muy bajas y límites definidos en relación a la longitud de la serie. Siguiendo la metodología propuesta por Hyndman y Athanasopuolos (2018) se prueba con modelos ejecutados automáticamente por el paquete Forecasts.

```{r,include=F, eval=T}
prueba4<-prueba %>% mutate(key="ETH")

model_arima_fit<-prueba4 %>% 
  filter(key=="ETH") %>% 
  model(ARIMA(log(value), stepwise=FALSE))

glance(model_arima_fit) %>% arrange(AICc) %>%  select(.model:BIC)

g5<-model_arima_fit %>% gg_tsresiduals()

```

El sistema realiza 2 tipos de búsquedas una paso a paso y otra en función de la minimización de AICc, coincidiendo en la selección de un modelo ARIMA(1,1,2)(1,0,0)[7]. Los valores de los criterios de información $AICc=5324$ y $BIC=5291$   

El modelo resulta entonces:

\begin{equation}\large{\mathbf{(1-\phi_1)(1-\Phi_1B^{7})(1-B)(1-B^7)y_t=(1-\theta_1B)(1-\theta_2B^2)\epsilon_t+c}}\end{equation}

Donde: $\phi_1=0.9169$ ,$\Phi_1=-0.9515$, $\theta_1=0.0686$, $\theta_2=-0.0460$, $c=0.0003$. Todos significativos a un nivel $\alpha=0.10$.

Se realiza la comprobación de los residuos del modelo ajustado:

```{r,include=FALSE,eval=TRUE}

aux_2<-g5

g73<-aux_2[[1]]+labs(x = "Año",y="Residuos de innovaciones")
g74<-aux_2[[2]]+labs(x = "Rezagos [1 día]",y="FAC")
g75<-aux_2[[3]]+labs(x = "Residuos",y="Cuenta")

readr::write_rds(g73, here::here('g73.RDS'))
readr::write_rds(g74, here::here('g74.RDS'))
readr::write_rds(g75, here::here('g75.RDS'))



```


```{r, grafCuanti13, fig.height=4, fig.width=7, fig.cap="Gráfico de residuos de modelo ARIMA(1,1,2) (1,0,0)(7) de retornos logarítmicos ETH/USD (2017-2022). Fuente: Elaboración Propia", fig.scap="Gráfico de residuos de modelo ARIMA(1,1,2) (1,0,0)(7) de retornos logarítmicos ETH/USD (2017-2022)", echo=F}

g73<-readRDS("~/g73.RDS")
g74<-readRDS("~/g74.RDS")
g75<-readRDS("~/g75.RDS")


g73


```

```{r,grafCuanti131, fig.height=8, fig.width=7, fig.cap="ACF y distribución de residuos de modelo ARIMA(1,1,2) (1,0,0)(7). Fuente: Elaboración Propia", fig.scap="ACF y distribución de residuos de modelo ARIMA(1,1,2) (1,0,0)(7).", echo=F}
(g74/
g75)

```


Los residuos parecen no estar correlacionados, si bien los límites del gráfico de autocorrelación no contienen a todos los puntos, los límites indican correlaciones muy débiles y solamente se ve una observación fuera de los límites en 30 rezagos (menos del $5\%$). Se observan algunos valores atípicos en los residuos, pero en general parece ser un ruido blanco. La distribución de los residuos es leptocúrtica, centrada en cero. 

A continuación se realiza el test de Ljung-Box para determinar si es posible considerar a los residuos como ruido blanco. El valor $p=0.0567$ no permite rechazar con un nivel $\alpha=0.05$ la hipótesis nula de que los residuos no están correlacionados.

```{r,eval=F, include=F}
augment(model_arima_fit) %>% 
  features(.innov, ljung_box, lag = 10, dof = 3)


```


```{r,eval=TRUE,include=FALSE}
aux_3<-gg_arma(model_arima_fit)


aux_3$data<-aux_3$data  %>% mutate(data, .model="ARIMA(log(valor))", type=case_when(
  type=="AR roots"~ "Raíces AR",
  type=="MA roots"~ "Raíces MA",
  TRUE~type),
  UnitCircle = case_when(
    UnitCircle=="Within"~"Dentro",
  TRUE~UnitCircle
  )
  )

g76<-aux_3+labs(x="Real(1/raíz)",y="Imaginario(1/raíz)")+theme(legend.position = "none")

readr::write_rds(g76, here::here('g76.RDS'))


```


```{r, grafCuanti14, fig.height=6, fig.width=7, fig.cap="Gráfico raíces características del modelo ARIMA(1,1,2) (1,0,0)(7) retornos logarítmicos de ETH/USD (2017-2022).Fuente:Elaboración Propia",fig.scap="Gráfico raíces características del modelo ARIMA(1,1,2) (1,0,0)(7) retornos logarítmicos ETH/USD (2017-2022)",echo=F}

g76<-readRDS("~/g76.RDS")

g76

```

Se observan que el modelo ajustado automáticamente presenta raíces que no están cerca de los límites del círculo unidad, lo que es recomendable porque el comportamiento es estable. El modelo es entonces, pronosticable y estable.

A continuación se grafica el pronóstico de la serie para 5 días:

```{r, grafCuanti15, fig.height=3.5, fig.width=6.5, fig.cap="Pronóstico a 5 días del modelo ARIMA(1,1,2)(1,0,0)(7) retornos logarítmicos de ETH/USD (2017-2022).Fuente: Elaboración Propia.",fig.scap="Pronóstico a 5 días del modelo ARIMA(1,1,2)(1,0,0)(7) retornos logarítmicos de ETH/USD (2017-2022).",echo=F}

model_arima_fit  %>% 
  forecast(h=5) %>% 
  autoplot(prueba2)+labs(x="Fecha",y="Cotización de cierre diario ETH/USD",level="Nivel")


```

Donde los intervalos de confianza del 80\% 95\% se muestran en color sombreado claro y oscuro respectivamente en los pronósticos ARIMA de todos los horizontes planteados.


El pronóstico con ARIMA para 15 días resulta:

```{r, grafCuanti16, fig.height=3.5, fig.width=6.5, fig.cap="Pronóstico a 15 días del modelo ARIMA(1,1,2)(1,0,0)(7) retornos logarítmicos de cierre de ETH/USD (2017-2022).Fuente: Elaboración Propia", fig.scap="Pronóstico a 15 días del modelo ARIMA(1,1,2)(1,0,0)(7) retornos logarítmicos de ETH/USD (2017-2022).",echo=F}

model_arima_fit  %>% 
  forecast(h=15) %>% 
  autoplot(prueba2)+labs(x="Fecha",y="Cotización de cierre diario ETH/USD",level="Nivel")


```

Para un horizonte de 30 días:

```{r, grafCuanti17, fig.height=3.5, fig.width=6.5, fig.cap="Pronóstico a 30 días del modelo ARIMA(1,1,2)(1,0,0)(7) ajustado a logaritmo de precios de cierre de ETH/USD (2017-2022). Fuente: Elaboración Propia",fig.scap="Pronóstico a 30 días del modelo ARIMA(1,1,2)(1,0,0)(7) ajustado a logaritmo de precios de cierre de ETH/USD (2017-2022)",echo=F}
model_arima_fit  %>% 
  forecast(h=30) %>% 
  autoplot(prueba2)+labs(x="Fecha",y="Cotización de cierre diario ETH/USD",level="Nivel")

#model_arima_fit$estimate
```

Las métricas del modelo seleccionado para el mes de enero 2022 son:

```{r,include=F, eval=F}

forecast_arima_5_d<-model_arima_fit  %>%
  forecast(h = "5 days")

forecast_arima_15_d<-model_arima_fit  %>%
  forecast(h = "15 days")

forecast_arima_30_d<-model_arima_fit  %>%
  forecast(h = "30 days")


predicted_arima<-forecast_arima_5_d$.mean
actual_arima<-ethTEST$close[1827:1831]

rmse(actual_arima, predicted_arima)
mae(actual_arima, predicted_arima)
mape(actual_arima, predicted_arima)


#Mincer-Zarnowsky:
# predicted_arima_5<-forecast_arima_5_d$.mean
# predicted_arima_15<-forecast_arima_15_d$.mean
# predicted_arima_30<-forecast_arima_30_d$.mean
# actual_arima_5<-ethTEST$close[1827:1831]
# actual_arima_15<-ethTEST$close[1827:1841]
# actual_arima_30<-ethTEST$close[1827:1856]
# 
# mz_arima_30<-lm(predicted_arima_30~actual_arima_30)
# summary(mz_arima_30)
# 
# mz_arima_15<-lm(predicted_arima_15~actual_arima_15)
# summary(mz_arima_15)
# 
# mz_arima_5<-lm(predicted_arima_5~actual_arima_5)
# summary(mz_arima_5)
# plot(mz_arima_30)

```

*Horizonte de 5 días*: pronóstico desde el 1/1/2022 al 5/1/2022: 

```{r, echo=F,warning=F}
tabla_fcst_ets_5_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c(117.8213,108.3372,2.9215)
                                      )

knitr::kable(tabla_fcst_ets_5_d,
        caption = "Resumen de metricas pronóstico a 5 días con ARIMA de ETH/USD (cierre diario). Fuente: Elaboración Propia.",caption.short="Resumen de metricas pronóstico a 5 días con ARIMA de ETH/USD (cierre diario)"
          
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 10)
```

*Horizonte de 15 días*: pronóstico desde el 1/1/2022 al 15/1/2022:

```{r, echo=F,warning=F}
tabla_fcst_ets_5_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c(470.5689,409.8491,12.5723)
                                      )

knitr::kable(tabla_fcst_ets_5_d,
        caption = "Resumen de metricas pronóstico a 15 días con modelo ARIMA ETH/USD (cierre diario). Fuente Elaboración: Propia", caption.short="Resumen de metricas pronóstico a 15 días con modelo ARIMA ETH/USD (cierre diario)"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

*Horizonte de 30 días*: pronóstico desde el 1/1/2022 al 30/1/2022:

```{r, echo=F,warning=F}
tabla_fcst_arima_30_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c(1026.5031,861.8827,31.5266)
                                      )

knitr::kable(tabla_fcst_arima_30_d,
        caption = "Resumen de metricas pronóstico a 30 días con modelo ARIMA ETH/USD (cierre diario). Fuente: Elaboración Propia.",caption.short="Resumen de metricas pronóstico a 30 días con modelo ARIMA ETH/USD (cierre diario)"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 10)
```

Se utiliza el mismo esquema de validación cruzada que en el modelo ETS para los distintos horizontes de pronóstico (ver Figura 9, con el ejemplo del esquema para 30 días). Se obtienen los siguientes resultados:

```{r,include=F, eval=F}

prueba %>% 
  slice(1:(n()-1)) %>% 
  stretch_tsibble(.init = 300,.step=200) %>% 
  model(ARIMA(log(value), stepwise=FALSE)) %>% 
  forecast(h = 5) %>% 
  fabletools::accuracy(prueba) %>% 
  select(.model, RMSE:MAPE)

```

Para un pronóstico a 5 días:

```{r, echo=F,warning=F}
tabla_fcst_arima_5_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("82.1100","34.8900","5.4500")
                                      )

knitr::kable(tabla_fcst_arima_5_d,
        caption = "Resumen de metricas de CV en pronóstico a 5 días con ARIMA de ETH/USD (cierre diario). Fuente: Elaboración Propia", caption.short="Resumen de metricas de CV en pronóstico a 5 días con ARIMA de ETH/USD (cierre diario)"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 10)
```

Para un pronóstico a 15 días:

```{r, echo=F,warning=F}
tabla_fcst_arima_15_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("145.1000","80.7100","12.4200")
                                      )

knitr::kable(tabla_fcst_arima_15_d,
        caption = "Resumen de metricas de CV en pronóstico a 15 días con ARIMA de ETH/USD (cierre diario). Fuente: Elaboración Propia", caption.short="Resumen de metricas de CV en pronóstico a 15 días con ARIMA de ETH/USD (cierre diario)"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 10)
```

Para un pronóstico a 30 días:

```{r, echo=F,warning=F}
tabla_fcst_arima_30_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("275.0300","152.0200","19.5000")
                                      )

knitr::kable(tabla_fcst_arima_30_d,
        caption = "Resumen de metricas de CV en pronóstico a 30 días con ARIMA ETH/USD (cierre diario). Fuente: Elaboración Propia", caption.short="Resumen de metricas de CV en pronóstico a 30 días con ARIMA ETH/USD (cierre diario)" 
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 10)
```

\newpage

## Modelo ARFIMA:

En este punto del trabajo se busca evaluar la performance para pronóstico de un modelo autorregresivo fraccionalmente integrado de medias móviles y continuar con el modelado de la media del proceso.

En relación con el modelo ARIMA ajustado surge la posibilidad de cuestionar si el modelo fue sobrediferenciado y presentarse un proceso de memoria larga en la media. De acuerdo a lo presentado en el marco teórico, Crato y Ray (1996) basándose en un extenso estudio realizado mediante simulación concluyen que únicamente cuando el número de observaciones es elevado y la persistencia muy fuerte puede estar justificado el uso de los modelos ARFIMA. 

En relación a la cantidad de observaciones disponibles, el conjunto total de precios de cierre entre los años 2017 y 2022 contiene 1826 observaciones con lo que se considera un conjunto de longitud apropiada para la técnica. Del resultado de la modelización se obtiene la persistencia.

Se modela entonces el ARFIMA utilizando el paquete Forecast en R. El modelo se selecciona de manera automática utilizando todos los datos disponibles de la serie. La identificación analítica de los parámetros de este modelo están fuera de los alcances del presente trabajo.

El modelo obtenido mediante Forecast es el siguiente:

\begin{equation}\large{\mathbf{(1-B)^{d}y_t=(1-\theta_1B)(1-\theta_2B^2)(1-\theta_3B^3)(1-\theta_4B^4)(1-\theta_5B^5)\epsilon_t}}\end{equation}

Donde: $d=0.4996$, $\theta_1=-0.5779$,$\theta_2=-0.4768$,$\theta_3=-0.3780$,$\theta_4=-0.3349$,$\theta_5=-0.1022$.

Al ser $d<0.5000$ se observa un proceso de memoria larga en el que las autocorrelaciones decrecen lentamente (aproximadamente de manera hiperbólica) como se observa en la figura a continuación. Los criterios de información del modelo son AIC=21064,6300 y AICc=21064,4500 y BIC=21103,2000.

```{r,grafCuanti18, fig.height=4, fig.width=6.5, fig.cap="Función de Autocorrelación ETH/UDS cierre diario (retardo 200 días). Fuente: Elaboración Propia.", fig.scap="Función de Autocorrelación ETH/UDS cierre diario (retardo 200 días).",echo=F}
ggAcf(ethTS,lag.max = 200)+ggtitle("")
```

Se muestran a continuación los residuos del modelo ARFIMA seleccionado:

```{r,grafCuanti19, fig.height=4.5, fig.width=7, fig.cap="Residuos modelo ARFIMA(0,0.5,5).Fuente: Elaboración propia",fig.scap="Residuos modelo ARFIMA(0,0.5,5)",echo=F,include=T,message=FALSE,warning=FALSE}
model_arfima_fit<-arfima(prueba$value)
#model_arfima_fit$residuals

date_vector<-seq.Date(as.Date("2017-01-01"),as.Date("2021-12-30"),by="day")

g90<-autoplot(forecast::forecast(model_arfima_fit,h=5)) 





#summary(model_arfima_fit)

g82<-autoplot(ts(model_arfima_fit$residuals,start=c(2017,1),frequency=365))+labs(x = "Año",y="Residuos de innovaciones")

g83<-ggAcf(model_arfima_fit$residuals)+labs(x = "Rezagos [1 día]",y="FAC",title = "") 

residuals_df_arfima <- data.frame(Residuos = model_arfima_fit$residuals)

g84<-ggplot(residuals_df_arfima, aes(x = Residuos)) +
  geom_histogram(bins = 300) + labs(x = "Residuos",y="Cuenta")


readr::write_rds(g82, here::here('g82.RDS'))
readr::write_rds(g83, here::here('g83.RDS'))
readr::write_rds(g84, here::here('g84.RDS'))

(g82|
 g83/g84)

```

```{r,include=F,echo=FALSE}
time_series <- g90$data$value[1626:1826]  # Replace ... with your actual time series data
aux_arfima_5<-data.frame(forecast::forecast(model_arfima_fit,h=5))
# Forecasts
forecast_values <- aux_arfima_5$Point.Forecast  # Replace ... with your forecasted values
forecast_80_lower <- aux_arfima_5$Lo.80  # Replace ... with your lower limit for the 80% forecast
forecast_80_upper <- aux_arfima_5$Hi.80  # Replace ... with your upper limit for the 80% forecast
forecast_95_lower <- aux_arfima_5$Lo.95  # Replace ... with your lower limit for the 95% forecast
forecast_95_upper <- aux_arfima_5$Hi.95  # Replace ... with your upper limit for the 95% forecast

# Create a data frame with the time series data
data <- data.frame(
  x = date_vector[1626:1826],
  y = time_series
)

# Create a data frame with the forecast data
forecast_data_5 <- data.frame(
  x = seq.Date(as.Date("2022-01-01"),as.Date("2022-01-05"),by="day"),
  forecast = forecast_values,
  lower_80 = forecast_80_lower,
  upper_80 = forecast_80_upper,
  lower_95 = forecast_95_lower,
  upper_95 = forecast_95_upper
)

# Plot the time series values and forecasts
g91<-ggplot() +
  geom_line(data = data, aes(x, y), color = "black", linetype = "solid", size = 0.3) +
  geom_line(data = forecast_data_5, aes(x, forecast), color = "blue", linetype = "solid", size = 0.5) +
  geom_ribbon(data = forecast_data_5, aes(x = x, ymin = lower_80, ymax = upper_80), fill = "blue", alpha = 0.3) +
  geom_ribbon(data = forecast_data_5, aes(x = x, ymin = lower_95, ymax = upper_95), fill = "blue", alpha = 0.2) +
  xlab("Tiempo") +
  ylab("Precio de Cierre ETH/USD") 


aux_arfima_15<-data.frame(forecast::forecast(model_arfima_fit,h=15))
# Forecasts
forecast_values_15 <- aux_arfima_15$Point.Forecast  # Replace ... with your forecasted values
forecast_80_lower_15 <- aux_arfima_15$Lo.80  # Replace ... with your lower limit for the 80% forecast
forecast_80_upper_15 <- aux_arfima_15$Hi.80  # Replace ... with your upper limit for the 80% forecast
forecast_95_lower_15 <- aux_arfima_15$Lo.95  # Replace ... with your lower limit for the 95% forecast
forecast_95_upper_15 <- aux_arfima_15$Hi.95  # Replace ... with your upper limit for the 95% forecast

forecast_data_15 <- data.frame(
  x = seq.Date(as.Date("2022-01-01"),as.Date("2022-01-15"),by="day"),
  forecast = forecast_values_15,
  lower_80 = forecast_80_lower_15,
  upper_80 = forecast_80_upper_15,
  lower_95 = forecast_95_lower_15,
  upper_95 = forecast_95_upper_15
)

g92<-ggplot() +
  geom_line(data = data, aes(x, y), color = "black", linetype = "solid", size = 0.3) +
  geom_line(data = forecast_data_15, aes(x, forecast), color = "blue", linetype = "solid", size = 0.5) +
  geom_ribbon(data = forecast_data_15, aes(x = x, ymin = lower_80, ymax = upper_80), fill = "blue", alpha = 0.3) +
  geom_ribbon(data = forecast_data_15, aes(x = x, ymin = lower_95, ymax = upper_95), fill = "blue", alpha = 0.2) +
  xlab("Tiempo") +
  ylab("Precio de Cierre ETH/USD") 

aux_arfima_30<-data.frame(forecast::forecast(model_arfima_fit,h=30))
# Forecasts
forecast_values_30 <- aux_arfima_30$Point.Forecast  # Replace ... with your forecasted values
forecast_80_lower_30 <- aux_arfima_30$Lo.80  # Replace ... with your lower limit for the 80% forecast
forecast_80_upper_30 <- aux_arfima_30$Hi.80  # Replace ... with your upper limit for the 80% forecast
forecast_95_lower_30 <- aux_arfima_30$Lo.95  # Replace ... with your lower limit for the 95% forecast
forecast_95_upper_30 <- aux_arfima_30$Hi.95  # Replace ... with your upper limit for the 95% forecast

forecast_data_30 <- data.frame(
  x = seq.Date(as.Date("2022-01-01"),as.Date("2022-01-30"),by="day"),
  forecast = forecast_values_30,
  lower_80 = forecast_80_lower_30,
  upper_80 = forecast_80_upper_30,
  lower_95 = forecast_95_lower_30,
  upper_95 = forecast_95_upper_30
)

g93<-ggplot() +
  geom_line(data = data, aes(x, y), color = "black", linetype = "solid", size = 0.3) +
  geom_line(data = forecast_data_30, aes(x, forecast), color = "blue", linetype = "solid", size = 0.5) +
  geom_ribbon(data = forecast_data_30, aes(x = x, ymin = lower_80, ymax = upper_80), fill = "blue", alpha = 0.3) +
  geom_ribbon(data = forecast_data_30, aes(x = x, ymin = lower_95, ymax = upper_95), fill = "blue", alpha = 0.2) +
  xlab("Tiempo") +
  ylab("Precio de Cierre ETH/USD") 

readr::write_rds(g91, here::here('g91.RDS'))
readr::write_rds(g92, here::here('g92.RDS'))
readr::write_rds(g93, here::here('g93.RDS'))




```



\newpage
Se observa en los gráficos anteriores que los residuos no presentan características de ruido blanco. La variabilidad de los mismos no es constante a lo largo del tiempo, existe autocorrelación significativa en los rezagos y se ve un comportamiento leptocúrtico en su distribución. Estos aspectos sugieren que no se está modelando correctamente la varianza de la serie y que los intervalos de confianza obtenidos con este modelo no van a representar correctamente la realidad por ser demasiado estrechos.

Igualmente, se procede a realizar los pronósticos puntuales para los horizontes de tiempo en estudio para evaluar las métricas propuestas.

Para el horizonte propuesto de 5 días:

```{r,include=F, eval=T}
#prueba4<-prueba %>% mutate(key="ETH")

actual_arima<-as.numeric(ethTEST$close[1827:1831])


actual_arima_15d<-ethTEST$close[1827:1841]


actual_arima_30d<-ethTEST$close[1827:1856]



forecast_arfima_5d<-forecast::forecast(model_arfima_fit,h=5,is.date=TRUE)

c_5<-as.tibble(forecast_arfima_5d$mean)
c_5<-mutate(c_5,'Fecha'= c('2022-01-01','2022-01-02','2022-01-03','2022-01-04','2022-01-05'))
c_5<-dplyr::rename(c_5,Valor=x)
#df%>% select (puntos, todo () )

c_5<-mutate(c_5,Valor_Real=actual_arima)

c_5<-c_5 %>% select(Fecha,Valor,Valor_Real)

c_5$Valor<-round(c_5$Valor,2)
#c_5$Valor_Real<-sprintf("%4.f",c_5$Valor_Real)


forecast_arfima_15_d<-forecast::forecast(model_arfima_fit,h=15,is.date=TRUE)

Fecha_15 <- (as.Date("2022-01-01") + 0:14)

c_15<-as.tibble(forecast_arfima_15_d$mean)
c_15<-mutate(c_15,Fecha=Fecha_15)

c_15<-mutate(c_15, Valor_Real=actual_arima_15d)
c_15<-dplyr::rename(c_15, Valor=x)
c_15<-c_15 %>% select(Fecha, Valor, Valor_Real)
c_15$Valor<-round(c_15$Valor,2)




forecast_arfima_30_d<-forecast::forecast(model_arfima_fit,h=30,is.date=TRUE)

Fecha_30 <- (as.Date("2022-01-01") + 0:29)

c_30<-as.tibble(forecast_arfima_30_d$mean)
c_30<-mutate(c_30,Fecha=Fecha_30)
c_30<-mutate(c_30, Valor_Real=actual_arima_30d)
c_30<- dplyr::rename(c_30, Valor=x)
c_30<-c_30 %>% select(Fecha,Valor, Valor_Real)
c_30$Valor<-round(c_30$Valor,2)


#rmse(actual_arima, predicted_arima)
#mae(actual_arima, predicted_arima)
#mape(actual_arima, predicted_arima)


#prueba2
#prueba2

#Mincer_Zarnowsky:

# actual_arima_30d
# forecast_arfima_30_d$mean
# 
# mz_arfima_30_d<-lm(forecast_arfima_30_d$mean~actual_arima_30d)
# summary(mz_arfima_30_d)
# 
# mz_arfima_15_d<-lm(forecast_arfima_15_d$mean~actual_arima_15d)
# summary(mz_arfima_15_d)
# 
# mz_arfima_5_d<-lm(forecast_arfima_5d$mean~actual_arima)
# summary(mz_arfima_5_d)
# car::linearHypothesis(mz_arfima_30_d, c("(Intercept) = 3769", "actual_arima_30d = 1"))

```

```{r, echo=F,warning=F}

knitr::kable(c_5,
        caption = "Pronóstico a 5 días con ARFIMA ETH/USD (cierre diario)",digits = 2,
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

Las métricas obtenidas para el pronóstico de los 5 primeros cierres de 2022 son las siguientes:

```{r,echo=F}
rmse_5_arfima_2022<-Metrics::rmse(c_5$Valor_Real, c_5$Valor)
mae_5_arfima_2022<-Metrics::mae(c_5$Valor_Real, c_5$Valor)
mape_5_arfima_2022<-Metrics::mape(c_5$Valor_Real, c_5$Valor)

metricas_arfima_5d<-tibble(RMSE=rmse_5_arfima_2022,MAE=mae_5_arfima_2022,MAPE=mape_5_arfima_2022*100)



knitr::kable(metricas_arfima_5d,
        caption = "Metricas de Pronóstico a 5 días con ARFIMA ETH/USD (cierre diario)",digits=4,
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)

```

Gráficamente el pronóstico para los 5 primeros días de enero 2022 resulta (se muestran los últimos 200 registros de la serie para mejor visualización):

```{r, grafCuanti151, fig.height=3.5, fig.width=6.5, fig.cap="Pronóstico a 5 días del Precio de cierre de ETH/USD. Modelo ARFIMA(0,0.5,5).Fuente: Elaboración Propia.",fig.scap="Pronóstico a 5 días del Precio de cierre de ETH/USD. Modelo ARFIMA(0,0.5,5)",echo=F}
g91
```


Al extender el horizonte de pronóstico a los primeros 15 días de 2022 se obtienen las siguientes estimaciones puntuales:


```{r, echo=F,warning=F}

knitr::kable(c_15,
        caption = "Pronóstico a 15 días con ARFIMA ETH/USD (cierre diario)",digits=2,
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

Las métricas obtenidas para el pronóstico de los 15 primeros cierres de 2022 son las siguientes:

```{r,echo=F}
rmse_15_arfima_2022<-Metrics::rmse(c_15$Valor_Real, c_15$Valor)
mae_15_arfima_2022<-Metrics::mae(c_15$Valor_Real, c_15$Valor)
mape_15_arfima_2022<-Metrics::mape(c_15$Valor_Real, c_15$Valor)

metricas_arfima_15d<-tibble(RMSE=rmse_15_arfima_2022,MAE=mae_15_arfima_2022,MAPE=mape_15_arfima_2022*100)

knitr::kable(metricas_arfima_15d,
        caption = "Metricas de Pronóstico a 15 días con ARFIMA ETH/USD (cierre diario)",digits=4,
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)

```

Gráficamente el pronóstico para los 15 primeros días de enero 2022 resulta:

```{r, grafCuanti152, fig.height=3.5, fig.width=6.5, fig.cap="Pronóstico a 15 días del Precio de cierre de ETH/USD. Modelo ARFIMA(0,0.5,5).Fuente: Elaboración Propia.",fig.scap="Pronóstico a 15 días del Precio de cierre de ETH/USD. Modelo ARFIMA(0,0.5,5).",echo=F}
g92
```


Si el horizonte de pronóstico es de 30 días se obtienen las siguientes estimaciones puntuales para los primeros días de 2022:


```{r, echo=F,warning=F}

knitr::kable(c_30,
        caption = "Pronóstico a 30 días con ARFIMA ETH/USD (cierre diario)",digits=2,
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

Las métricas obtenidas para el pronóstico de los 30 primeros cierres de 2022 son las siguientes:

```{r,echo=F}
rmse_30_arfima_2022<-Metrics::rmse(c_30$Valor_Real, c_30$Valor)
mae_30_arfima_2022<-Metrics::mae(c_30$Valor_Real, c_30$Valor)
mape_30_arfima_2022<-Metrics::mape(c_30$Valor_Real, c_30$Valor)

metricas_arfima_30d<-tibble(RMSE=rmse_30_arfima_2022,MAE="360.6290",MAPE=mape_30_arfima_2022*100)

knitr::kable(metricas_arfima_30d,
        caption = "Metricas de Pronóstico a 30 días con ARFIMA ETH/USD (cierre diario)",digits=4,
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)

```

\newpage
Gráficamente el pronóstico para los 30 primeros días de 2022 resulta:

```{r, grafCuanti153, fig.height=3.5, fig.width=6.5, fig.cap="Pronóstico a 30 días del Precio de cierre de ETH/USD. Modelo ARFIMA(0,0.5,5).Fuente: Elaboración Propia.",fig.scap="Pronóstico a 30 días del Precio de cierre de ETH/USD. Modelo ARFIMA(0,0.5,5).",echo=F}
g93
```



Validación cruzada para el modelo ARFIMA: se aplica el mismo esquema que en los modelos estudiados anteriormente con 8 cortes de la serie de tiempo. Se evalúa cada instancia y se promedian las métricas, resultando:

Para un pronóstico a 5 días:

```{r, echo=F,warning=F}
tabla_fcst_arfima_5_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("88.0600","75.4400","5.1700")
                                      )

knitr::kable(tabla_fcst_arfima_5_d,
        caption = "Resumen de metricas de CV en pronóstico a 5 días con ARFIMA de ETH/USD (cierre diario)"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

Para un pronóstico a 15 días:

```{r, echo=F,warning=F}
tabla_fcst_arfima_15_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("112.2200","92.8500","8.6200")
                                      )

knitr::kable(tabla_fcst_arfima_15_d,
        caption = "Resumen de metricas de CV en pronóstico a 15 días con ARFIMA de ETH/USD (cierre diario)"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

Para un pronóstico a 30 días:

```{r, echo=F,warning=F}
tabla_fcst_arfima_30_d <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("157.4900","141.0200","14.2300")
                                      )

knitr::kable(tabla_fcst_arfima_30_d,
        caption = "Resumen de metricas de CV en pronóstico a 30 días con ARFIMA ETH/USD (cierre diario)"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

\newpage

## Modelo ARCH: 

En esta sección del marco empírico se trabaja sobre modelos que pueden captar la variabilidad condicional no constante de la serie de en estudio. Se comienza con el modelo autorregresivo de heterocedasticidad condicional. Estos modelos, como se detalla en el marco teórico fueron propuestos por Engle (1982) para representar la volatilidad de las series en los mercados financieros. La volatilidad es la desviación estandard condicional del precio de un activo. Tiene un importante rol para el cálculo del riesgo de una posición financiera. Modelar la volatilidad de una serie temporal mejora la estimación de parámetros y la precisión de pronósticos de intervalos (Tsay,2010).

La volatilidad no es diréctamente observable. Para estimar la volatilidad diaria se podría trabajar con modelos que ajustan la volatilidad intradiaria pero lamentablemente los retornos intradiarios contienen muy poca información sobre la volatilidad del día siguiente (Tsay, 2010). A pesar de no ser directamente observable se presentan ciertas características comunes a los retornos de los activos financieros. Existen clústeres de volatilidad (momentos en los que se presenta alta volatilidad y momentos en los que presenta baja volatilidad agrupados), la volatilidad evoluciona a lo largo del tiempo de manera continua (los saltos de volatilidad son raros), la volatilidad no diverge hacia el infinito. Estadísticamente hablando puede considerarse a la volatilidad como estacionaria. La volatilidad parece reaccionar de manera diferente a una gran suba de precios o a una gran baja (efecto leverage). Este último punto se puede observar en los modelos EGARCH.

Hay 2 grupos de modelos de heterocedasticidad condicional. Los que tienen una ecuación para la evolución de $\sigma_t^2$ y los que utilizan una ecuación estocástica para modelar $\sigma_t^2$.

Tsay (2010) describe el proceso de  construcción de un modelo de volatilidad con los siguientes 4 pasos:

1-Especificar la ecuación de la media de la serie de retornos con un modelo ARMA o ARIMA.

2-Utilizar la ecuación de la media para testear los efectos ARCH.

3-Especificar un modelo de volatilidad si los efectos ARCH resultan significativos y realizar la estimación de las ecuaciones del mismo.

4-Chequear el ajuste del modelo y re definir en caso necesario.


Con esto se intenta capturar información existente en la serie que no puede ser correctamente modelada con los ARIMA o ARFIMA anteriormente aplicados, justamente porque son modelos que modelan la media de la serie. En esta caso se busca complementar el estudio con el modelado de la heterocedasticidad condicional.

El método que se aplica a continuación incluye la prueba de multiplicador de Lagrange para determinar la presencia de efectos ARCH. Se estima una ecuación para la media, se obtienen los residuos y sus cuadrados $\hat{e}_t^2$ y se ajusta una regresión de los errores al cuadrado con los errores al cuadrado rezagados.

En la sección en la que se modela ARIMA se observa que la serie de las diferencias de los logaritmos de los precios de cierre de ETH/USD presentan el siguiente comportamiento (Figura 18).

```{r,  grafCuanti20, fig.height=4, fig.width=5.5, fig.cap="Gráfico de retornos logarítmicos de precios de cierre de ETH/USD (2017-2022) y Función de Autocorrelación Parcial",echo=F,message=FALSE}
# Se vuelve a testear raíces unitarias con test de Dickey Fuller Aumentado:
#urdftest_lag2 = floor(12*(length(diff(ethTS))/100)^0.25)
#ur.df(diff(ethTS), lags = urdftest_lag2, type = "none")
#summary(ur.df(diff(ethTS), lags = urdftest_lag2, type = "none"))
#Mediante KPSS:
#ur.kpss(diff(log(ethTS)), lags = "long", type = "tau")
#summary(ur.kpss(diff(log(ethTS)), lags = "long", type = "tau"))

#ggAcf(ret_eth)

g27<-prueba %>% autoplot(difference(log(value)))+ ylab("Log(Retorno)")+xlab("Fecha")
#summary(auto.arima(na.omit(difference(log(prueba$value)))))
#garch(na.omit(difference(log(prueba$value))),control=garch.control(grad="numerical",trace=FALSE))

g28<-ggAcf(difference(log(prueba$value))^2,type = 'partial')+ggtitle('Retornos al cuadrado')
g27/
g28
```

Al visualizar la serie de los rendimientos se detecta que la varianza de los mismos no es constante. Al observar la función de autocorrelacion parcial de los retornos al cuadrado se observa en los 4 primeros retardos que la serie de retornos al cuadrado no es serialmente independiente y existe la presencia de comportamiento ARCH. 

Recordando la ecuación de la media estimada con el ARIMA(1,1,2)(1,0,0)[7] a continuación se grafican los residuos del modelo.

```{r,grafCuanti21, fig.height=4, fig.width=5.5, fig.cap="Gráfico residuos al cuadrado de modelo ARIMA de precios de cierre de ETH/USD (2017-2022)",message=F,echo=F}
#g5<-model_arima_fit %>% gg_tsresiduals()

#model_arima_fit

residuos_arima_cuadrado<-tsibble((g5[[1]][["data"]][[".resid"]])^2, prueba$date)
autoplot(residuos_arima_cuadrado)+ylab("Residuos^2")+xlab("Fecha")


```

En el gráfico anterior se observa que los residuos al cuadrado presentan momentos de mayor volatilidad, principalmente entre 2017 y 2018, el primer trimestre de 2020 y mitad de año del 2021. La varianza es heterocedástica. Se debe tener en cuenta que el estimador de la varianza condicional de los residuos al cuadrado no es un estimador eficiente de la varianza condicional como se menciona en el marco teórico pero sirve como aproximación para determinar el orden del modelo ARCH.

Se testea si los residuos al cuadrado rezagados son significativos para explicar el comportamiento de los residuos al cuadrado. Esto corresponde a realizar un ajuste por regresión entre los valores de residuos en el momento $t$ y $t-l$ donde $l$ es el retardo que de ser significativo daría el orden del modelo ARCH. Para hacer esto, se utiliza una función en R del paquete FinTS (ArchTest) que computa el test de los multiplicadores de Lagrange. El test muestra valores de rechazo de la hipótesis nula (la misma afirma queno se presenta comportamiento ARCH) para los primeros 4 rezagos, con valor p cercano a cero. 


Para el ajuste del modelo ARCH(4) se utilizó el paquete RUGARCH(Ghalanos, 2015) en R. Se realizó una comparación de diferentes variantes de modelos ARCH(4) considerando las distintas distribuciones de densidad para las innovaciones. Los resultados se resumen en la tabla a continuación.

```{r,echo=FALSE}
tabla_arch<-read_xlsx('~/tabla_resumen_arch.xlsx')#,header=TRUE,sep=',')

knitr::kable(tabla_arch,
        caption = "Resumen de tests en ARCH(4)"
        ) %>%
  kable_styling(full_width = T,latex_options = c("hold_position"), font_size = 10)

```

Donde las distribuciones empleadas son NORM para la distribución normal, SNORM la distribución normal sesgada, STD la distribución t-Student, SSTD la t-Student sesgada, GED la distribución de error generalizada, SGED la distribución de error generalizada sesgada, NIG la distribución normal inversa, GHYP la distribución generalizada hiperbólica. De acuerdo a la información resumida en el cuadro anterior, se pudo obtener coeficientes significativos del modelo sólo en el caso de ARCH(4) bajo el supuesto de distribución de densidad condicional normal par las innovaciones y los residuos obtenidos del modelo son leptocúrticos y se rechaza el test de bondad de ajuste de Pearson. Por otro lado en el ARCH (4) con distribución de innovaciones normal no se rechazan las hipótesis nulas en los test de correlación de residuos ni de residuos al cuadrado, tampoco el test ARCH de rezagos superiores con lo que se considera que el orden del modelo ARCH seleccionado es correcto. 

Se ajusta entonces un modelo ARCH(4) y el mismo tiene la ecuación:

\begin{equation}\large{\mathbf{\sigma_t^2=\omega+\alpha_1a_{t-1}^2+\alpha_2a_{t-2}^2+\alpha_3a_{t-3}^2+\alpha_4a_{t-4}^2}}\end{equation}

Donde los valores estimados de $\omega=0.0016$, $\alpha_1=0.1392$,$\alpha_2=0.0968$, $\alpha_3=0.0629$, $\alpha_4=0.1805$ todos significativos a un nivel $\alpha=0.05$. Con estos valores en los coeficientes del modelo el desvío estandard no condicional resulta $\sqrt{\frac{0.0016}{1-0.1392+0.0968+0.0629+0.1805}}=0.0582$


El valor de $\hat{\alpha_1}^2=0.0191<1/3$ con lo que el cuarto momento no condicional de los retornos diarios existe.  

```{r,include=F,eval=F,echo=F, message=FALSE}
library(FinTS)
ArchTest(difference(log(prueba$value)),lags=4,demean=TRUE)

```


```{r,grafCuanti22, fig.height=4, fig.width=5.5, fig.cap="Gráfico residuos al cuadrado de modelo ARIMA y ARCH(4)",echo=F,message=F,warning=FALSE}
#variance.model=list(model="iGARCH", garchOrder=c(1,1))

ug_spec<-ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(4, 0), 
submodel = NULL, external.regressors = NULL, variance.targeting = FALSE),mean.model=list(arfima=TRUE)) # no se puede especificar bien el ARIMA o ARFIMA.

ethTSTEST_tibble<-as_tibble(ethTSTEST)

prueba_LSTM_30<-tsibble::tsibble(date = as.Date("2017-01-01") + 0:1855,
  value = ethTSTEST_tibble$x)

prueba_arch_30<-na.omit(prueba_LSTM_30 %>% mutate(value=difference(log(prueba_LSTM_30$value))))

prueba_arch_30_ts<-as.ts(prueba_arch_30,start=c(2017,2), end=c(2022,30),frequency = 365)


ugfit<-ugarchfit(spec=ug_spec,data=prueba_arch_30_ts)


#ugarchfit(spec=ug_spec,data=na.omit(difference(log(prueba$value))),out.sample = 200)


#ug_var<-ugfit@fit$var # son las varianzas condicionales estimadas

#ug_res2<-ugfit@fit$residuals^2

#Armado del DF para gráfica de residuos y Varianzas Condicionales
#Fecha_ugfit <- (as.Date("2017-01-01") + 1:1825)
#length(Fecha_ugfit)
#ug_grafica<-data.frame(Fecha=Fecha_ugfit,Valor=ug_var)
#ug_grafica<-mutate(ug_grafica, Referencia="Varianza Condicional Estimada")
#ug_grafica2<-data.frame(Fecha=Fecha_ugfit,Valor=ug_res2)
#ug_grafica2<-mutate(ug_grafica2, Referencia="Residuos al cuadrado")
#ug_grafica_total<-rbind(ug_grafica,ug_grafica2)
#gráfico de residuos y varianzas condicionales:
#g7<-ggplot(ug_grafica_total, aes(x = Fecha, y = Valor , color = Referencia)) +
#  geom_line() +
#  theme(legend.position = "bottom")+scale_color_manual(values=c('#000000','#40E0D0')) 
#g7

```

En el gráfico anterior se observa que el modelo ARCH(4) muestra los picos de varianza condicional estimada de manera coincidente con los picos de los residuos al cuadrado, con lo que se considera apropiado para el modelo de la varianza heterocedástica de la serie. Bajo el marco de modelos ARCH los grandes shocks tienden a ser seguidos por otro gran shock. Además se observa que los residuos al cuadrado presentan mayor amplitud que los valores ajustados del modelo ARCH(4) y esto coincide con las observaciones realizadas en Poon (2005) de acuerdo a utilizar residuos al cuadrado como aproximación de la volatilidad.  

Se muestran a continuación las funciones de autocorrelación de los residuos estandarizados y de los residuos estandarizados al cuadrado.

```{r, graffoto1,fig.align='center',fig.height=6,fig.width=6,fig.cap="ACF Residuos al cuadrado ARCH(4). Elaboración Propia",fig.scap="ACF Residuos al cuadrado ARCH(4)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\ACF-residuos_cuadrados_ARCH(4).png")
```

```{r,graffoto2,fig.align='center',fig.height=6,fig.width=7, fig.cap="ACF Residuos estandarizados ARCH(4). Elaboración Propia",fig.scap="ACF Residuos estandarizados ARCH(4)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\ACF_residuos_estandarizados_ARCH(4).png")
```


En relación a los gráficos se puede observar que el modelo parece ser adecuado para describir la dependencia lineal tanto en la serie de los retornos como en la volatilidad.

Se realiza el pronóstico del desvío estándar condicional en la forma de fuera de la muestra con un horizonte de pronóstico de 1 día y dejando 300 observaciones fuera de la muestra. El método utilizado es de horizonte deslizante de tiempo. 

```{r,grafCuanti23, fig.align='center',fig.height=4,fig.width=6,fig.height=5, fig.width=10.5, fig.cap="Gráfico de pronóstico de desvio estándar condicional (h=5), desvío estandar condicional estimado con ARCH(4) y residuos al cuadrado de modelo ARIMA ",echo=F,include=F}

ug_fore_5<-ugarchforecast(ugfit,n.ahead=5)

train_arch<-ts(prueba_arch_30_ts[1:1500],frequency = 365)
length(train_arch)
test_arch<-ts(prueba_arch_30_ts[1501:1856],frequency = 365)
length(test_arch)

ug_spec<-ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(4, 0), 
submodel = NULL, external.regressors = NULL, variance.targeting = FALSE),mean.model=list(arfima=TRUE),distribution.model = "norm") # no se puede especificar bien el ARIMA o ARFIMA.

ugfit<-ugarchfit(spec=ug_spec,data=prueba_arch_30_ts,out.sample = 300)



forecast<-ugarchforecast(ugfit,n.ahead = 1,n.roll = 300)

predictions_selected <- forecast@forecast$sigmaFor
target_values <- forecast@forecast$seriesFor
selected_model_arch_rmse <- Metrics::rmse(predictions_selected, target_values)
selected_model_arch_mae <- Metrics::mae(predictions_selected, target_values)
selected_model_arch_mape <- Metrics::mape(predictions_selected, target_values)




plot(forecast, which=2)

g66<-plot(forecast,which=4)


# ug_fore_15<-ugarchforecast(ugfit,n.ahead=15)
# ug_fore_30<-ugarchforecast(ugfit,n.ahead=30)
# 
# #Se crean Los data frames para graficar la varianza condicional.
# 
# #ug_fore_5@forecast$sigmaFor[1:5]
# 
# fecha_fore_var_cond_5<-(as.Date("2022-01-01") + 0:4)
# graf_var_conf_5_d<-data.frame(Fecha=fecha_fore_var_cond_5, Valor=ug_fore_5@forecast$sigmaFor[1:5]^2 ,Referencia="Forecast con ARCH(4)")
# 
# ug_grafica_forecast<-rbind(ug_grafica_total,graf_var_conf_5_d)
# 
# g10<-ug_grafica_forecast %>% dplyr::filter(Fecha>'2021-11-30') %>% 
#   ggplot(aes(x = Fecha, y = Valor,color=Referencia )) +
#   geom_line()
# 
# 
# 
# g8<-ggplot(graf_var_conf_5_d, aes(x = Fecha, y = Valor )) +
#   geom_line()
# 
# #(g65 + g66)
# 
# #https://www.youtube.com/watch?v=8VXmRl5gzEU

```


```{r, graffoto3,fig.align='center',fig.height=6,fig.width=6,fig.cap="Pronósticos a un día fuera de la muestra (n=300) ARCH(4). Elaboración Propia",fig.scap="Pronósticos a un día fuera de la muestra (n=300) ARCH(4)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\grafico_ARCH_300_hdeslizante.png")
```



En el gráfico anterior se observa el comportamiento de los pronósticos a un día con horizonte deslizante para un conjunto fuera de la muestra de tamaño 300. Se muestran los intervalos de confianza al 95% para los valores pronosticados. 

```{r, graffoto4,fig.align='center',fig.height=6,fig.width=6,fig.cap="Pronósticos a un día y serie de valor absoluto de log retornos - modelo ARCH(4). Elaboración Propia",fig.scap="Pronósticos a un día y serie de valor absoluto de log retornos - modelo ARCH(4)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\pronostico_arch_300_versus_valor_abs.png")
```



Aquí se observa como los pronósticos fuera de la muestra para la serie replican con un leve retraso las variaciones de los retornos logarítmicos. 


```{r,grafCuanti24, fig.height=5, fig.width=10.5, fig.cap="Gráfico de pronóstico de varianza condicional (h=15), varianza condicional estimada con ARCH(4) y residuos al cuadrado de modelo ARIMA ",echo=F,eval=FALSE}
ug_fore_15<-ugarchforecast(ugfit,n.ahead=15)
ug_fore_30<-ugarchforecast(ugfit,n.ahead=30)

#Se crean Los data frames para graficar la varianza condicional.

#ug_fore_5@forecast$sigmaFor[1:5]

fecha_fore_var_cond_15<-(as.Date("2022-01-01") + 0:14)
graf_var_conf_15_d<-data.frame(Fecha=fecha_fore_var_cond_15, Valor=ug_fore_15@forecast$sigmaFor[1:15]^2,Referencia="Forecast con ARCH(4)")

ug_grafica_forecast_15<-rbind(ug_grafica_total,graf_var_conf_15_d)

g11<-ug_grafica_forecast_15 %>% dplyr::filter(Fecha>'2021-11-30') %>% 
  ggplot(aes(x = Fecha, y = Valor,color=Referencia )) +
  geom_line()



g12<-ggplot(graf_var_conf_15_d, aes(x = Fecha, y = Valor )) +
  geom_line()

(g12 + g11)

```

Las métricas obtenidas para el modelo ARCH (4) seleccionado en el pronóstico propuesto se muestran el el siguiente cuadro:

```{r,echo=F}
selected_model_arch_rmse <- Metrics::rmse(predictions_selected, target_values)
selected_model_arch_mae <- Metrics::mae(predictions_selected, target_values)
selected_model_arch_mape <- Metrics::mape(predictions_selected, target_values)

tabla_fcst_arch <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("0.0540","0.0510","0.9520")
                                      )

knitr::kable(tabla_fcst_arch,
        caption = "Resumen de metricas para el pronóstico de volatilidad con ARCH(4) fuera de la muestra a un día con horizonte deslizante de tamaño 300"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 10)




```


```{r,grafCuanti25, fig.height=5, fig.width=10.5, fig.cap="Gráfico de pronóstico de varianza condicional (h=30), varianza condicional estimada con ARCH(4) y residuos al cuadrado de modelo ARIMA ",echo=F,eval=FALSE}

ug_fore_30<-ugarchforecast(ugfit,n.ahead=30)

#Se crean Los data frames para graficar la varianza condicional.

#ug_fore_5@forecast$sigmaFor[1:5]

fecha_fore_var_cond_30<-(as.Date("2022-01-01") + 0:29)
graf_var_conf_30_d<-data.frame(Fecha=fecha_fore_var_cond_30, Valor=ug_fore_30@forecast$sigmaFor[1:30]^2,Referencia="Forecast con ARCH(4)")

ug_grafica_forecast_30<-rbind(ug_grafica_total,graf_var_conf_30_d)

g15<-ug_grafica_forecast_30 %>% dplyr::filter(Fecha>'2021-11-30') %>% 
  ggplot(aes(x = Fecha, y = Valor,color=Referencia )) +
  geom_line()



g14<-ggplot(graf_var_conf_30_d, aes(x = Fecha, y = Valor )) +
  geom_line()

(g14 + g15)

```

Dentro de las debilidades de los modelos ARCH se pueden mencionar que el modelo asume que los shocks positivos y negativos tienen el mismo efecto en la volatilidad porque dependen del cuadrado de los shocks previos. Esto no se observa en la serie donde los shocks se ven al alza y a la baja de precio. Otro aspecto a considerar es la leptocurtosis en los residuos detectada en el modelado con ARCH(4), situación descripta como hecho estilizado en la bibliografía empleada en el marco teórico.

```{r,grafCuanti26, fig.height=5, fig.width=10.5, fig.cap="Precios de Cierre - Varianza Condicional (log(Retornos)) ARCH(4)",echo=F,eval=F,include=FALSE}

g16<-prueba %>% autoplot((value))+ ylab("Precio Cierre ETH/USD")+xlab("Fecha")

#ug_var<-ugfit@fit$var

g17<-ug_grafica_total %>% filter(Referencia=='Varianza Condicional Estimada') %>% 
  ggplot(aes(x=Fecha, y=Valor))+geom_line()+ylab('Varianza Condicional ARCH(4)')+
  ylim(0,0.06)

g16/
g17



  
```


El modelo ARCH es restrictivo donde por ejemplo $\alpha_1$ en un ARCH(1) debe estar en el [0,1/3]. El modelo ARCH no provee información sobre la fuente que ocasiona la variación del precio de la serie temporal. ARCH tiende a sobre predecir la volatilidad porque responde lentamente a grandes shocks aislados en las series de tiempo (Tsay, 2010). 

\newpage
## Modelo GARCH:

Continuando con la modelización de la varianza condicional se aplica el modelo ARCH generalizado de Bollerslev (1986).

Para la serie de retornos logarítmicos $r_t$, se considera $a_t=r_t-\mu_t$ la innovación en el momento t. Entonces $a_t$ sigue un modelo GARCH(m,s) si
$a_t=\sigma_t\epsilon_t$ y 


\begin{equation}\large{\mathbf{\sigma_t^2=\alpha_0+\sum_{i=1}^m\alpha_ia_{t-i}^2+\sum_{j=1}^s\beta_j\sigma_{t-j}^2}}\end{equation}

donde $\epsilon_t$ es una secuencia de variables aleatorias independientes e idénticamente distribuidas con media cero y varianza 1, $\alpha_0>0$, $\alpha_i\geq0$,$\beta_j\geq0$ y $\sum_{i=1}^{max(m,s)}(\alpha_i+\beta_j)<1$. $\alpha_i$ es el parámetro ARCH y $\beta_j$ es el parámetro GARCH.

Un valor grande de $a_{t-1}^2$ o de $\sigma_{t-1}^2$ dan origen a una gran varianza. El valor grande de $a_{t-1}$ tiende a ser seguido por otro valor grande de $a_t$ y esto ocasiona comportamiento de clúster de volatilidad que se mencionó anteriormente.

De manera similar a los modelos ARCH la distribución de un modelo GARCH tiene colas más pesadas que la distribución normal. El modelo provee al igual que el ARCH una ecuación paramétrica para evaluar la evolución de la volatilidad. Los pronósticos de más de un paso en adelante con GARCH convergen a la varianza no condicional del mismo modo que en el caso de los ARCH. Dentro de las desventajas del modelo se menciona que tiene el mismo efecto que el ARCH en relación a los shocks positivos y negativos.

Del mismo modo que en el modelo ARCH se estudia el ajuste del modelo con diferentes distribuciones de densidad condicional para las innovaciones. En la tabla a continuación se detallan los resultados de los tests sobre los residuos de los modelos con un nivel de confianza del 5\%.

```{r,echo=FALSE}
tabla_garch<-read_xlsx('~/tabla_resumen_garch.xlsx')#,header=TRUE,sep=',')

knitr::kable(tabla_garch,
        caption = "Resumen de tests en ajuste de modelos GARCH(1,1)"
        ) %>%
  kable_styling(full_width = T,latex_options = c("hold_position"), font_size = 10)

```

Donde las distribuciones empleadas son NORM para la distribución normal, SNORM la distribución normal sesgada, STD la distribución t-Student, SSTD la t-Student sesgada, GED la distribución de error generalizada, SGED la distribución de error generalizada sesgada, NIG la distribución normal inversa, GHYP la distribución generalizada hiperbólica. De acuerdo a la información resumida en el cuadro anterior, se pudo obtener coeficientes significativos del modelo sólo en el caso de GARCH(1,1) bajo el supuesto de distribución de densidad condicional normal par las innovaciones y los residuos obtenidos del modelo son leptocúrticos y se rechaza el test de bondad de ajuste de Pearson con valor p de cero. Por otro lado, en el GARCH (1,1) con distribución de innovaciones normal no se rechazan las hipótesis nulas en los test de correlación de residuos ni de residuos al cuadrado, tampoco el test ARCH de rezagos superiores con lo que se considera que el orden del modelo GARCH (1,1) seleccionado es apropiado en relación al orden del modelo. 

Se ajusta a continuación el modelo GARCH (1,1) y su ecuación resulta:

\begin{equation}\large{\mathbf{\sigma_t^2=\alpha_0+\alpha_1a_{t-1}^2+\beta_1\sigma_{t-1}^2}}\end{equation}

Donde $\alpha_0=0.0002$,$\alpha_1=0.1337$, $\beta_1=0.7845$ 

Con estos coeficientes el desvío estándar no condicional estimado resulta
$\sqrt{\frac{0.0002}{1-(0.1337+0.7845)}}=0.0586$, similar al obtenido con el modelo ARCH(4)(0.0582). 

En este caso la suma de $\alpha_1+\beta_1=0.9182$, cercana a 1 lo que indica que los shocks de volatilidad son persistentes, representando que  los cambios grandes de retornos tienden a ser seguidos por grandes cambios y los cambios pequeños de retornos tienden a ser seguidos por pequeños cambios.


```{r,grafCuanti27, fig.height=4, fig.width=5.5, fig.cap="Gráfico valor absoluto de residuos de modelo ARFIMA y desvío estandar condicional estimado con GARCH(1,1)",echo=F}
#variance.model=list(model="iGARCH", garchOrder=c(1,1))

#ug_spec<-ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(2, 0), 
#submodel = NULL, external.regressors = NULL, variance.targeting = FALSE),mean.model=list(armaOrder=c(1,1))) # no se puede especificar bien el ARIMA o ARFIMA.

ug_spec_GARCH<-ugarchspec(mean.model = list(arfima=TRUE),distribution.model = "norm")#,arfima=TRUE)) # no se puede especificar bien el ARIMA o ARFIMA.



ugfit_GARCH<-ugarchfit(spec=ug_spec_GARCH,data=prueba_arch_30_ts)

#plot(ugfit_GARCH)

ug_var_GARCH<-ugfit_GARCH@fit$sigma # son las varianzas condicionales estimadas

ug_res2_GARCH<-abs(ugfit_GARCH@fit$residuals)

#Armado del DF para gráfica de residuos y Varianzas Condicionales
Fecha_ugfit_GARCH <- (as.Date("2017-01-01") + 1:1855)
#length(Fecha_ugfit)
ug_grafica_GARCH<-data.frame(Fecha=Fecha_ugfit_GARCH,Valor=ug_var_GARCH)
ug_grafica_GARCH<-mutate(ug_grafica_GARCH, Referencia="Desvío Estándar Condicional Estimada GARCH(1,1)")
ug_grafica2_GARCH<-data.frame(Fecha=Fecha_ugfit_GARCH,Valor=ug_res2_GARCH)
ug_grafica2_GARCH<-mutate(ug_grafica2_GARCH, Referencia="Valor Absoluto de Residuos ")
ug_grafica_total_GARCH<-rbind(ug_grafica_GARCH,ug_grafica2_GARCH)
#gráfico de residuos y varianzas condicionales:
g18<-ggplot(ug_grafica_total_GARCH, aes(x = Fecha, y = Valor , color = Referencia)) +
  geom_line(linetype="solid",alpha=0.7) +
  theme(legend.position = "bottom")+scale_color_manual(values=c('#FFFF00','#000000')) 
g18


```

Se muestran a continuación las funciones de autocorrelación de los residuos estandarizados y de los residuos estandarizados al cuadrado.

```{r, graffoto5,fig.align='center',fig.height=6,fig.width=6,fig.cap="FAC Residuos Estandarizados al cuadrado Modelo GARCH(1,1). Elaboración Propia",fig.scap="FAC Residuos Estandarizados al cuadrado Modelo GARCH(1,1)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\ACF_Residuos_Estandarizados_cuadrado_GARCH(1,1).png")
```


```{r, graffoto6,fig.align='center',fig.height=6,fig.width=6,fig.cap="FAC Residuos estandarizados GARCH(1,1). Elaboración Propia",fig.scap="FAC Residuos estandarizados GARCH(1,1)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\ACF_Residuos_Estandarizados_GARCH(1,1).png")
```


En relación a los gráficos se puede observar que el modelo parece ser adecuado para describir la dependencia lineal tanto en la serie de los retornos como en la volatilidad.

Del mismo modo que con el modelo ARCH se realizan los pronósticos para el desvío estandar  condicional utlizando un pronóstico fuera de la muestra con un horizonte de 1 día y horizonte deslizante de 300 muestras fuera de la muestra. Se presentan a continuación las gráficas obtenidas con el pronóstico de GARCH(1,1).

```{r,eval=T, include=F, echo=FALSE}

ug_spec_GARCH<-ugarchspec(mean.model = list(arfima=TRUE),distribution.model = "norm")

ugfit_GARCH_2<-ugarchfit(spec=ug_spec_GARCH,data=prueba_arch_30_ts,out.sample = 300)


forecast_garch<-ugarchforecast(ugfit_GARCH_2,n.ahead = 1,n.roll = 300)

predictions_selected_garch <- forecast_garch@forecast$sigmaFor
target_values_garch <- forecast_garch@forecast$seriesFor
selected_model_garch_rmse <- Metrics::rmse(predictions_selected_garch, target_values_garch)
selected_model_garch_mae <- Metrics::mae(predictions_selected_garch, target_values_garch)
selected_model_garch_mape <- Metrics::mape(predictions_selected_garch, target_values_garch)



g67<-plot(forecast_garch, which=2)
g68<-plot(forecast_garch,which=4)


```


```{r, graffoto7,fig.align='center',fig.height=6,fig.width=6,fig.cap="Pronósticos a un día fuera de la muestra (n=300) GARCH(1,1). Elaboración Propia",fig.scap="Pronósticos a un día fuera de la muestra (n=300) GARCH(1,1)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\forecast_garch_rolling_300.png")
```



En el gráfico anterior se observa el comportamiento de los pronósticos a un día con horizonte deslizante para un conjunto fuera de la muestra de tamaño 300. Se muestran los intervalos de confianza al 95% para los valores pronosticados. 

```{r, graffoto8,fig.align='center',fig.height=6,fig.width=6,fig.cap="Pronósticos a un día y serie de valor absoluto de log retornos - modelo GARCH(1,1). Elaboración Propia",fig.scap="Pronósticos a un día y serie de valor absoluto de log retornos - modelo GARCH(1,1)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\forecast_garch_rolling_sigma_abs_series.png")
```


Aquí se observa como los pronósticos fuera de la muestra para la serie replican con un leve retraso las variaciones de los retornos logarítmicos. 

Se presentan a continuación las métricas obtenidas para el pronóstico realizado.

```{r,echo=F}
selected_model_garch_rmse <- Metrics::rmse(predictions_selected_garch, target_values_garch)
selected_model_garch_mae <- Metrics::mae(predictions_selected_garch, target_values_garch)
selected_model_garch_mape <- Metrics::mape(predictions_selected_garch, target_values_garch)

tabla_fcst_garch <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("0.0530","0.0510","0.9430")
                                      )

knitr::kable(tabla_fcst_garch,
        caption = "Resumen de metricas para el pronóstico de volatilidad con GARCH(1,1) fuera de la muestra a un día con horizonte deslizante de tamaño 300"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 10)




```




El modelo generalizado modela la volatilidad de la serie de retornos y estos están relacionados con momentos en los que la serie original sufre cambios de precio relativamente abruptos, aunque los shocks pueden ser positivos o negativos. En relación al modelo ARCH previamente analizado se puede destacar el hecho de que el modelo resulta potencialmente más simple y parsimonioso dado que captura los afectos ARCH(4) con 2 parámetros.




```{r,grafCuanti29, fig.height=5, fig.width=10.5, fig.cap="Gráfico de pronóstico de varianza condicional (h=5), varianza condicional estimada con GARCH (1,1) y residuos al cuadrado de modelo ARIMA ",echo=F,eval=F,include=FALSE}

ug_fore_30_GARCH<-ugarchforecast(ugfit_GARCH,n.ahead=30)
ug_fore_15_GARCH<-ugarchforecast(ugfit_GARCH,n.ahead=15)
ug_fore_5_GARCH<-ugarchforecast(ugfit_GARCH,n.ahead=5)


#Se crean Los data frames para graficar la varianza condicional.

#ug_fore_5@forecast$sigmaFor[1:5]

fecha_fore_var_cond_5<-(as.Date("2022-01-01") + 0:4)
graf_var_conf_5_d_GARCH<-data.frame(Fecha=fecha_fore_var_cond_5, Valor=ug_fore_5_GARCH@forecast$sigmaFor[1:5]^2,Referencia="Forecast con GARCH(1,1)")

ug_grafica_forecast_5_GARCH<-rbind(ug_grafica_total_GARCH,graf_var_conf_5_d_GARCH)

g21<-ug_grafica_forecast_5_GARCH %>% dplyr::filter(Fecha>'2021-11-30') %>% 
  ggplot(aes(x = Fecha, y = Valor,color=Referencia )) +
  geom_line()



g22<-ggplot(graf_var_conf_5_d_GARCH, aes(x = Fecha, y = Valor )) +
  geom_line()

(g22 + g21)

```

```{r,grafCuanti30, fig.height=5, fig.width=10.5, fig.cap="Gráfico de pronóstico de varianza condicional (h=15), varianza condicional estimada con GARCH (1,1) y residuos al cuadrado de modelo ARIMA ",echo=F,include=FALSE, eval=FALSE}

ug_fore_30_GARCH<-ugarchforecast(ugfit_GARCH,n.ahead=30)
ug_fore_15_GARCH<-ugarchforecast(ugfit_GARCH,n.ahead=15)
ug_fore_5_GARCH<-ugarchforecast(ugfit_GARCH,n.ahead=5)


#Se crean Los data frames para graficar la varianza condicional.

#ug_fore_5@forecast$sigmaFor[1:5]

fecha_fore_var_cond_15<-(as.Date("2022-01-01") + 0:14)
graf_var_conf_15_d_GARCH<-data.frame(Fecha=fecha_fore_var_cond_15, Valor=ug_fore_15_GARCH@forecast$sigmaFor[1:15]^2,Referencia="Forecast con GARCH(1,1)")

ug_grafica_forecast_15_GARCH<-rbind(ug_grafica_total_GARCH,graf_var_conf_15_d_GARCH)

g23<-ug_grafica_forecast_15_GARCH %>% dplyr::filter(Fecha>'2021-11-30') %>% 
  ggplot(aes(x = Fecha, y = Valor,color=Referencia )) +
  geom_line()



g24<-ggplot(graf_var_conf_15_d_GARCH, aes(x = Fecha, y = Valor )) +
  geom_line()

(g24 + g23)

```



```{r,grafCuanti28, fig.height=5, fig.width=10.5, fig.cap="Gráfico de pronóstico de varianza condicional (h=30), varianza condicional estimada con GARCH (1,1) y residuos al cuadrado de modelo ARIMA ",echo=F,eval=FALSE, include=FALSE}

ug_fore_30_GARCH<-ugarchforecast(ugfit_GARCH,n.ahead=30)
ug_fore_15_GARCH<-ugarchforecast(ugfit_GARCH,n.ahead=15)
ug_fore_5_GARCH<-ugarchforecast(ugfit_GARCH,n.ahead=5)


#Se crean Los data frames para graficar la varianza condicional.

#ug_fore_5@forecast$sigmaFor[1:5]

fecha_fore_var_cond_30<-(as.Date("2022-01-01") + 0:29)
graf_var_conf_30_d_GARCH<-data.frame(Fecha=fecha_fore_var_cond_30, Valor=ug_fore_30_GARCH@forecast$sigmaFor[1:30]^2,Referencia="Forecast con GARCH(1,1)")

ug_grafica_forecast_30_GARCH<-rbind(ug_grafica_total_GARCH,graf_var_conf_30_d_GARCH)

g19<-ug_grafica_forecast_30_GARCH %>% dplyr::filter(Fecha>'2021-11-30') %>% 
  ggplot(aes(x = Fecha, y = Valor,color=Referencia )) +
  geom_line()



g20<-ggplot(graf_var_conf_30_d_GARCH, aes(x = Fecha, y = Valor )) +
  geom_line()

(g20 + g19)

```


```{r,grafCuanti31, fig.height=9, fig.width=10.5, fig.cap="Precios de Cierre - Varianza Condicional (log(Retornos)) GARCH(1,1) - ARCH(4) ",echo=F,include=FALSE,eval=FALSE}

g25<-prueba %>% autoplot((value))+ ylab("Precio Cierre ETH/USD")+xlab("Fecha")

#ug_var<-ugfit@fit$var

g26<-ug_grafica_total_GARCH %>% filter(Referencia=='Varianza Condicional Estimada GARCH(1,1)') %>% 
  ggplot(aes(x=Fecha, y=Valor))+geom_line()+ylab('Varianza Condicional GARCH(1,1)')+ ylim(0,0.06)

g25/
g26
#g17




  
```

\newpage
## Modelo EGARCH:

Para el modelado de la varianza el paquete de R (rugarch) utiliza la siguiente función:

\begin{equation}\large{\mathbf{ln(\sigma_t^2)=\omega+\sum_{j=1}^q(\alpha_jz_{t-j}+\gamma_j(|z_{t-j}|-E(|z_{t-j}|)))+\sum_{j=1}^p\beta_j ln(\sigma_{t-j}^2)}}\end{equation}

Donde la ecuación anterior es una adaptación a la presentada por Ghalanos(2022) por el hecho de no incluir variables regresoras externas en el modelo. Los coeficientes $\alpha_j$ capturan el efecto de signo y $\gamma_j$ el efecto de tamaño. La persistencia se da como $\hat{P}=\sum_{j=1}^p \beta_j$.

Del mismo modo que con los modelos ARCH y GARCH se estima modelos EGARCH con diferentes distribuciones de densidad condicional para las innovaciones. Los resultados se presentan en la tabla a continuación. Todos los tests a un nivel de confianza alfa del 5\%.


```{r,echo=FALSE}
tabla_egarch<-read_xlsx('~/tabla_resumen_egarch.xlsx')#,header=TRUE,sep=',')

knitr::kable(tabla_egarch,
        caption = "Resumen de tests en ajuste de modelos eGARCH(1,1)"
        ) %>%
  kable_styling(full_width = T,latex_options = c("hold_position"), font_size = 10)

```

Donde las distribuciones empleadas son NORM para la distribución normal, SNORM la distribución normal sesgada, STD la distribución t-Student, SSTD la t-Student sesgada, GED la distribución de error generalizada, SGED la distribución de error generalizada sesgada, NIG la distribución normal inversa, GHYP la distribución generalizada hiperbólica. De acuerdo a la información resumida en el cuadro anterior, se pudo obtener coeficientes significativos del modelo sólo en el caso de EGARCH(1,1), de manera parcial porque uno de los coeficientes no resulta significativo a un nivel $\alpha=0.05$, bajo el supuesto de distribución de densidad condicional normal par las innovaciones y los residuos obtenidos del modelo son leptocúrticos y se rechaza el test de bondad de ajuste de Pearson con valor p cercano a cero. Por otro lado, en el EGARCH (1,1) con distribución de innovaciones normal no se rechazan las hipótesis nulas en los test de correlación de residuos ni de residuos al cuadrado, tampoco el test ARCH de rezagos superiores con lo que se considera que el orden del modelo EGARCH (1,1) seleccionado es apropiado en relación al orden del modelo. 

Se estima a continuación un modelo EGARCH(1,1). La ecuación resultante es:

\begin{equation}\large{\mathbf{ln(\sigma_t^2)=\omega+\alpha_1z_{t-j}+\gamma_1(|z_{t-j}|-E(|z_{t-j}|)))+\beta_1 ln(\sigma_{t-j}^2)}}\end{equation}

Donde los coeficientes resultan: $\omega=-0.4354$, $\alpha_1=-0.0211$, $\beta_1=0.9220$, $\gamma_1=0.2267$. La persistencia estimada del modelo es relativamente alta en relación al nivel del coeficiente $\beta$. El efecto del leverage resulta positivo y significativo. El coeficiente $\alpha_1$ no resulta significativo a un nivel del 5\%,
sin embargo, los demás coeficientes del modelo son significativos a ese nivel, los estadísticos de Ljung Box sobre residuos estandarizados y sobre los residuos estandarizados al cuadrado indican que no hay autocorrelación de residuos del modelo. Los residuos del modelo resultan leptocúrticos. 

A continuación se grafican la varianza condicional estimada con EGARCH(1,1) versus los residuos al cuadrado.

```{r,grafCuanti32, fig.height=4, fig.width=5.5, fig.cap="Gráfico residuos al cuadrado de modelo ARFIMA y EGARCH(1,1)",echo=F}

ug_spec_EGARCH<-ugarchspec(mean.model = list(armaOrder = c(1, 1),arfima=TRUE),variance.model = list(model="eGARCH",garchOrder=c(1,1)))#,arfima=TRUE)) # no se puede especificar bien el ARIMA o ARFIMA.

#mean.model = list(arfima=TRUE)

ugfit_EGARCH<-ugarchfit(spec=ug_spec_EGARCH,data=na.omit(difference(log(prueba$value))))


ug_var_EGARCH<-ugfit_EGARCH@fit$sigma # son las varianzas condicionales estimadas

ug_res2_EGARCH<-abs(ugfit_EGARCH@fit$residuals)

#Armado del DF para gráfica de residuos y Varianzas Condicionales
Fecha_ugfit_EGARCH <- (as.Date("2017-01-01") + 1:1825)
#length(Fecha_ugfit)
ug_grafica_EGARCH<-data.frame(Fecha=Fecha_ugfit_EGARCH,Valor=ug_var_EGARCH)
ug_grafica_EGARCH<-mutate(ug_grafica_EGARCH, Referencia="SD Condicional Estimada EGARCH(1,1)")
ug_grafica2_EGARCH<-data.frame(Fecha=Fecha_ugfit_EGARCH,Valor=ug_res2_EGARCH)
ug_grafica2_EGARCH<-mutate(ug_grafica2_EGARCH, Referencia="Residuos ARFIMA (valor absoluto)")
ug_grafica_total_EGARCH<-rbind(ug_grafica_EGARCH,ug_grafica2_EGARCH)
#gráfico de residuos y varianzas condicionales:
g30<-ggplot(ug_grafica_total_EGARCH, aes(x = Fecha, y = Valor , color = Referencia)) +
  geom_line() +
  theme(legend.position = "bottom")+scale_color_manual(values=c('#000000','#40E0D0')) 
g30

```

```{r,include=FALSE,echo=FALSE}

  ug_spec_EGARCH<-ugarchspec(mean.model = list(armaOrder = c(1, 1),arfima=TRUE),variance.model = list(model="eGARCH",garchOrder=c(1,1)),distribution.model = "sstd")#,arfima=TRUE)) # no se puede especificar bien el ARIMA o ARFIMA.

  #mean.model = list(arfima=TRUE)

  ugfit_EGARCH_2<-ugarchfit(spec=ug_spec_EGARCH,data=na.omit(difference(log(prueba$value))),out.sample = 300)
  
#plot(ugfit_EGARCH_2)




forecast_egarch<-ugarchforecast(ugfit_EGARCH_2,n.ahead = 1,n.roll = 300)




predictions_selected_egarch <- forecast_egarch@forecast$sigmaFor
target_values_egarch <- forecast_egarch@forecast$seriesFor
selected_model_egarch_rmse <- Metrics::rmse(predictions_selected_egarch, target_values_egarch)
selected_model_egarch_mae <- Metrics::mae(predictions_selected_egarch, target_values_egarch)
selected_model_egarch_mape <- Metrics::mape(predictions_selected_egarch, target_values_egarch)



g69<-plot(forecast_egarch, which=2)
g70<-plot(forecast_egarch,which=4)


```


```{r, graffoto9,fig.align='center',fig.height=6,fig.width=6,fig.cap="FAC Residuos estandarizados al cuadrado EGARCH(1,1). Elaboración Propia",fig.scap="FAC Residuos estandarizados al cuadrado EGARCH(1,1)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\ACF_residuos_estandarizados_cuadrado_EGARCH(1,1).png")
```

```{r, graffoto10,fig.align='center',fig.height=6,fig.width=6,fig.cap="FAC Residuos estandarizados EGARCH(1,1). Elaboración Propia",fig.scap="FAC Residuos estandarizados EGARCH(1,1)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\ACF_residuos_estandarizados_EGARCH(1,1).png")
```


Del mismo modo que en los modelos ARCH (4) y GARCH (1,1) se realiza el pronóstico de la volatilidad como el desvío estandar condicional. Para ello se utiliza un pronóstico fuera de la muestra a un día con horizonte deslizante de 300 muestras. Los valores pronosticados entonces corresponden a los últimos 300 días del estudio desde el 5/4/2021 al 30/1/2022.

```{r, graffoto11,fig.align='center',fig.height=6,fig.width=6,fig.cap="Pronósticos a un día fuera de la muestra (n=300) EGARCH(1,1)",fig.scap="Pronósticos a un día fuera de la muestra (n=300) EGARCH(1,1)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\forecast_egarch_300_rolling.png")
```



En el gráfico anterior se observa el comportamiento de los pronósticos a un día con horizonte deslizante para un conjunto fuera de la muestra de tamaño 300. Se muestran los intervalos de confianza al 95% para los valores pronosticados. 


```{r, graffoto12,fig.align='center',fig.height=6,fig.width=6,fig.cap="Pronósticos a un día y serie de valor absoluto de log retornos - modelo EGARCH(1,1)",fig.scap="Pronósticos a un día y serie de valor absoluto de log retornos - modelo EGARCH(1,1)",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\forecast_egarch_sigma_valor_abs.png")
```


Aquí se observa como los pronósticos fuera de la muestra para la serie replican con un leve retraso las variaciones de los retornos logarítmicos. 

Se presentan a continuación las métricas obtenidas para el pronóstico realizado.


```{r,echo=F}

predictions_selected_egarch <- forecast_egarch@forecast$sigmaFor
target_values_egarch <- forecast_egarch@forecast$seriesFor
selected_model_egarch_rmse <- Metrics::rmse(predictions_selected_egarch, target_values_egarch)
selected_model_egarch_mae <- Metrics::mae(predictions_selected_egarch, target_values_egarch)
selected_model_egarch_mape <- Metrics::mape(predictions_selected_egarch, target_values_egarch)




tabla_fcst_egarch <- tibble::tibble(  Métrica = c("RMSE","MAE","MAPE %"),
                    Valor = c("0.0540","0.0530","0.9460")
                                      )

knitr::kable(tabla_fcst_egarch,
        caption = "Resumen de metricas para el pronóstico de volatilidad con EGARCH(1,1) fuera de la muestra a un día con horizonte deslizante de tamaño 300"
        ) %>%
  kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 10)




```


```{r,grafCuanti33, fig.height=5, fig.width=10.5, fig.cap="Gráfico de pronóstico de varianza condicional (h=5), varianza condicional estimada con EGARCH (1,1) y residuos al cuadrado de modelo ARIMA ",echo=F, include=FALSE,eval=FALSE}

ug_fore_30_EGARCH<-ugarchforecast(ugfit_EGARCH,n.ahead=30)
ug_fore_15_EGARCH<-ugarchforecast(ugfit_EGARCH,n.ahead=15)
ug_fore_5_EGARCH<-ugarchforecast(ugfit_EGARCH,n.ahead=5)

#plot(ug_fore_30_EGARCH)

#Se crean Los data frames para graficar la varianza condicional.

#ug_fore_5@forecast$sigmaFor[1:5]

fecha_fore_var_cond_5<-(as.Date("2022-01-01") + 0:4)
graf_var_conf_5_d_EGARCH<-data.frame(Fecha=fecha_fore_var_cond_5, Valor=ug_fore_5_EGARCH@forecast$sigmaFor[1:5]^2,Referencia="Forecast con EGARCH(1,1)")

ug_grafica_forecast_5_EGARCH<-rbind(ug_grafica_total_EGARCH,graf_var_conf_5_d_EGARCH)

g31<-ug_grafica_forecast_5_EGARCH %>% dplyr::filter(Fecha>'2021-11-30') %>% 
  ggplot(aes(x = Fecha, y = Valor,color=Referencia )) +
  geom_line()



g32<-ggplot(graf_var_conf_5_d_EGARCH, aes(x = Fecha, y = Valor )) +
  geom_line()

(g32 + g31)

```

Para un horizonte de 15 días:

```{r,grafCuanti34, fig.height=5, fig.width=10.5, fig.cap="Gráfico de pronóstico de varianza condicional (h=15), varianza condicional estimada con EGARCH (1,1) y residuos al cuadrado de modelo ARIMA ",echo=F,include=FALSE,eval=FALSE}

ug_fore_30_GARCH<-ugarchforecast(ugfit_EGARCH,n.ahead=30)
ug_fore_15_GARCH<-ugarchforecast(ugfit_EGARCH,n.ahead=15)
ug_fore_5_GARCH<-ugarchforecast(ugfit_EGARCH,n.ahead=5)


#Se crean Los data frames para graficar la varianza condicional.

#ug_fore_5@forecast$sigmaFor[1:5]

fecha_fore_var_cond_15<-(as.Date("2022-01-01") + 0:14)
graf_var_conf_15_d_EGARCH<-data.frame(Fecha=fecha_fore_var_cond_15, Valor=ug_fore_15_EGARCH@forecast$sigmaFor[1:15]^2,Referencia="Forecast con EGARCH(1,1)")

ug_grafica_forecast_15_EGARCH<-rbind(ug_grafica_total_EGARCH,graf_var_conf_15_d_EGARCH)

g33<-ug_grafica_forecast_15_EGARCH %>% dplyr::filter(Fecha>'2021-11-30') %>% 
  ggplot(aes(x = Fecha, y = Valor,color=Referencia )) +
  geom_line()



g34<-ggplot(graf_var_conf_15_d_EGARCH, aes(x = Fecha, y = Valor )) +
  geom_line()

(g34 + g33)

```



```{r,grafCuanti35, fig.height=5, fig.width=10.5, fig.cap="Gráfico de pronóstico de varianza condicional (h=30), varianza condicional estimada con EGARCH (1,1) y residuos al cuadrado de modelo ARIMA ",echo=F,include=FALSE,eval=FALSE}

ug_fore_30_EGARCH<-ugarchforecast(ugfit_EGARCH,n.ahead=30)
ug_fore_15_EGARCH<-ugarchforecast(ugfit_EGARCH,n.ahead=15)
ug_fore_5_EGARCH<-ugarchforecast(ugfit_EGARCH,n.ahead=5)


#Se crean Los data frames para graficar la varianza condicional.

#ug_fore_5@forecast$sigmaFor[1:5]

fecha_fore_var_cond_30<-(as.Date("2022-01-01") + 0:29)
graf_var_conf_30_d_EGARCH<-data.frame(Fecha=fecha_fore_var_cond_30, Valor=ug_fore_30_EGARCH@forecast$sigmaFor[1:30]^2,Referencia="Forecast con EGARCH(1,1)")

ug_grafica_forecast_30_EGARCH<-rbind(ug_grafica_total_EGARCH,graf_var_conf_30_d_EGARCH)

g35<-ug_grafica_forecast_30_EGARCH %>% dplyr::filter(Fecha>'2021-11-30') %>% 
  ggplot(aes(x = Fecha, y = Valor,color=Referencia )) +
  geom_line()



g36<-ggplot(graf_var_conf_30_d_EGARCH, aes(x = Fecha, y = Valor )) +
  geom_line()

(g36 + g35)

```


\newpage

```{r,grafCuanti36, fig.height=10.5, fig.width=10.5, fig.cap="Precios de Cierre - Varianza Condicional (log(Retornos)) EGARCH(1,1) - GARCH(1,1) - ARCH(4) ",echo=F,eval=F,include=FALSE}

g25<-prueba %>% autoplot((value))+ ylab("Precio Cierre ETH/USD")+xlab("Fecha")

#ug_var<-ugfit@fit$var

g37<-ug_grafica_total_EGARCH %>% filter(Referencia=='Varianza Condicional Estimada EGARCH(1,1)') %>% 
  ggplot(aes(x=Fecha, y=Valor))+geom_line()+ylab('Varianza Condicional EGARCH(1,1)')+ ylim(0,0.06)

g25/
g37/
g26/
g17

```

```{r,include=FALSE,eval=FALSE}

#Comparación modelos ARCH-GARCH-EGARCH

#Criterios de informacíón:

#ARCH:

#Mincer-Zarnowitz: regresion 

ARCH_N.mz <- lm(as.vector(predictions_selected)~as.vector(target_values))  
summary(ARCH_N.mz)$coef

error3<-as.vector(predictions_selected)-as.vector(target_values)

GARCH_N.mz<-lm(as.vector(predictions_selected_garch)~as.vector(target_values_garch))
summary(GARCH_N.mz)$coef

EGARCH_N.mz<-lm(as.vector(predictions_selected_egarch)~as.vector(target_values_egarch))

summary(EGARCH_N.mz)$coef

forecast_egarch@seriesFor

forecast_egarch@forecast$seriesFor
forecast_egarch@forecast$sigmaFor

forecast_garch@forecast$seriesFor
forecast_garch@forecast$sigmaFor

error1<-as.vector(forecast_egarch@forecast$seriesFor)-as.vector(forecast_egarch@forecast$sigmaFor)

error2<-as.vector(forecast_garch@forecast$seriesFor)-as.vector(forecast_garch@forecast$sigmaFor)



dm_egarch_garch<-dm.test(error1,error2,h=1,power=1)
dm_arch_garch<-dm.test(error2, error3, h=1,power=1)
dm_arch_egarch<-dm.test(error3,error1,h=1,power=1)


```






\newpage
\newpage
## Redes neuronales-NNETAR:

A continuación se modela la serie de precios de cierre de Ethereum ETH/USD con una red neuronal de una capa oculta y para la realización de los pronósticos se realiza la forma iterativa de manera de pronosticar un paso adelante y luego tomar ese pronóstico como un nuevo dato histórico y avanzar sucesivamente.

El trabajo con redes neuronales permiten identificar estructuras no lineales en el comportamiento de la serie. Se utilizará la función NNETAR del paquete de R Forecast que realiza modelos con una capa oculta de redes neuronales de alimentación y paso hacía adelante (FNN) en donde cada uno de los nodos recibe entradas de las capas anteriores. No se produce la realimentación entre capas que se da en los modelos RNN, o de redes neuronales recursivas.  

Se utiliza la notación NNETAR(p,k) donde p es el número de rezagos que se utilizan como entradas y k es el número de nodos ocultos que están presentes (cantidad de neuronas en la capa oculta).

Para el trabajo con NNETAR se estableció el mismo esquema de validación cruzada que en el resto de los modelos para la media. Además para la evaluación se separó un conjunto de entrenamiento y de testeo. Al requerir para la modelización la mayor cantidad de datos disponibles se elige dividir el conjunto de entrenamiento final con todos los datos disponibles desde el 01-01-2017 al 30-12-2021 y utilizar los días del mes de enero de 2022 como conjunto de testeo para los distintos horizontes de pronóstico. 

En las figuras a continuación se muestra el esquema de valización cruzada propuesto y la división de entrenamiento y testeo para el horizonte de 30 días (los demás esquemas de validación cruzada y entrenamiento/testeo se incluyen en el Apéndice).


```{r,include=F,eval=F, echo=FALSE,message=FALSE,warning=FALSE}

ethTSTEST_tibble<-as_tibble(ethTSTEST)

prueba_nnetar<-tsibble::tsibble(
  date = as.Date("2017-01-01") + 0:1855,
  value = ethTSTEST_tibble$x
)


 
splits<- time_series_split(data.frame(prueba_nnetar),assess= "30 days", cumulative = TRUE)
 
splits %>% tk_time_series_cv_plan() %>% 
  plot_time_series_cv_plan(date, value)


receta_nodos_1 <- recipe(value ~ ., data = training(splits)) %>% 
  step_timeseries_signature(date) %>% step_dummy(all_nominal(),one_hot = TRUE)
  
 
 
m_nnetar_nodos <- workflow() %>%
  add_recipe(receta_nodos_1) %>%
  add_model(nnetar_reg() %>% set_engine("nnetar"))
 
resample_spec <- time_series_cv(data = training(splits),
                                initial     = 300,
                                assess      = 30,
                                skip        = 200,
                                cumulative  = FALSE,
                                slice_limit = 8)


readr::write_rds(resample_spec, here::here('resample_spec.RDS'))

#resample_spec

# VISUALIZE CV PLAN ----

# Select date and value columns from the tscv diagnostic tool
#resample_spec %>% tk_time_series_cv_plan()

# Plot the date and value columns to see the CV Plan
resample_spec %>%
    plot_time_series_cv_plan(date, value, .interactive = FALSE,.title = "")



 
#NNAR modelo secuencial ARIMA, ARIMA BOOST, NNETAR, ETS, TBATS no se puede usar k-fold validation por la randomización.
 
#Receta:
 
m_nnetar_nodos %>% pull_workflow_preprocessor() %>% prep() %>% juice() %>% glimpse()
 
# Especificaciones del modelo:
 
m_nnetar_nodos %>% pull_workflow_spec()
 
model_spec_nnetar<-nnetar_reg(
  seasonal_period = 1,
  seasonal_ar = 0,
  non_seasonal_ar = tune(),
  hidden_units = tune(),
  num_networks = 20,
  penalty=tune(),
  epochs= tune()) %>% set_engine("nnetar") 
 

# en model_spec_nnetar se tiene que dar una indicación inicual para poder hacer luego tunning de hiperparámetros.

# Primera ronda de tunning:
set.seed(44)
grid_spec_nnetar_1<-grid_latin_hypercube(
  parameters(model_spec_nnetar),
  size=15
)

readr::write_rds(grid_spec_nnetar_1, here::here('grid_spec_nnetar_1.RDS'))

 
# Tuning:
 
wflw_tune_nnetar<-m_nnetar_nodos %>% update_model(model_spec_nnetar)
 
# correr la grilla de tuneo:

set.seed(44)
tune_results_nnetar_1<-wflw_tune_nnetar %>% 
  tune_grid(
    resamples=resample_spec,
    grid= grid_spec_nnetar_1,
    metrics = default_forecast_accuracy_metric_set(),
    control= control_grid(verbose=TRUE, save_pred=TRUE)
  )

 
# ver resultados:
 
resultados_nnetar_1<-tune_results_nnetar_1 %>% show_best(metric="rmse", n=Inf)
readr::write_rds(resultados_nnetar_1, here::here('resultados_nnetar_1.RDS'))

 
g40<-tune_results_nnetar_1 %>% autoplot()+
  geom_smooth(se=FALSE)

readr::write_rds(g40, here::here('g40.RDS'))

 
ggplotly(g40)
 
# Segunda ronda de tuning:
set.seed(44)
grid_spec_nnetar_2<-grid_latin_hypercube(
  non_seasonal_ar(range=c(1,5)),
  hidden_units(range=c(5,10)),
  penalty(range=c(0.01,-5),trans=scales::log10_trans()),
  epochs(range=c(0,500)),
  size=15
)

readr::write_rds(grid_spec_nnetar_2, here::here('grid_spec_nnetar_2.RDS'))

 
# Seteamos procesamiento en paralelo:

#Segunda ronda de tuneo:
 
wflw_tune_nnetar_2<-m_nnetar_nodos %>% update_model(model_spec_nnetar)

set.seed(44)
tune_results_nnetar_2<-wflw_tune_nnetar_2 %>% 
  tune_grid(
    resamples=resample_spec,
    grid= grid_spec_nnetar_2,
    metrics = default_forecast_accuracy_metric_set(),
    control= control_grid(verbose=TRUE, save_pred=TRUE)
  )
# Vemos los mejores resultados de la etapa 2:
 
resultados_nnetar_2<-tune_results_nnetar_2 %>% show_best(metric="rmse", n=Inf)
readr::write_rds(resultados_nnetar_2, here::here('resultados_nnetar_2.RDS'))

 
g41<-tune_results_nnetar_2 %>% autoplot()+
  geom_smooth(se=FALSE)
readr::write_rds(g41, here::here('g41.RDS'))

 
ggplotly(g41)
 
#######################################
# Re Entrenamiento 
 
set.seed(042)
wflw_fit_nnetar_tscv<-wflw_tune_nnetar %>% finalize_workflow(
  tune_results_nnetar_2 %>% show_best(metric="rmse", n=Inf) %>% 
    slice(2)
        ) %>% 
              fit(training(splits))
 
res<-calibrate_and_plot(wflw_fit_nnetar_tscv,.data = prueba_nnetar,.splits_obj = splits,.interactive = TRUE)

readr::write_rds(res, here::here('res_nnetar_30.RDS'))


########################################

```

```{r,grafCuanti37, fig.height=12, fig.width=9,fig.cap="Gráfico de Esquema de Validación Cruzada (h=30 días)",echo=F}

resample_spec<-readRDS('~/resample_spec.RDS')

g38<-resample_spec %>% 
  tk_time_series_cv_plan() %>% mutate(.key = case_when(
    .key == "training" ~ "Entrenamiento",
    .key == "testing" ~ "Testeo",
    TRUE ~ .key
  ),.id = case_when(
    .id=="Slice1"~"Conjunto 1",
    .id=="Slice2"~"Conjunto 2",
    .id=="Slice3"~"Conjunto 3",
    .id=="Slice4"~"Conjunto 4",
    .id=="Slice5"~"Conjunto 5",
    .id=="Slice6"~"Conjunto 6",
    .id=="Slice7"~"Conjunto 7",
    .id=="Slice8"~"Conjunto 8",
    TRUE~.id
    
   )) %>% 
  timetk::plot_time_series_cv_plan(date,value,.interactive=FALSE)
g38$labels$colour<-"Leyenda"
g38$labels$title<-""
g38
```

```{r,grafCuanti38, fig.height=9, fig.width=9,fig.cap="Gráfico de conjuntos de entrenamiento y testeo (h=30 días).Fuente: Elaboración propia.",fig.scap="Gráfico de conjuntos de entrenamiento y testeo (h=30 días)",echo=F,message=F,warning=FALSE}

ethTSTEST_tibble<-as_tibble(ethTSTEST)

prueba_nnetar<-tsibble::tsibble(
  date = as.Date("2017-01-01") + 0:1855,
  value = ethTSTEST_tibble$x
)

splits<- time_series_split(data.frame(prueba_nnetar),assess= "30 days", cumulative = TRUE)

g39<-splits %>% tk_time_series_cv_plan() %>% mutate(.key = case_when(
    .key == "training" ~ "Entrenamiento",
    .key == "testing" ~ "Testeo",
    TRUE ~ .key
  ),.id = if_else(.id == "Slice1", "Datos", .id)) %>% 
  timetk::plot_time_series_cv_plan(date, value,.interactive = FALSE,.title = "")
g39$labels$colour<-"Leyenda"
g39
```

Al ser un proceso de cálculo estocástico se fija una semilla para los números aleatorios a fin de poder reproducir los resultados. 

Otro de los aspectos importantes en el desarrollo de pronósticos con NNETAR es la correcta hiper-parametrización del modelo. En este caso se desarrollan grillas de selección de hiperparámetros para poder ajustar las configuraciones en relación a las métricas de error seleccionadas (RMSE, MAE y MAPE%).

Los hiper-parámetros a configurar en la red son los siguiente:

Modo: "regresión". Este hiper-parámetro es fijo y no puede ser alterado.

Período de estacionalidad: En el caso del presente trabajo como ya se indicó en el análsis exploratorio no se encontraron evidencias de comportamiento estacional, con lo que se descarta modelar este hiperparámetro.

Coeficiente autorregresivo no estacionario: es el orden p del modelo.

Coeficiente autorregresivo estacionario: no se configura dada la naturaleza de la serie.

Unidades ocultas: representa la cantidad de neuronas en la capa oculta, es un valor entero.

Número de redes: corresponde al número de diferentes estimaciones a realizar para luego ser promediadas al producir pronósticos. Se fija en 20 en este caso.

Penalidad: corresponde a un valor no negativo que explica el grado de descenso del peso que se le otorga a cada nodo.

Épocas: un número entero que representa la cantidad de iteraciones de entrenamiento que realiza el modelo.

Con los parámetros que se eligen configurar se plantea una primer ronda de parametrización de la serie utilizando una función que genera combinaciones aleatorias entre parámetros. La tabla a continuación muestra la selección realizada.

```{r, echo=F,warning=F}

grid_spec_nnetar_1<-readRDS('~/grid_spec_nnetar_1.RDS')

knitr::kable(grid_spec_nnetar_1,col.names = c("Coeficiente p (AR)","Nodos en capa culta","Penalidad","Épocas"), digits = 4,
        caption = "Primera grilla de hiper-parámetros para modelo NNETAR. Fuente: Elaboración Propia", caption.short="Primera grilla de hiper-parámetros para modelo NNETAR."
        ) %>%
  kableExtra::kable_styling(row_label_position = "center",position = "center",full_width = F,latex_options = c("hold_position"), font_size = 9)
```

Se procede previamente a correr este modelo al tratamiento del conjunto de datos en lo que se denomina una receta de ingeniería de atributos. En esta receta se codifican las variables de tiempo codificándolas con método "one hot encoding" y se estandarizan los datos para alimentar el modelo.

Luego de una primera ronda de entrenamiento con los datos especificados en el esquema de validación cruzada se obtiene el siguiente ordenamiento de candidatos (en base al error cuadrático medio):

```{r, echo=F,warning=F}

resultados_nnetar_1<-readRDS('~/resultados_nnetar_1.RDS')

knitr::kable(resultados_nnetar_1 %>% select(-.estimator,-.config,-n),col.names = c("Coeficiente p (AR)","Nodos en capa culta","Penalidad","Épocas","Métrica","Media","Desv. STD"),digits = 4,
        caption = "Resultados de primera ronda de calibración con NNETAR (h=30 días). Fuente: Elaboración Propia", caption.short="Resultados de primera ronda de calibración con NNETAR (h=30 días)."
        ) %>%
  kableExtra::kable_styling(position = "center",full_width = F,latex_options = c("hold_position"), font_size = 9)
```

Estos resultados permiten la configuración de una primer guilla de calibración de hiperparámetros que se muestra en la gráfica a continuación:

```{r,grafCuanti39, fig.height=10.5, fig.width=10.5, fig.cap="Gráfico de grilla de hiper-parámetros y métricas asociadas ronda 1 (h=30 días).Fuente:Elaboración Propia",fig.scap="Gráfico de grilla de hiper-parámetros y métricas asociadas ronda 1 (h=30 días)",echo=F,message=FALSE}
g40<-readRDS("~/g40.RDS")
g40$data<-g40$data %>% mutate(name =case_when(
  name=="Non-seasonal AR Term"~"Término AR (no estacional)",
  name=="# Hidden Units"~"Unidades Ocultas",
  name=="Amount of Regularization (log-10)"~"Regularización (log-10)",
  name=="# Epochs"~"Épocas",
  TRUE~name))
  
g40  

```

El gráfico anterior permite la identificación de zonas en donde los parámetros configurados presentan mejores métricas. En base a esto se trabaja en una segunda ronda de configuración en la que se toman las mejores regiones. En el parámetro de número de nodos ocultos se trabaja con la zona entre 5 y 10 nodos, el coeficiente p se deja variar entre 1 y 5, el número de épocas entre 0 y 500 y la penalidad (log-10) entre 0 y (-5). 

En una segunda etapa de configuración de parámetros se utilizan los datos de la primera ronda y se obtienen la siguiente grilla de parámetros:

```{r, echo=F,warning=F}
grid_spec_nnetar_2<-readRDS('~/grid_spec_nnetar_2.RDS')

knitr::kable(grid_spec_nnetar_2,col.names = c("Coeficiente p (AR)", "Nodos en la capa oculta", "Penalidad", "Épocas"),digits = 4,
        caption = "Primera grilla de hiper-parámetros para modelo NNETAR. Fuente: Elaboración Propia.", caption.short="Primera grilla de hiper-parámetros para modelo NNETAR."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```


Los resultados de la segunda ronda de calibración de modelos son los que se muestran en la tabla a continuación:

```{r, echo=F,warning=F}
resultados_nnetar_2<-readRDS('~/resultados_nnetar_2.RDS')




knitr::kable(resultados_nnetar_2 %>% select(-.estimator,-.config,-n),col.names = c("Coeficiente p (AR)","Nodos en capa culta","Penalidad","Épocas","Métrica","Media","Desv. STD"),digits=4,
        caption = "Resultados de segunda ronda de calibración con NNETAR (h=30 días). Fuente: Elaboración Propia", caption.short="Resultados de segunda ronda de calibración con NNETAR (h=30 días)."
        ) %>%
  kableExtra::kable_styling(position = "center",full_width = F,latex_options = c("hold_position"), font_size = 9)
```

Los resultados de la segunda ronde de calibración se muestran en la siguiente gráfica:

```{r,grafCuanti40, fig.height=10.5, fig.width=10.5, fig.cap="Gráfico de grilla de hiper-parámetros y métricas asociadas ronda 2 (h=30 días).Fuente:Elaboración propia",fig.scap="Gráfico de grilla de hiper-parámetros y métricas asociadas ronda 2 (h=30 días)",echo=F,message=FALSE}
g41<-readRDS('~/g41.RDS')

g41$data<-g41$data %>% mutate(name =case_when(
  name=="Non-seasonal AR Term"~"Término AR (no estacional)",
  name=="# Hidden Units"~"Unidades Ocultas",
  name=="Amount of Regularization (log-10)"~"Regularización (log-10)",
  name=="# Epochs"~"Épocas",
  TRUE~name))
  
  





g41

```

Se calibra el modelo en función de las definiciones de parámetros determinadas previamente y se grafican los resultados del pronóstico para 30 días. Se elige el modelo NNAR(4,8) para hacer el pronóstico.



```{r,graffoto30,fig.align='center',fig.height=6,fig.width=6,fig.cap="Pronóstico a 30 días con NNETAR(4,8). Fuente: Elaboración propia",fig.scap="Pronóstico a 30 días con NNETAR(4,8).",echo=F,message=FALSE}

library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\NNAR(4,8)-Forecast(30).JPG")

```


Las métricas obtenidas para el pronóstico a 30 días se muestran en la siguiente tabla:

```{r, echo=F,warning=F}

resultados_nnar_30<-readRDS('~/res_nnetar_30.RDS')

knitr::kable(resultados_nnar_30$model_accuracy %>% select(-.model_id,-mase,-smape,-rsq),col.names = c("Modelo","Evaluación","MAE","MAPE%","RMSE"),digits=4,
        caption = "Métricas de resultados de pronóstico con NNEAR(4,8) (h=30 días). Fuente: Elaboración Propia.", caption.short="Métricas de resultados de pronóstico con NNEAR(4,8) (h=30 días)."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

A continuación se aplica la misma metodología al horizonte de 15 días de pronóstico.

```{r,include=F,eval=F, echo=FALSE,message=FALSE,warning=FALSE}

ethTSTEST_tibble_15<-as_tibble(ethTSTEST)

prueba_nnetar_15<-tsibble::tsibble(
  date = as.Date("2017-01-01") + 0:1840,
  value = ethTSTEST_tibble$x[0:1841]
)


splits_15<- time_series_split(data.frame(prueba_nnetar_15),assess= "15 days", cumulative = TRUE)
 
splits_15 %>% tk_time_series_cv_plan() %>% 
  plot_time_series_cv_plan(date, value)


readr::write_rds(splits_15, here::here('splits_15.RDS'))


receta_nodos_1_15 <- recipe(value ~ ., data = training(splits_15)) %>% 
  step_timeseries_signature(date) %>% step_dummy(all_nominal(),one_hot = TRUE) %>% 
  step_rm(date_index.num)
  
 
 
m_nnetar_nodos <- workflow() %>%
  add_recipe(receta_nodos_1_15) %>%
  add_model(nnetar_reg() %>% set_engine("nnetar"))
 
resample_spec_15 <- time_series_cv(data = training(splits_15),
                                initial     = 90,
                                assess      = 5,
                                skip        = 200,
                                cumulative  = FALSE,
                                slice_limit = 8)


readr::write_rds(resample_spec_15, here::here('resample_spec_15.RDS'))

#resample_spec

# VISUALIZE CV PLAN ----

# Select date and value columns from the tscv diagnostic tool
#resample_spec %>% tk_time_series_cv_plan()

# Plot the date and value columns to see the CV Plan
resample_spec_15 %>%
    plot_time_series_cv_plan(date, value, .interactive = FALSE,.title = "")



 
#NNAR modelo secuencial ARIMA, ARIMA BOOST, NNETAR, ETS, TBATS no se puede usar k-fold validation por la randomización.
 
#Receta:
 
m_nnetar_nodos %>% pull_workflow_preprocessor() %>% prep() %>% juice() %>% glimpse()
 
# Especificaciones del modelo:
 
m_nnetar_nodos %>% pull_workflow_spec()
 
model_spec_nnetar<-nnetar_reg(
  seasonal_period = 1,
  seasonal_ar = 0,
  non_seasonal_ar = tune(),
  hidden_units = tune(),
  num_networks = 20,
  penalty=tune(),
  epochs= tune()) %>% set_engine("nnetar") 
 

# en model_spec_nnetar se tiene que dar una indicación inicual para poder hacer luego tunning de hiperparámetros.

# Primera ronda de tunning:
set.seed(44)
grid_spec_nnetar_1<-grid_latin_hypercube(
  parameters(model_spec_nnetar),
  size=15
)
 
# Tuning:
 
wflw_tune_nnetar<-m_nnetar_nodos %>% update_model(model_spec_nnetar)
 
# correr la grilla de tuneo:

set.seed(44)
tune_results_nnetar_1_15<-wflw_tune_nnetar %>% 
  tune_grid(
    resamples=resample_spec_15,
    grid= grid_spec_nnetar_1,
    metrics = default_forecast_accuracy_metric_set(),
    control= control_grid(verbose=TRUE, save_pred=TRUE)
  )

 
# ver resultados:
 
resultados_nnetar_1_15<-tune_results_nnetar_1_15 %>% show_best(metric="rmse", n=Inf)

readr::write_rds(resultados_nnetar_1_15, here::here('resultados_nnetar_1_15.RDS'))



readr::write_rds(tune_results_nnetar_1_15, here::here('tune_results_nnetar_1_15.RDS'))

g46<-tune_results_nnetar_1_15 %>% autoplot()+
  geom_smooth(se=FALSE)

readr::write_rds(g46, here::here('g46.RDS'))

 
ggplotly(g46)
 
# Segunda ronda de tuning:
set.seed(44)
grid_spec_nnetar_2<-grid_latin_hypercube(
  non_seasonal_ar(range=c(1,5)),
  hidden_units(range=c(1,10)),
  penalty(range=c(-7.5,-5),trans=scales::log10_trans()),
  epochs(range=c(1,1000)),
  size=15
)
 
# Seteamos procesamiento en paralelo:

#Segunda ronda de tuneo:
 
wflw_tune_nnetar_2<-m_nnetar_nodos %>% update_model(model_spec_nnetar)

set.seed(44)
tune_results_nnetar_2_15<-wflw_tune_nnetar_2 %>% 
  tune_grid(
    resamples=resample_spec_15,
    grid= grid_spec_nnetar_2,
    metrics = default_forecast_accuracy_metric_set(),
    control= control_grid(verbose=TRUE, save_pred=TRUE)
  )
# Vemos los mejores resultados de la etapa 2:
 
resultados_nnetar_2_15<-tune_results_nnetar_2_15 %>% show_best(metric="rmse", n=Inf)
readr::write_rds(resultados_nnetar_2_15, here::here('resultados_nnetar_2_15.RDS'))


 
g47<-tune_results_nnetar_2_15 %>% autoplot()+
  geom_smooth(se=FALSE)

readr::write_rds(tune_results_nnetar_2_15, here::here('tune_results_nnetar_2_15.RDS'))
 
ggplotly(g47)
 
#######################################
# Re Entrenamiento 
 
set.seed(044)
wflw_fit_nnetar_tscv<-wflw_tune_nnetar %>% finalize_workflow(
  tune_results_nnetar_2_15 %>% show_best(metric="rmse", n=Inf) %>% 
    slice(2)
        ) %>% 
              fit(training(splits_15))
 
res<-calibrate_and_plot(wflw_fit_nnetar_tscv,.data = prueba_nnetar_15,.splits_obj = splits_15,.interactive = TRUE)

res$plot

readr::write_rds(res, here::here('res_nnetar_5.RDS'))

readRDS('~/res_nnetar_5.RDS')
#fit_eth_nnetar <- forecast::nnetar(prueba$value,p = 4,size = 8, decay=0.432, maxit=150)

#a<-forecast::forecast(fit_eth_nnetar,h=30)
#a$model
#plot(a)
#lines(prueba_nnetar_5$value)

#fit <- forecast::nnetar(prueba$value,p=3,,size = 9, decay=0.432, maxit=150)
#fit %>% 
#  forecast::forecast(h = 30) %>% 
#  autoplot(prueba) +
#  labs(x = "Tiempo", y = "ETH/UDS")


#a<-fable::NNETAR(AR(p=3,P=1,period=1))

#a$formula


#fit <- nnetar(prueba$value,p = 4,size = 8, decay=0.432, maxit=150)
#fit %>% 
#  forecast::forecast(h = 30)  %>% 
#  autoplot(prueba) +
#  labs(x = "Year", y = "Counts", title = "Yearly sunspots")


########################################

```


El esquema de validación cruzada es similar al aplicado para el horizonte de 30 días. En este caso, se realizan 8 particiones de la serie en intervalos de 90 días con un horizonte de 15 días para el testeo. 

```{r,grafCuanti45, fig.height=12, fig.width=9, fig.cap="Gráfico de Esquema de Validación Cruzada (h=15 días). Fuente:Elaboración propia.",fig.scap="Gráfico de Esquema de Validación Cruzada (h=15 días)",echo=F,message=FALSE}
resample_spec_15<-readRDS("~/resample_spec_15.RDS")
g49<-resample_spec_15 %>% 
  tk_time_series_cv_plan() %>% 
  mutate(.key = case_when(
    .key == "training" ~ "Entrenamiento",
    .key == "testing" ~ "Testeo",
    TRUE ~ .key
  ),.id = case_when(
    .id=="Slice1"~"Conjunto 1",
    .id=="Slice2"~"Conjunto 2",
    .id=="Slice3"~"Conjunto 3",
    .id=="Slice4"~"Conjunto 4",
    .id=="Slice5"~"Conjunto 5",
    .id=="Slice6"~"Conjunto 6",
    .id=="Slice7"~"Conjunto 7",
    .id=="Slice8"~"Conjunto 8",
    TRUE~.id
    
   )) %>% 
  
  timetk::plot_time_series_cv_plan(date,value,.interactive = FALSE,.title = "")
g49$labels$colour<-"Leyenda"
g49
```

El testeo del método de pronóstico se hace en los primeros 15 días del mes de enero de 2022, tal como muestra la siguiente figura:

```{r,grafCuanti46, fig.height=9, fig.width=9,fig.cap="Gráfico de conjuntos de entrenamiento y testeo (h=5 días). Fuente:Elaboración propia.",fig.scap="Gráfico de conjuntos de entrenamiento y testeo (h=5 días) NNETAR",echo=F,message=F,warning=FALSE}

splits_15<- readRDS('~/splits_15.RDS')

g48<-splits_15 %>% tk_time_series_cv_plan() %>% mutate(.key = case_when(
    .key == "training" ~ "Entrenamiento",
    .key == "testing" ~ "Testeo",
    TRUE ~ .key
  ),.id = if_else(.id == "Slice1", "Datos", .id)) %>% 
  timetk::plot_time_series_cv_plan(date, value,.interactive = FALSE,.title = "")

g48$labels$colour<-"Leyenda"


g48
```

Los resultados de la primera ronda de calibración para este horizonte de tiempo son los siguientes:

```{r, echo=F,warning=F}

resul_nnetar_1_15<-readRDS('~/resultados_nnetar_1_15.RDS')

knitr::kable(resul_nnetar_1_15 %>% select(-.estimator,-.config,-n),col.names = c("Coeficiente p (AR)","Nodos en capa culta","Penalidad","Épocas","Métrica","Media","Desv. STD"),digits=4,
        caption = "Resultados de primera ronda de calibración con NNETAR (h=15 días). Fuente: Elaboración Propia", caption.short="Resultados de primera ronda de calibración con NNETAR (h=15 días)."
        ) %>%
  kableExtra::kable_styling(position = "center",full_width = F,latex_options = c("hold_position"), font_size = 9)
```

\newpage

La grilla de hiperparámetros y métricas obtenidas durante la primera calibración se presentan en el siguiente gráfico:

```{r,grafCuanti47, fig.height=10.5, fig.width=10.5, fig.cap="Gráfico de grilla de hiper-parámetros y métricas asociadas ronda 1 (h=15 días). Fuente:Elaboración propia.",fig.scap="Gráfico de grilla de hiper-parámetros y métricas asociadas ronda 1 (h=15 días) NNETAR",echo=F,message=FALSE}

g46 <- readRDS("~/g46.RDS")

g46$data<-g46$data %>% mutate(name =case_when(
  name=="Non-seasonal AR Term"~"Término AR (no estacional)",
  name=="# Hidden Units"~"Unidades Ocultas",
  name=="Amount of Regularization (log-10)"~"Regularización (log-10)",
  name=="# Epochs"~"Épocas",
  TRUE~name))



g46 
```

En base a lo observado en los resultados de las métricas de la primera calibración se decide configurar una segunda ronda de calibración con los siguiente hiperparámetros: el termino autorregresivo variando entre 1 y 5, el nivel de regularización (en base 10 de logaritmo) entre -7,5 y -5, no se altera el rango de unidades ocultas y el número de épocas se limita entre 500 y 1000. 

Los resultados de la segunda calibración para el horizonte de 15 días son:

```{r, echo=F,warning=F}

resultados_nnetar_2_15<-readRDS('~/resultados_nnetar_2_15.RDS')

knitr::kable(resultados_nnetar_2_15 %>% select(-.estimator,-n,-.config),col.names = c("Coeficiente p (AR)","Nodos en capa culta","Penalidad","Épocas","Métrica","Media","Desv. STD"),digits=4,
        caption = "Resultados de primera ronda de calibración con NNETAR (h=15 días). Fuente: Elaboración Propia", caption.short="Resultados de primera ronda de calibración con NNETAR (h=15 días)."
        ) %>%
  kableExtra::kable_styling(position = "center",full_width = F,latex_options = c("hold_position"), font_size = 9)
```

La grilla de métricas e hiperparámetros de la segunda ronda de entrenamiento se representa en el siguiente gráfico:

```{r,grafCuanti48, fig.height=10.5, fig.width=10.5, fig.cap="Gráfico de grilla de hiper-parámetros y métricas asociadas ronda 2 (h=5 días). Fuente:Elaboración propia.",fig.scap="Gráfico de grilla de hiper-parámetros y métricas asociadas ronda 2 (h=5 días) NNETAR",echo=F,message=FALSE}

tune_results_nnetar_2_15 <- readRDS("~/tune_results_nnetar_2_15.RDS")
g45<- tune_results_nnetar_2_15%>% autoplot()+
  geom_smooth(se=FALSE)
g45$data<-g45$data %>% mutate(name =case_when(
  name=="Non-seasonal AR Term"~"Término AR (no estacional)",
  name=="# Hidden Units"~"Unidades Ocultas",
  name=="Amount of Regularization (log-10)"~"Regularización (log-10)",
  name=="# Epochs"~"Épocas",
  TRUE~name))



g45 
```

Finalmente el pronóstico obtenido para el horizonte de 15 días:

```{r,graffoto31,fig.align='center',fig.height=6,fig.width=6,fig.cap="Pronóstico a 15 días con NNETAR(2,2). Fuente: Elaboración Propia",fig.scap="Pronóstico a 15 días con NNETAR(2,2).",echo=F,message=FALSE}

library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\Forecast_NNETAR_15_d.JPG")







```


Las métricas obtenidas para el pronóstico a 15 días se muestran en la siguiente tabla:

```{r,echo=FALSE,message=FALSE}

resultados_nnetar_15_dias<-readRDS('~/res_nnetar_15.RDS')
knitr::kable(resultados_nnetar_15_dias$model_accuracy %>% select(-.model_id,-mase,-smape,-rsq),col.names = c("Modelo","Evaluación","MAE","MAPE%","RMSE"), digits=4,
        caption = "Métricas de resultados de pronóstico con NNEAR(2,2) (h=15 días). Fuente: Elaboración propia.", caption.short="Métricas de resultados de pronóstico con NNEAR(2,2) (h=15 días)."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```



La metodología presentada para NNETAR se repite para los distintos horizontes de pronóstico. Para el pronóstico a 5 días se presenta a continuación.

```{r,include=F,eval=F, echo=FALSE,message=FALSE,warning=FALSE}

ethTSTEST_tibble_5<-as_tibble(ethTSTEST)

prueba_nnetar_5<-tsibble::tsibble(
  date = as.Date("2017-01-01") + 0:1830,
  value = ethTSTEST_tibble$x[0:1831]
)


splits_5<- time_series_split(data.frame(prueba_nnetar_5),assess= "5 days", cumulative = TRUE)
 
splits_5 %>% tk_time_series_cv_plan() %>% 
  plot_time_series_cv_plan(date, value)


readr::write_rds(splits_5, here::here('splits_5.RDS'))


receta_nodos_1_5 <- recipe(value ~ ., data = training(splits_5)) %>% 
  step_timeseries_signature(date) %>% step_dummy(all_nominal(),one_hot = TRUE) %>% 
  step_rm(date_index.num)
  
 
 
m_nnetar_nodos <- workflow() %>%
  add_recipe(receta_nodos_1_5) %>%
  add_model(nnetar_reg() %>% set_engine("nnetar"))
 
resample_spec_5 <- time_series_cv(data = training(splits_5),
                                initial     = 30,
                                assess      = 5,
                                skip        = 200,
                                cumulative  = FALSE,
                                slice_limit = 8)


readr::write_rds(resample_spec_5, here::here('resample_spec_5.RDS'))

#resample_spec

# VISUALIZE CV PLAN ----

# Select date and value columns from the tscv diagnostic tool
#resample_spec %>% tk_time_series_cv_plan()

# Plot the date and value columns to see the CV Plan
resample_spec_5 %>%
    plot_time_series_cv_plan(date, value, .interactive = FALSE,.title = "")



 
#NNAR modelo secuencial ARIMA, ARIMA BOOST, NNETAR, ETS, TBATS no se puede usar k-fold validation por la randomización.
 
#Receta:
 
m_nnetar_nodos %>% pull_workflow_preprocessor() %>% prep() %>% juice() %>% glimpse()
 
# Especificaciones del modelo:
 
m_nnetar_nodos %>% pull_workflow_spec()
 
model_spec_nnetar<-nnetar_reg(
  seasonal_period = 1,
  seasonal_ar = 0,
  non_seasonal_ar = tune(),
  hidden_units = tune(),
  num_networks = 20,
  penalty=tune(),
  epochs= tune()) %>% set_engine("nnetar") 
 

# en model_spec_nnetar se tiene que dar una indicación inicual para poder hacer luego tunning de hiperparámetros.

# Primera ronda de tunning:
set.seed(44)
grid_spec_nnetar_1<-grid_latin_hypercube(
  parameters(model_spec_nnetar),
  size=15
)
 
# Tuning:
 
wflw_tune_nnetar<-m_nnetar_nodos %>% update_model(model_spec_nnetar)
 
# correr la grilla de tuneo:

set.seed(44)
tune_results_nnetar_1_5<-wflw_tune_nnetar %>% 
  tune_grid(
    resamples=resample_spec_5,
    grid= grid_spec_nnetar_1,
    metrics = default_forecast_accuracy_metric_set(),
    control= control_grid(verbose=TRUE, save_pred=TRUE)
  )

 
# ver resultados:
 
resultados_nnetar_1_5<-tune_results_nnetar_1_5 %>% show_best(metric="rmse", n=Inf)

readr::write_rds(resultados_nnetar_1_5, here::here('resultados_nnetar_1_5.RDS'))



readr::write_rds(tune_results_nnetar_1_5, here::here('nuevo-archivo.RDS'))

g40<-tune_results_nnetar_1_5 %>% autoplot()+
  geom_smooth(se=FALSE)

readr::write_rds(g40, here::here('g40.RDS'))

 
ggplotly(g40)
 
# Segunda ronda de tuning:
set.seed(44)
grid_spec_nnetar_2<-grid_latin_hypercube(
  non_seasonal_ar(range=c(1,5)),
  hidden_units(range=c(1,10)),
  penalty(range=c(-7.5,-5),trans=scales::log10_trans()),
  epochs(range=c(1,1000)),
  size=15
)
 
# Seteamos procesamiento en paralelo:

#Segunda ronda de tuneo:
 
wflw_tune_nnetar_2<-m_nnetar_nodos %>% update_model(model_spec_nnetar)

set.seed(44)
tune_results_nnetar_2_5<-wflw_tune_nnetar_2 %>% 
  tune_grid(
    resamples=resample_spec_5,
    grid= grid_spec_nnetar_2,
    metrics = default_forecast_accuracy_metric_set(),
    control= control_grid(verbose=TRUE, save_pred=TRUE)
  )
# Vemos los mejores resultados de la etapa 2:
 
resultados_nnetar_2_5<-tune_results_nnetar_2_5 %>% show_best(metric="rmse", n=Inf)
readr::write_rds(resultados_nnetar_2_5, here::here('resultados_nnetar_2_5.RDS'))


 
g41<-tune_results_nnetar_2_5 %>% autoplot()+
  geom_smooth(se=FALSE)

readr::write_rds(tune_results_nnetar_2_5, here::here('tune_results_nnetar_2_5.RDS'))
 
ggplotly(g41)
 
#######################################
# Re Entrenamiento 
 
set.seed(049)
wflw_fit_nnetar_tscv<-wflw_tune_nnetar %>% finalize_workflow(
  tune_results_nnetar_2_5 %>% show_best(metric="rmse", n=Inf) %>% 
    slice(1)
        ) %>% 
              fit(training(splits_5))
 
res<-calibrate_and_plot(wflw_fit_nnetar_tscv,.data = prueba_nnetar_5,.splits_obj = splits_5,.interactive = TRUE)

res$plot

readr::write_rds(res, here::here('res_nnetar_5.RDS'))


#fit_eth_nnetar <- forecast::nnetar(prueba$value,p = 4,size = 8, decay=0.432, maxit=150)

#a<-forecast::forecast(fit_eth_nnetar,h=30)
#a$model
#plot(a)
#lines(prueba_nnetar_5$value)

#fit <- forecast::nnetar(prueba$value,p=3,,size = 9, decay=0.432, maxit=150)
#fit %>% 
#  forecast::forecast(h = 30) %>% 
#  autoplot(prueba) +
#  labs(x = "Tiempo", y = "ETH/UDS")


#a<-fable::NNETAR(AR(p=3,P=1,period=1))

#a$formula


#fit <- nnetar(prueba$value,p = 4,size = 8, decay=0.432, maxit=150)
#fit %>% 
#  forecast::forecast(h = 30)  %>% 
#  autoplot(prueba) +
#  labs(x = "Year", y = "Counts", title = "Yearly sunspots")


########################################

```

El esquema de validación cruzada es similar al aplicado para el horizonte de 30 días. Se realizan 8 particiones de la serie en intervalos de 30 días con un horizonte de 5 días para el testeo. 

```{r,grafCuanti41, fig.height=12, fig.width=9, fig.cap="Gráfico de Esquema de Validación Cruzada (h=5 días). Fuente:Elaboración propia.",fig.scap="Gráfico de Esquema de Validación Cruzada (h=5 días) NNETAR",echo=F}
resample_spec_5<-readRDS("~/resample_spec_5.RDS")
g42<-resample_spec_5 %>% 
  tk_time_series_cv_plan() %>% mutate(.key = case_when(
    .key == "training" ~ "Entrenamiento",
    .key == "testing" ~ "Testeo",
    TRUE ~ .key
  ),.id = case_when(
    .id=="Slice1"~"Conjunto 1",
    .id=="Slice2"~"Conjunto 2",
    .id=="Slice3"~"Conjunto 3",
    .id=="Slice4"~"Conjunto 4",
    .id=="Slice5"~"Conjunto 5",
    .id=="Slice6"~"Conjunto 6",
    .id=="Slice7"~"Conjunto 7",
    .id=="Slice8"~"Conjunto 8",
    TRUE~.id
    
   )) %>% 
  
  timetk::plot_time_series_cv_plan(date,value,.interactive = FALSE,.title = "")

g42$labels$colour<-"Leyenda"
g42$labels$title<-""

g42
```

El testeo del método de pronóstico se hace en los primeros 5 días del mes de enero de 2022, tal como muestra la siguiente figura:

```{r,grafCuanti42, fig.height=10.5, fig.width=10.5,fig.cap="Gráfico de conjuntos de entrenamiento y testeo (h=5 días). Fuente:Elaboración propia.",fig.scap="Gráfico de conjuntos de entrenamiento y testeo (h=5 días) NNETAR",echo=F,message=F,warning=FALSE}

splits_5<- readRDS('~/splits_5.RDS')
g43<-splits_5 %>% tk_time_series_cv_plan() %>% mutate(.key = case_when(
    .key == "training" ~ "Entrenamiento",
    .key == "testing" ~ "Testeo",
    TRUE ~ .key
  ),.id = if_else(.id == "Slice1", "Datos", .id)) %>%  timetk::plot_time_series_cv_plan(date, value,.interactive = FALSE,.title = "")

g43$labels$colour<-"Leyenda"

g43
```

Los resultados de la primera ronda de calibración para este horizonte de tiempo son los siguientes:

```{r, echo=F,warning=F}

resul_nnetar_1_5<-readRDS('~/resultados_nnetar_1_5.RDS')

knitr::kable(resul_nnetar_1_5 %>% select(-.estimator,-.config,-n),col.names = c("Coeficiente p (AR)","Nodos en capa culta","Penalidad","Épocas","Métrica","Media","Desv. STD"),digits=4,
        caption = "Resultados de primera ronda de calibración con NNETAR (h=5 días). Fuente: Elaboración Propia", caption.short="Resultados de primera ronda de calibración con NNETAR (h=5 días)."
        ) %>%
  kableExtra::kable_styling(position = "center",full_width = F,latex_options = c("hold_position"), font_size = 9)
```

La grilla de hiperparámetros y métricas obtenidas durante la primera calibración se presentan en el siguiente gráfico:

```{r,grafCuanti43, fig.height=10.5, fig.width=10.5, fig.cap="Gráfico de grilla de hiper-parámetros y métricas asociadas ronda 1 (h=5 días). Fuente: Elaboración Propia.",fig.scap="Gráfico de grilla de hiper-parámetros y métricas asociadas ronda 1 (h=5 días) NNETAR",echo=FALSE, message=FALSE}

`nuevo-archivo` <- readRDS("~/nuevo-archivo.RDS")
g44<- `nuevo-archivo`%>% autoplot()+
  geom_smooth(se=FALSE)

g44$data<-g44$data %>% mutate(name =case_when(
  name=="Non-seasonal AR Term"~"Término AR (no estacional)",
  name=="# Hidden Units"~"Unidades Ocultas",
  name=="Amount of Regularization (log-10)"~"Regularización (log-10)",
  name=="# Epochs"~"Épocas",
  TRUE~name))


g44 
```

En base a lo observado en los resultados de las métricas de la primera calibración se decide configurar una segunda ronda de calibración con los siguiente hiperparámetros: el termino autorregresivo variando entre 1 y 5, el nivel de regularización (en base 10 de logaritmo) entre -7,5 y -5, no se altera el rango de unidades ocultas y el número de épocas se limita entre 500 y 1000. 

Los resultados de la segunda calibración para el horizonte de 5 días son:

```{r, echo=F,warning=F}

resultados_nnetar_2_5<-readRDS('~/resultados_nnetar_2_5.RDS')

knitr::kable(resultados_nnetar_2_5 %>% select(-.estimator,-n,-.config),col.names = c("Coeficiente p (AR)","Nodos en capa culta","Penalidad","Épocas","Métrica","Media","Desv. STD"), digits=4,
        caption = "Resultados de segunda ronda de calibración con NNETAR (h=5 días). Fuente: Elaboración Propia", caption.short="Resultados de segunda ronda de calibración con NNETAR (h=5 días)."
        ) %>%
  kableExtra::kable_styling(position = "center",full_width = F,latex_options = c("hold_position"), font_size = 9)
```

La grilla de métricas e hiperparámetros de la segunda ronda de entrenamiento se representa en el siguiente gráfico:

```{r,grafCuanti44, fig.height=10.5, fig.width=10.5, fig.cap="Gráfico de grilla de hiper-parámetros y métricas asociadas ronda 2 (h=5 días)",echo=F,message=FALSE}

tune_results_nnetar_2_5 <- readRDS("~/tune_results_nnetar_2_5.RDS")
g45<- tune_results_nnetar_2_5%>% autoplot()+
  geom_smooth(se=FALSE)

g45$data<-g45$data %>% mutate(name =case_when(
  name=="Non-seasonal AR Term"~"Término AR (no estacional)",
  name=="# Hidden Units"~"Unidades Ocultas",
  name=="Amount of Regularization (log-10)"~"Regularización (log-10)",
  name=="# Epochs"~"Épocas",
  TRUE~name))


g45 
```

Finalmente el pronóstico obtenido para el horizonte de 5 días:

```{r,graffoto32,fig.align='center',fig.height=6,fig.width=6,fig.cap="Pronóstico a 5 días con NNETAR(4,1). Fuente: Elaboración Propia",fig.scap="Pronóstico a 5 días con NNETAR(4,1).",echo=F,message=FALSE}

library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\Forecast_NNETAR_5d.jpeg")


```

Las métricas obtenidas para el pronóstico a 5 días se muestran en la siguiente tabla:

```{r,message=FALSE,echo=FALSE}
resultados_nnetar_5_dias<-readRDS('~/res_nnetar_5.RDS')
knitr::kable(resultados_nnetar_5_dias$model_accuracy %>% select(-.model_id,-mase,-smape,-rsq),col.names = c("Modelo","Evaluación","MAE","MAPE%","RMSE"), digits=4,
        caption = "Métricas de resultados de pronóstico con NNEAR(4,1) (h=5 días). Fuente: Elaboración Propia", caption.short="Métricas de resultados de pronóstico con NNEAR(4,1) (h=5 días)."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

\newpage
## Redes neuronales recursivas-LSTM: 

Para aplicar la técnica LSTM es necesario evaluar que la serie original presente niveles de autocorrelación suficientes como para generar las secuencias de predicciones. 

```{r,eval=F,include=FALSE}
prueba

tidy_acf <- function(data, value, lags = 0:20) {
    
    value_expr <- enquo(value)
    
    acf_values <- data %>%
        pull(value) %>%
        acf(lag.max = tail(lags, 1), plot = FALSE) %>%
        .$acf %>%
        .[,,1]
    
    ret <- tibble(acf = acf_values) %>%
        rowid_to_column(var = "lag") %>%
        mutate(lag = lag - 1) %>%
        filter(lag %in% lags)
    
    return(ret)
}

ethTSTEST_tibble<-as_tibble(ethTSTEST)

prueba_LSTM_30<-tsibble::tsibble(
  date = as.Date("2017-01-01") + 0:1855,
  value = ethTSTEST_tibble$x
)

splits_LSTM_30<- time_series_split(data.frame(prueba_LSTM_30),assess= "30 days", cumulative = TRUE)
 
splits_LSTM_30 %>% tk_time_series_cv_plan() %>% 
  plot_time_series_cv_plan(date, value)

df_trn <- training(resample_LSTM_30$splits)%>% add_column(key = "training")
df_tst <- testing(resample_LSTM_30$splits)%>% add_column(key = "testing")

df_LSTM <- bind_rows(df_trn,df_tst) %>% arrange(date)

####################
rolling_origin_resamples <- rolling_origin(
    prueba_LSTM_30,
    initial    = 300,
    assess     = 30,
    cumulative = FALSE,
    skip       = 200
)

rolling_origin_resamples


split    <- rolling_origin_resamples$splits[[8]]
split_id <- rolling_origin_resamples$id[[8]]

   df_trn <- training(split)
        df_tst <- testing(split)
        
        df <- bind_rows(
            df_trn %>% add_column(key = "training"),
            df_tst %>% add_column(key = "testing")) %>% 
            dplyr::rename(index=date)

        

receta_nodos_1 <- recipe(value ~ ., data = training(splits)) %>% 
  step_timeseries_signature(date) %>% step_dummy(all_nominal(),one_hot = TRUE)
  
 
 
m_nnetar_nodos <- workflow() %>%
  add_recipe(receta_nodos_1) %>%
  add_model(nnetar_reg() %>% set_engine("nnetar"))



resample_LSTM_30 <- time_series_cv(data = training(splits_LSTM_30),
                                initial     = 270,
                                assess      = 30,
                                skip        = 200,
                                cumulative  = FALSE,
                                slice_limit = 8)

resample_LSTM_30$splits

#gráficas de los diferentes re-sampleos de la muestra para los distintos horizontes.
g55<-resample_LSTM_30 %>% 
    mutate(id = case_when(
    id == "Slice1" ~ "Conjunto1",
    id == "Slice2" ~ "Conjunto2",
    id == "Slice3" ~ "Conjunto3",
    id == "Slice4" ~ "Conjunto4",
    id == "Slice5" ~ "Conjunto5",
    id == "Slice6" ~ "Conjunto6",
    id == "Slice7" ~ "Conjunto7",
    id == "Slice8" ~ "Conjunto8",
    TRUE ~ id
  )) %>% 
    plot_time_series_cv_plan(date, value, .interactive = FALSE,.title = "",.legend_show=FALSE)
g55$labels$colour<-"Leyenda"

 readr::write_rds(g55, here::here('grafico_cv_LSTM_30.RDS'))


readr::write_rds(resample_LSTM_30, here::here('resample_spec_LSTM_30.RDS'))


autocorr<-ggAcf(prueba,lag.max = 200)+ggtitle('FAC ETH/USD (200 retardos)')

readr::write_rds(autocorr, here::here('g49.RDS'))

#### Función LSTM

### Función Calculo RMSE:

calc_rmse <- function(prediction_tbl) {
    
    rmse_calculation <- function(data) {
        data %>%
            spread(key = key, value = value) %>%
            select(-index) %>%
            filter(!is.na(predict)) %>%
            rename(
                truth    = actual,
                estimate = predict
            ) %>%
            rmse(truth, estimate)
    }
    
    safe_rmse <- possibly(rmse_calculation, otherwise = NA)
    
    safe_rmse(prediction_tbl)
        
}

#######


split    <- rolling_origin_resamples$splits[[8]]
split_id <- rolling_origin_resamples$id[[8]]

qw<-predict_keras_lstm(split, epochs = 300)

predict_keras_lstm()
sample_predictions_lstm_tbl <- rolling_origin_resamples %>%
     mutate(predict = map(splits, predict_keras_lstm, epochs = 300))

sample_rmse_tbl <- sample_predictions_lstm_tbl %>%
    mutate(rmse = map_dbl(predict, calc_rmse)) %>%
    select(id, rmse)

sample_rmse_tbl


#https://www.business-science.io/timeseries-analysis/2018/04/18/keras-lstm-sunspots-time-series-prediction.html

#https://www.business-science.io/timeseries-analysis/2018/07/01/keras-lstm-sunspots-part2.html


```

En la figura a continuación se puede observar que la autocorrelación de la serie es fuerte para los primeros 200 rezagos.

```{r,grafCuanti49, fig.height=5, fig.width=5, fig.cap="Función de autocorrelación de ETH/USD (200 rezagos). Fuente: Elaboración propia.",fig.scap="Función de autocorrelación de ETH/USD (200 rezagos).",echo=F}
g49<-readRDS('~/g49.RDS')
g49
```

Para el entrenamiento y calibración de la red recurrente LSTM se utiliza el esquema de validación cruzada que divide la serie original en 8 particiones de 270 elementos cada una con los respectivos  horizontes de pronóstico. En el gráfico a continuación se muestra el esquema de validación cruzada para un horizonte de pronóstico de 30 días. 

```{r,grafCuanti50, fig.height=12, fig.width=9, fig.cap="Esquema de Validación Cruzada para pronóstico a 30 días. Fuente: Elaboración Propia.", fig.scap="Esquema de Validación Cruzada para pronóstico a 30 días.",echo=F}
grafico_cv_lstm_30<-readRDS('~/grafico_cv_LSTM_30.RDS')
grafico_cv_lstm_30
```


Se procede luego a la preparación de los datos de cada partición con un centrado y escalado de los mismos, dada la especificación técnica para la aplicación. Los datos se centran con la media de cada partición y se escalan con la desviación estandar. Se escalan los datos de entrenamiento de cada sub muestra y con esos parámetros de escala luego se transforman los valores de testeo para no introducir un error de utilizar los valores de la muestra de testeo en la configuración del entrenamiento. 

Se necesita además configurar los hiperparámetros del modelo, para ello se generan iteraciones en una grilla de parámetros a lo largo de los conjuntos de entrenamiento y testeo definidos. Se registran las métricas de error cuadrático medio, MAPE y MAE para cada combinación de parámetros. Los parámetros elegidos para configurar son:


Unidades: la unidad elemental de una red LSTM es una celda. Esta contiene componentes que incluyen la compuerta de entrada (que controla el flujo de información desde la entrada al estado de la celda), la compuerta de olvido (que contempla el flujo de información que pasa del estado anterior de una celda al estado actual de la misma), la compuerta de salida (que controla el flujo de información del estado de la celda a la salida), el estado de la celda (la memoria de la celda que guarda información a lo largo del tiempo) y el estado oculto (la salida de la LSTM que es pasada al próximo paso de tiempo). En el caso del presente trabajo se itera entre 25 y 200 unidades. Un número más grande de neuronas puede captar patrones complejos como los de la serie en estudio pero también ocasionar problemas de sobreajuste.

Tasa de descarte: es una técnica de regularización que previene el sobreajuste seteando aleatoriamente algunas celdas como cero. Es la probabilidad de que en una iteración determinada neurona sea "apagada" en el entrenamiento. Con esto se apunta a que el modelo tenga estimaciones un poco más robustas de los datos mejorando la generalización. En este caso se itera entre 0.01 y 0.5.

Épocas: una época se refiere a una completa pasada por los datos de entrenamiento donde cada secuencia de entrada se presenta una vez a la red recurrente. Luego de cada época los costos de la red se actualizan en función de la pérdida promedio a lo largo de la secuencia de entrada. Se decide iterar entre 25 y 200 épocas a lo largo de los subconjuntos entrenados.

Tamaño del lote: el tamaño de la muestra que se pasará a través del algoritmo en cada época. En este caso se fija en cada caso igual al horizonte de pronóstico.

Se itera a lo largo de cada conjunto de entrenamiento y validación definido generando un total de 25 combinaciones por partición de la serie. Para la generación de una grilla se utilizan números aleatorios con el objetivo de generar una grilla de prueba. A continuación se muestra la tabla utilizada para correr en los conjuntos de validación cruzada.

```{r,echo=FALSE,message=FALSE}

random_grid_LSTM_30<-readRDS('~/random_grid_LSTM_30.RDS')
knitr::kable( random_grid_LSTM_30,col.names = c("Unidades","Tasa de Descarte","Épocas","Split de validación"),digits=4,
        caption = "Grilla de hiper parámetros para calibración LSTM. Fuente: Elaboración Propia", caption.short="Grilla de hiper parámetros para calibración LSTM."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

\newpage

Se generan las gráficas de los parámetros y se seleccionan las combinaciones que minimizan las métricas mencionadas. En la gráfica a continuación se visualizan los resultados de la calibración con los distintos hiper parámetros considerados.

```{r,grafCuanti51, fig.height=13, fig.width=11, fig.cap="RMSE versus Hiperparámetros configurados en la calibración de LSTM a lo largo de 8 pasos de validación cruzada. Fuente: Elaboración propia",fig.scap="RMSE versus Hiperparámetros configurados en la calibración de LSTM a lo largo de 8 pasos de validación cruzada.",echo=F,message=FALSE}

g155<-readRDS('~/g155.RDS')
g156<-readRDS('~/g156.RDS')
g157<-readRDS('~/g157.RDS')
g158<-readRDS('~/g158.RDS')

g1552<-g155+labs(x="Split de Validación")
g1562<-g156+labs(x="Épocas")
g1572<-g157+labs(x="Unidades")
g1582<-g158+labs(x="Tasa de descarte")
(g1552/
 g1562/
 g1572/
 g1582)
```

En base a lo observado en las gráficas y considerando el orden obtenido para las métricas en estudio en los conjuntos de prueba mostrados en la siguiente tabla.

```{r,echo=FALSE, message=FALSE}

tabla_resultados_calibracion_lstm<-readRDS('~/tabla_resultados_calibracion_LSTM_30_1_d.RDS')

knitr::kable(tabla_resultados_calibracion_lstm,col.names = c("Nro Iteración","RMSE","MAPE","MAE",'Unidades','Tasa de Descarte','Épocas','Split de Validación'),digits=4,
        caption = "Resultados de calibración LSTM en conjunto de entrenamiento. Fuente: Elaboración Propia", caption.short="Resultados de calibración LSTM en conjunto de entrenamiento."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```

Se decide realizar el pronóstico de la serie de tiempo con los siguientes utilizando los siguientes hiper parámetros: 272 unidades, fijar la tasa de descarte en 0.0780, utilizar 229 épocas y un split de validación de 0.1157.

Se re entrena el modelo con los parámetros seleccionados a lo largo del total de los datos y se realizan los pronósticos en los horizontes definidos. Para ello se realizaron 100 corridas del modelo calibrado según la especificación obtenida en cada modelo con todos los datos disponibles para el pronóstico de la serie. 

Los gráficos a de pronósticos que se presentan incluyen la representación de la serie a lo largo del período 2021-enero 2022 para una mejor visualización de la información.

```{r,grafCuanti52, fig.height=5, fig.width=8, fig.cap="Pronóstico con LSTM ETH/USD (30 días-100 corridas). Fuente: Elaboración propia.",fig.scap="Pronóstico con LSTM ETH/USD (30 días-100 corridas).",echo=F,message=FALSE}

g65<-readRDS('~/g65.RDS')
g65

```

```{r,grafCuanti53, fig.height=5, fig.width=8, fig.cap="Pronóstico con LSTM ETH/USD (15 días-100 corridas). Fuente: Elaboración propia",fig.scap="Pronóstico con LSTM ETH/USD (15 días-100 corridas).",echo=F,message=FALSE}

g66<-readRDS('~/g66.RDS')
g66

```

```{r,grafCuanti54, fig.height=5, fig.width=8, fig.cap="Pronóstico con LSTM ETH/USD (5 días-100 corridas). Fuente: Elaboración propia",fig.scap="Pronóstico con LSTM ETH/USD (5 días-100 corridas).",echo=F}

g67<-readRDS('~/g67.RDS')
g67

```

Se promedian los resultados de las métricas obtenidas en cada una de las corridas y se muestran en la siguiente tabla:

```{r,echo=FALSE}

resultados_promedio_lstm<-readRDS('~/resultados_promedio_lstm.RDS')

knitr::kable(resultados_promedio_lstm,
        caption = "Métricas promedio obtenidas con 100 corridas de LSTM para los horizontes de pronóstico definidos. Fuente: Elaboración Propia", caption.short="Métricas promedio obtenidas con 100 corridas de LSTM para los horizontes de pronóstico definidos."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)
```



```{r,eval=FALSE,include=FALSE}
#############################################################
#####ACA ESTAN LAS TABLAS y GRAFICOS DE CALIBRACION ############
############################################################


## VIEW SOBRE LOS PRIMEROS DATOS DEL DATASET



rolling_origin_resamples_30 <- rolling_origin(
    prueba_LSTM_30,
    initial    = 270,
    assess     = 30,
    cumulative = FALSE,
    skip       = 200
)


            #dplyr::rename(index=date)

#for (i in 1:2){ 

set.seed(44)
grid_spec_LSTM<-grid_latin_hypercube(
  units(range=c(50,300)),
  layer_drop(range=c(0.01,0.5)),
  epochs(range=c(10,300)),
  validation_split(range=c(0.05,0.4)),
  size=25
)

# Define the parameter ranges
units_range <- c(50, 300)
layer_drop_range <- c(0.01, 0.5)
epochs_range <- c(10, 300)
validation_split_range <- c(0.05, 0.4)

# Set the random seed for reproducibility
set.seed(123)

# Generate the random grid
random_grid_LSTM_30 <- data.frame(
  UNITS = round(runif(25, units_range[1], units_range[2])),
  LAYER_DROP = runif(25, layer_drop_range[1], layer_drop_range[2]),
  EPOCHS = round(runif(25, epochs_range[1], epochs_range[2])),
  VALIDATION_SPLIT = runif(25, validation_split_range[1], validation_split_range[2])
)

readr::write_rds(random_grid_LSTM_30, here::here('random_grid_LSTM_30.RDS'))


# View the random grid




for (d in 1:8){  
#serie<-tibble(prueba %>% filter(date>'2021-03-06'))
#set.seed(123)
split    <- rolling_origin_resamples_30$splits[[d]]
split_id <- rolling_origin_resamples_30$id[[d]]

   df_trn <- training(split)
        df_tst <- testing(split)

serie_30 <- tibble(bind_rows(df_trn,df_tst))
  

#tail(serie)

data.class(serie_30$value)

#autoplot(ts(serie$value))

## DETERMINAR LOS FACTORES DE ESCALAMIENTO: MEDIA Y DESVIO DE LA SERIE
scale_factors <- c(mean(serie_30$value), sd(serie_30$value))
print(scale_factors)

## SELECCIONAR VARIABLE DE INTERÉS Y ESCALARLA
scaled_train <- serie_30 %>% 
  select(value) %>%
  mutate(value = (value - scale_factors[1]) / scale_factors[2])

## DEFINIR HORIZONTE DE PREDICCIÓN
prediction <- 30
lag <- prediction

## DEFINIR COMO UNA MATRIZ AL CONJUNTO DE DATOS ESCALADOS
scaled_train <- as.matrix(scaled_train)

## RETRASAR LOS DATOS 29 VECES Y ORGANIZARLOS COMO COLUMNAS
x_train_data <- t(sapply(
  1:(length(scaled_train) - lag - prediction + 1),
  function(x) scaled_train[x:(x + lag - 1), 1]
))

## TRANSFORMAR EN FORMATO ARRAY
x_train_arr <- array(
  data = as.numeric(unlist(x_train_data)),
  dim = c(nrow(x_train_data),lag,1)
)

## SIMILAR TRANSFORMACIÓN PARA LOS VALORES DE Y
y_train_data <- t(sapply(
  (1 + lag):(length(scaled_train) - prediction + 1),
  function(x) scaled_train[x:(x + prediction - 1)]
))

y_train_arr <- array(
  data = as.numeric(unlist(y_train_data)),
  dim = c(nrow(y_train_data),prediction,1)
)

## PREPARAR DATOS PARA LA PREDICCIÓN
x_test <- serie_30$value[(nrow(scaled_train) - prediction + 1):nrow(scaled_train)]

## ESCALAR Y TRANSFORMAR
x_test_scaled <- (x_test - scale_factors[1])/scale_factors[2]

## LA MATRIZ TIENE UNA MUESTRA Y SE PRETENDE UNA PREDICCIÓN PARA LOS PRÓXIMOS 12 MESES
x_pred_arr <- array(
  data = x_test_scaled,
  dim = c(1,lag,1)
)

for (u in 1:25){
## CREAR EL MODELO
lstm_model <- keras_model_sequential() %>%
  layer_lstm(units = random_grid_LSTM_30$UNITS[u], # size of the layer 70
             batch_input_shape = c(1, 30, 1), # batch size, timesteps, features
             return_sequences = TRUE,
             stateful = TRUE) %>%
  # fraction of the units to drop for the linear transformation of the inputs
  layer_dropout(rate = random_grid_LSTM_30$LAYER_DROP[u]) %>% # dr 0.05
  layer_lstm(units = random_grid_LSTM_30$UNITS[u] ,
             return_sequences = TRUE,
             stateful = TRUE) %>%
  layer_dropout(rate = random_grid_LSTM_30$LAYER_DROP[u]) %>%
  time_distributed(keras::layer_dense(units = 1))

## COMPILAR EL MODELO
lstm_model %>%
  compile(loss = 'mae', 
          optimizer = 'adam', 
          metrics = 'accuracy')

## "binary_crossentropy"       ## PARA CLASIFICACIÓN BINARIO
## "categorical_crossentropy"  ## PARA CLASIFICACIÓN MULTICLASE

# Keras admite varios optimizadores: 
# optimizador de descenso de gradiente estocástico (sgd) 
# estimación de monumento adaptativo (adam)
# tasa de aprendizaje adaptativo (Adadelta), entre otros. 

# Mientras llama al optimizador, el usuario puede especificar la tasa de aprendizaje (lr)
# la tasa de aprendizaje decae en cada actualización (decadencia) 
# entre otros argumentos específicos de cada optimizador.

## MIRAR LA ESTRUCTURA DEL MODELO
summary(lstm_model)

## FIT EL MODELO
lstm_model %>% fit(
  x = x_train_arr,
  y = y_train_arr,
  batch_size = 1,
  epochs = random_grid_LSTM_30$EPOCHS[u], #epochs
  callbacks = callback_early_stopping(patience = 5, monitor="accuracy"),
  validation_split = random_grid_LSTM_30$VALIDATION_SPLIT[u]
)

## verbose = 0,
## shuffle = F,
# epochs: el número de veces que el algoritmo 've' todos los datos de entrenamiento.
# batch_size: el tamaño de la muestra que se pasará a través del algoritmo en cada época. 

## https://keras.rstudio.com/reference/fit.html

## FORECASTING CON EL MODELO LSTM
lstm_forecast <- lstm_model %>%
  predict(x_pred_arr, batch_size = 1) %>%
  .[, , 1]

## SE REESCALA LAS PREDICCIONES A LOS NIVELES ORIGINALES
lstm_forecast <- lstm_forecast * scale_factors[2] + scale_factors[1]

## PREDICCIÓN CON LOS DATOS DE ENTRENAMIENTO
fitted <- predict(lstm_model, x_train_arr, batch_size = 1) %>%
  .[, , 1]

## SE NECESITA TRANSFORMAR LOS DATOS PARA OBTENER SOLO UNA PREDICCIÓN POR CADA FECHA
if(dim(fitted)[2] > 1){
  fit <- c(fitted[, 1], fitted[dim(fitted)[1], 2:dim(fitted)[2]])
} else {
  fit <- fitted[, 1]
}

## ADICIONALMENTE SE REESCALAN LOS DATOS
fitted <- fit * scale_factors[2] + scale_factors[1]
length(fitted)

## ESPECIFICAR LOS PRIMEROS VALORES DE PRONOSTICO COMO NO DISPONIBLES
fitted <- c(rep(NA, lag), fitted)
class(forecast_list) <- "forecast"

  



start_date <- serie_30$date[271]
end_date <- serie_30$date[300] # set the end date to one year after the start date

date_vector <- seq.Date(start_date, end_date, by = "day")
length(date_vector)


papa_30<-serie_30 %>% select(date, value) %>% mutate(descripcion="Historia") %>% tsibble(index = date, key=descripcion)

mama_30<-tsibble(date = date_vector,
  value= lstm_forecast, descripcion="Pronóstico", index=date, key=descripcion)

mofle_30<-tsibble(date=serie_30$date, value=fitted, descripcion="Ajuste LSTM", index=date, key=descripcion) %>% na.omit()


abu_30<-bind_rows(papa_30,mama_30,mofle_30)



#tail(abu)

#olvaldo<-autoplot(abu,mapping=(aes(x=date, y =value, color=descripcion)))

#ggplotly(olvaldo)

actual<-serie_30$value[31:300]

fitted<-mofle_30$value

Metrics::rmse(actual,fitted)
Metrics::mape(actual,fitted)*100
Metrics::mae(actual,fitted)
pan_30_2<-append(pan_30_2,list(Procesamiento=abu_30,rmse=Metrics::rmse(actual,fitted),mape=Metrics::mape(actual,fitted),mae=Metrics::mae(actual,fitted),random_grid_LSTM_30$UNITS[u],random_grid_LSTM_30$LAYER_DROP[u],random_grid_LSTM_30$EPOCHS[u],d,u))

vector_resultados_30_2<-rbind(vector_resultados_30_2,c(Metrics::rmse(actual,fitted),Metrics::mape(actual,fitted)*100,Metrics::mae(actual,fitted),random_grid_LSTM_30$UNITS[u],random_grid_LSTM_30$LAYER_DROP[u],random_grid_LSTM_30$EPOCHS[u],d,u))
}
}


readr::write_rds(pan_30_1, here::here('entrenamiento_resultados_testeo_LSTM_30_1_d.RDS'))
readr::write_rds(vector_calibración_LSTM, here::here('entrenamiento_validacion_cruzada_LSTM_30_1_d.RDS'))




######## Esto es para analizar después #########

vector_calibración_LSTM 

mean_results <- vector_calibración_LSTM %>%
  group_by(ITERACION) %>%
  summarize(mean_RMSE = mean(RMSE),
            mean_MAPE = mean(MAPE),
            mean_MAE = mean(MAE))
head(vector_calibración_LSTM)

mean_results <- aggregate(cbind(RMSE, MAPE, MAE,UNITS,LAYER_DROP,EPOCHS,VALIDATION_SPLIT) ~ ITERACION, data = vector_calibración_LSTM, FUN = mean)

tabla_resultados_calibracion<-mean_results %>% arrange(RMSE)

readr::write_rds(tabla_resultados_calibracion, here::here('tabla_resultados_calibracion_LSTM_30_1_d.RDS'))


g155<-ggplot(data = vector_calibración_LSTM, aes(x = VALIDATION_SPLIT, y = RMSE)) +
  facet_wrap(vars(SPLIT),nrow=1)+  
  geom_point()+
  geom_smooth(se = FALSE)

g156<-ggplot(data = vector_calibración_LSTM, aes(x = EPOCHS, y = RMSE)) +
  facet_wrap(vars(SPLIT),nrow=1)+  
  geom_point()+
  geom_smooth(se = FALSE)


g157<-ggplot(data = vector_calibración_LSTM, aes(x = UNITS, y = RMSE)) +
  facet_wrap(vars(SPLIT),nrow=1)+  
  geom_point()+
  geom_smooth(se = FALSE)

g158<-ggplot(data = vector_calibración_LSTM, aes(x = LAYER_DROP, y = RMSE)) +
  facet_wrap(vars(SPLIT),nrow=1)+  
  geom_point()+
  geom_smooth(se = FALSE)


readr::write_rds(g155, here::here('g155.RDS'))
readr::write_rds(g156, here::here('g156.RDS'))
readr::write_rds(g157, here::here('g157.RDS'))
readr::write_rds(g158, here::here('g158.RDS'))


tail(vector_calibración_LSTM)


readr::write_rds(g55, here::here('g55.RDS'))

ggplot(data = resultados_cv_lstm_30, aes(x = EPOCHS, y = RMSE)) +
  geom_point() +
  facet_grid(SPLIT ~ LAYER_DROP, 
             labeller = labeller(LAYER_DROP = label_both, SPLIT = label_both)) +
  scale_x_discrete(limits = c(50, 100)) +
  geom_smooth(se = FALSE) +
  labs(title = "MAPE by EPOCHS, LAYER_DROP, and SPLIT",
       subtitle = "CV LSTM 30",
       y = "MAPE",
       x = "EPOCHS",
       color = "LAYER_DROP",
       shape = "LAYER_DROP",
       caption = "Source: resultados_cv_lstm_30 dataset")

g56<-ggplot(data = resultados_cv_lstm_30, aes(x = EPOCHS, y = MAPE)) +
  geom_point() +  
  facet_grid(rows = vars(LAYER_DROP), cols = vars(SPLIT), 
             labeller = labeller(LAYER_DROP = label_both, SPLIT = label_both)) +
  scale_x_discrete(limits=c(50,100)) +
  geom_smooth(se=FALSE) +
  labs(
       
       y = "MAPE",
       x = "EPOCHS",
       color = "LAYER_DROP",
       shape = "SPLIT",
       )

readr::write_rds(g56, here::here('g56.RDS'))

g57<-ggplot(data = resultados_cv_lstm_30, aes(x = EPOCHS, y = MAE)) +
  geom_point() +  facet_grid(rows = vars(LAYER_DROP), cols = vars(SPLIT)) +
  scale_x_discrete(limits=c(50,100) )+geom_smooth(se=FALSE)

readr::write_rds(g57, here::here('g57.RDS'))


ggplot(resultados_cv_lstm_30, aes(x=LAYER_DROP, y=RMSE, color=as.factor(SPLIT))) + 
  geom_point() + geom_line()+
  labs(x="Layer Drop", y="RMSE") +
  scale_color_discrete(name="Split")


```

```{r,eval=FALSE,include=FALSE}
rolling_origin_resamples_15 <- rolling_origin(
    prueba_LSTM_30,
    initial    = 270,
    assess     = 15,
    cumulative = FALSE,
    skip       = 200,
)

rolling_origin_resamples_15


split    <- rolling_origin_resamples$splits[[8]]
split_id <- rolling_origin_resamples$id[[8]]

   df_trn <- training(split)
        df_tst <- testing(split)

df <- tibble(bind_rows(df_trn,df_tst))
            
            #dplyr::rename(index=date)

for (i in 1:2){ 

for (d in 1:8){  
#serie<-tibble(prueba %>% filter(date>'2021-03-06'))
#set.seed(123)
split    <- rolling_origin_resamples_15$splits[[d]]
split_id <- rolling_origin_resamples_15$id[[d]]

   df_trn <- training(split)
        df_tst <- testing(split)

serie_15 <- tibble(bind_rows(df_trn,df_tst))
  

#tail(serie)

data.class(serie_15$value)

autoplot(ts(serie_15$value))

## DETERMINAR LOS FACTORES DE ESCALAMIENTO: MEDIA Y DESVIO DE LA SERIE
scale_factors <- c(mean(serie_15$value), sd(serie_15$value))
print(scale_factors)

## SELECCIONAR VARIABLE DE INTERÉS Y ESCALARLA
scaled_train <- serie_15 %>% 
  select(value) %>%
  mutate(value = (value - scale_factors[1]) / scale_factors[2])

## DEFINIR HORIZONTE DE PREDICCIÓN
prediction <- 15
lag <- prediction

## DEFINIR COMO UNA MATRIZ AL CONJUNTO DE DATOS ESCALADOS
scaled_train <- as.matrix(scaled_train)

## RETRASAR LOS DATOS 29 VECES Y ORGANIZARLOS COMO COLUMNAS
x_train_data <- t(sapply(
  1:(length(scaled_train) - lag - prediction + 1),
  function(x) scaled_train[x:(x + lag - 1), 1]
))

## TRANSFORMAR EN FORMATO ARRAY
x_train_arr <- array(
  data = as.numeric(unlist(x_train_data)),
  dim = c(nrow(x_train_data),lag,1)
)

## SIMILAR TRANSFORMACIÓN PARA LOS VALORES DE Y
y_train_data <- t(sapply(
  (1 + lag):(length(scaled_train) - prediction + 1),
  function(x) scaled_train[x:(x + prediction - 1)]
))

y_train_arr <- array(
  data = as.numeric(unlist(y_train_data)),
  dim = c(nrow(y_train_data),prediction,1)
)

## PREPARAR DATOS PARA LA PREDICCIÓN
x_test <- serie$value[(nrow(scaled_train) - prediction + 1):nrow(scaled_train)]

## ESCALAR Y TRANSFORMAR
x_test_scaled <- (x_test - scale_factors[1])/scale_factors[2]

## LA MATRIZ TIENE UNA MUESTRA Y SE PRETENDE UNA PREDICCIÓN PARA LOS PRÓXIMOS 12 MESES
x_pred_arr <- array(
  data = x_test_scaled,
  dim = c(1,lag,1)
)

for (u in c(25,150)){
  for (dr in c(0.3,0.1)){
    for (e in c(25,200)){
## CREAR EL MODELO
lstm_model <- keras_model_sequential() %>%
  layer_lstm(units = u, # size of the layer 70
             batch_input_shape = c(1, 15, 1), # batch size, timesteps, features
             return_sequences = TRUE,
             stateful = TRUE) %>%
  # fraction of the units to drop for the linear transformation of the inputs
  layer_dropout(rate = dr) %>% # dr 0.05
  layer_lstm(units = u,
             return_sequences = TRUE,
             stateful = TRUE) %>%
  layer_dropout(rate = dr) %>%
  time_distributed(keras::layer_dense(units = 1))

## COMPILAR EL MODELO
lstm_model %>%
  compile(loss = 'mae', 
          optimizer = 'adam', 
          metrics = 'accuracy')

## "binary_crossentropy"       ## PARA CLASIFICACIÓN BINARIO
## "categorical_crossentropy"  ## PARA CLASIFICACIÓN MULTICLASE

# Keras admite varios optimizadores: 
# optimizador de descenso de gradiente estocástico (sgd) 
# estimación de monumento adaptativo (adam)
# tasa de aprendizaje adaptativo (Adadelta), entre otros. 

# Mientras llama al optimizador, el usuario puede especificar la tasa de aprendizaje (lr)
# la tasa de aprendizaje decae en cada actualización (decadencia) 
# entre otros argumentos específicos de cada optimizador.

## MIRAR LA ESTRUCTURA DEL MODELO
summary(lstm_model)

## FIT EL MODELO
lstm_model %>% fit(
  x = x_train_arr,
  y = y_train_arr,
  batch_size = 1,
  epochs = e, #epochs
  callbacks = callback_early_stopping(patience = 5, monitor="accuracy"),
  validation_split = 0.1
)

## verbose = 0,
## shuffle = F,
# epochs: el número de veces que el algoritmo 've' todos los datos de entrenamiento.
# batch_size: el tamaño de la muestra que se pasará a través del algoritmo en cada época. 

## https://keras.rstudio.com/reference/fit.html

## FORECASTING CON EL MODELO LSTM
lstm_forecast <- lstm_model %>%
  predict(x_pred_arr, batch_size = 1) %>%
  .[, , 1]

## SE REESCALA LAS PREDICCIONES A LOS NIVELES ORIGINALES
lstm_forecast <- lstm_forecast * scale_factors[2] + scale_factors[1]

## PREDICCIÓN CON LOS DATOS DE ENTRENAMIENTO
fitted <- predict(lstm_model, x_train_arr, batch_size = 1) %>%
  .[, , 1]

## SE NECESITA TRANSFORMAR LOS DATOS PARA OBTENER SOLO UNA PREDICCIÓN POR CADA FECHA
if(dim(fitted)[2] > 1){
  fit <- c(fitted[, 1], fitted[dim(fitted)[1], 2:dim(fitted)[2]])
} else {
  fit <- fitted[, 1]
}

## ADICIONALMENTE SE REESCALAN LOS DATOS
fitted <- fit * scale_factors[2] + scale_factors[1]
length(fitted)

## ESPECIFICAR LOS PRIMEROS VALORES DE PRONOSTICO COMO NO DISPONIBLES
fitted <- c(rep(NA, lag), fitted)
class(forecast_list) <- "forecast"

  



start_date <- serie$date[271]
end_date <- serie$date[285] # set the end date to one year after the start date

date_vector <- seq.Date(start_date, end_date, by = "day")
length(date_vector)


papa_15<-serie_15 %>% select(date, value) %>% mutate(descripcion="Historia") %>% tsibble(index = date, key=descripcion)

mama_15<-tsibble(date = date_vector,
  value= lstm_forecast, descripcion="Pronóstico", index=date, key=descripcion)

mofle_15<-tsibble(date=serie_15$date, value=fitted, descripcion="Ajuste LSTM", index=date, key=descripcion) %>% na.omit()

abu_15<-bind_rows(papa_15,mama_15,mofle_15)




#tail(abu)

#olvaldo<-autoplot(abu,mapping=(aes(x=date, y =value, color=descripcion)))

#ggplotly(olvaldo)

actual<-serie_15$value[16:275]

fitted<-mofle_15$value

Metrics::rmse(actual,fitted)
Metrics::mape(actual,fitted)*100
Metrics::mae(actual,fitted)

pan_15<-append(pan_15,list(Procesamiento=abu_15,rmse=Metrics::rmse(actual,fitted),mape=Metrics::mape(actual,fitted),mae=Metrics::mae(actual,fitted),u,dr,e,d,i))
vector_resultados_15<-rbind(vector_resultados_15,c(Metrics::rmse(actual,fitted),Metrics::mape(actual,fitted)*100,Metrics::mae(actual,fitted),u,dr,e,d,i))
}
}
}
}
}
readr::write_rds(pan_15, here::here('entrenamiento_resultados_testeo_LSTM_15_d.RDS'))
readr::write_rds(vector_resultados_15, here::here('entrenamiento_validacion_cruzada_LSTM_15_d.RDS'))

resultados_cv_lstm_15<-data.frame(vector_resultados_15) %>% dplyr::rename(RMSE=X1,MAPE=X2,MAE=X3,UNITS=X4,LAYER_DROP=X5,EPOCHS=X6,SPLIT=X7,INDICE=X8)%>%arrange(RMSE,SPLIT,by_group=SPLIT)
  

g58<-ggplot(data = resultados_cv_lstm_15, aes(x = EPOCHS, y = RMSE)) +
  geom_point() +  facet_grid(rows = vars(LAYER_DROP), cols = vars(SPLIT)) +
  scale_x_discrete(limits=c(50,100) )+geom_smooth(se=FALSE)

readr::write_rds(g58, here::here('g58.RDS'))


g59<-ggplot(data = resultados_cv_lstm_15, aes(x = EPOCHS, y = MAPE)) +
  geom_point() +  facet_grid(rows = vars(LAYER_DROP), cols = vars(SPLIT)) +
  scale_x_discrete(limits=c(50,100) )+geom_smooth(se=FALSE)

readr::write_rds(g59, here::here('g59.RDS'))

g60<-ggplot(data = resultados_cv_lstm_15, aes(x = EPOCHS, y = MAE)) +
  geom_point() +  facet_grid(rows = vars(LAYER_DROP), cols = vars(SPLIT)) +
  scale_x_discrete(limits=c(50,100) )+geom_smooth(se=FALSE)

readr::write_rds(g60, here::here('g60.RDS'))




```


```{r,eval=FALSE,include=FALSE}
rolling_origin_resamples_5 <- rolling_origin(
    prueba_LSTM_30,
    initial    = 270,
    assess     = 5,
    cumulative = FALSE,
    skip       = 200,
)

rolling_origin_resamples_5

for (i in 1:2){ 

for (d in 1:8){  
#serie<-tibble(prueba %>% filter(date>'2021-03-06'))
#set.seed(123)
split    <- rolling_origin_resamples_5$splits[[d]]
split_id <- rolling_origin_resamples_5$id[[d]]

   df_trn <- training(split)
        df_tst <- testing(split)

serie_5 <- tibble(bind_rows(df_trn,df_tst))
  

#tail(serie)

data.class(serie_5$value)

autoplot(ts(serie_5$value))

## DETERMINAR LOS FACTORES DE ESCALAMIENTO: MEDIA Y DESVIO DE LA SERIE
scale_factors <- c(mean(serie_5$value), sd(serie_5$value))
print(scale_factors)

## SELECCIONAR VARIABLE DE INTERÉS Y ESCALARLA
scaled_train <- serie_5 %>% 
  select(value) %>%
  mutate(value = (value - scale_factors[1]) / scale_factors[2])

## DEFINIR HORIZONTE DE PREDICCIÓN
prediction <- 5
lag <- prediction

## DEFINIR COMO UNA MATRIZ AL CONJUNTO DE DATOS ESCALADOS
scaled_train <- as.matrix(scaled_train)

## RETRASAR LOS DATOS 29 VECES Y ORGANIZARLOS COMO COLUMNAS
x_train_data <- t(sapply(
  1:(length(scaled_train) - lag - prediction + 1),
  function(x) scaled_train[x:(x + lag - 1), 1]
))

## TRANSFORMAR EN FORMATO ARRAY
x_train_arr <- array(
  data = as.numeric(unlist(x_train_data)),
  dim = c(nrow(x_train_data),lag,1)
)

## SIMILAR TRANSFORMACIÓN PARA LOS VALORES DE Y
y_train_data <- t(sapply(
  (1 + lag):(length(scaled_train) - prediction + 1),
  function(x) scaled_train[x:(x + prediction - 1)]
))

y_train_arr <- array(
  data = as.numeric(unlist(y_train_data)),
  dim = c(nrow(y_train_data),prediction,1)
)

## PREPARAR DATOS PARA LA PREDICCIÓN
x_test <- serie$value[(nrow(scaled_train) - prediction + 1):nrow(scaled_train)]

## ESCALAR Y TRANSFORMAR
x_test_scaled <- (x_test - scale_factors[1])/scale_factors[2]

## LA MATRIZ TIENE UNA MUESTRA Y SE PRETENDE UNA PREDICCIÓN PARA LOS PRÓXIMOS 12 MESES
x_pred_arr <- array(
  data = x_test_scaled,
  dim = c(1,lag,1)
)

for (u in c(25,150)){
  for (dr in c(0.3,0.1)){
    for (e in c(25,200)){
## CREAR EL MODELO
lstm_model <- keras_model_sequential() %>%
  layer_lstm(units = u, # size of the layer 70
             batch_input_shape = c(1, 5, 1), # batch size, timesteps, features
             return_sequences = TRUE,
             stateful = TRUE) %>%
  # fraction of the units to drop for the linear transformation of the inputs
  layer_dropout(rate = dr) %>% # dr 0.05
  layer_lstm(units = u,
             return_sequences = TRUE,
             stateful = TRUE) %>%
  layer_dropout(rate = dr) %>%
  time_distributed(keras::layer_dense(units = 1))

## COMPILAR EL MODELO
lstm_model %>%
  compile(loss = 'mae', 
          optimizer = 'adam', 
          metrics = 'accuracy')

## "binary_crossentropy"       ## PARA CLASIFICACIÓN BINARIO
## "categorical_crossentropy"  ## PARA CLASIFICACIÓN MULTICLASE

# Keras admite varios optimizadores: 
# optimizador de descenso de gradiente estocástico (sgd) 
# estimación de monumento adaptativo (adam)
# tasa de aprendizaje adaptativo (Adadelta), entre otros. 

# Mientras llama al optimizador, el usuario puede especificar la tasa de aprendizaje (lr)
# la tasa de aprendizaje decae en cada actualización (decadencia) 
# entre otros argumentos específicos de cada optimizador.

## MIRAR LA ESTRUCTURA DEL MODELO
summary(lstm_model)

## FIT EL MODELO
lstm_model %>% fit(
  x = x_train_arr,
  y = y_train_arr,
  batch_size = 1,
  epochs = e, #epochs
  callbacks = callback_early_stopping(patience = 5, monitor="accuracy"),
  validation_split = 0.1
)

## verbose = 0,
## shuffle = F,
# epochs: el número de veces que el algoritmo 've' todos los datos de entrenamiento.
# batch_size: el tamaño de la muestra que se pasará a través del algoritmo en cada época. 

## https://keras.rstudio.com/reference/fit.html

## FORECASTING CON EL MODELO LSTM
lstm_forecast <- lstm_model %>%
  predict(x_pred_arr, batch_size = 1) %>%
  .[, , 1]

## SE REESCALA LAS PREDICCIONES A LOS NIVELES ORIGINALES
lstm_forecast <- lstm_forecast * scale_factors[2] + scale_factors[1]

## PREDICCIÓN CON LOS DATOS DE ENTRENAMIENTO
fitted <- predict(lstm_model, x_train_arr, batch_size = 1) %>%
  .[, , 1]

## SE NECESITA TRANSFORMAR LOS DATOS PARA OBTENER SOLO UNA PREDICCIÓN POR CADA FECHA
if(dim(fitted)[2] > 1){
  fit <- c(fitted[, 1], fitted[dim(fitted)[1], 2:dim(fitted)[2]])
} else {
  fit <- fitted[, 1]
}

## ADICIONALMENTE SE REESCALAN LOS DATOS
fitted <- fit * scale_factors[2] + scale_factors[1]
length(fitted)

## ESPECIFICAR LOS PRIMEROS VALORES DE PRONOSTICO COMO NO DISPONIBLES
fitted <- c(rep(NA, lag), fitted)
class(forecast_list) <- "forecast"

  



start_date <- serie_5$date[271]
end_date <- serie_5$date[275] # set the end date to one year after the start date

date_vector <- seq.Date(start_date, end_date, by = "day")
length(date_vector)


papa_5<-serie_5 %>% select(date, value) %>% mutate(descripcion="Historia") %>% tsibble(index = date, key=descripcion)

mama_5<-tsibble(date = date_vector,
  value= lstm_forecast, descripcion="Pronóstico", index=date, key=descripcion)

mofle_5<-tsibble(date=serie_5$date, value=fitted, descripcion="Ajuste LSTM", index=date, key=descripcion) %>% na.omit()

abu_5<-bind_rows(papa_5,mama_5,mofle_5)




#tail(abu)

#olvaldo<-autoplot(abu,mapping=(aes(x=date, y =value, color=descripcion)))

#ggplotly(olvaldo)

actual<-serie_5$value[6:270]

fitted<-mofle_5$value

Metrics::rmse(actual,fitted)
Metrics::mape(actual,fitted)*100
Metrics::mae(actual,fitted)
pan_5<-append(pan_5,list(Procesamiento=abu_5,rmse=Metrics::rmse(actual,fitted),mape=Metrics::mape(actual,fitted),mae=Metrics::mae(actual,fitted),u,dr,e,d,i))
vector_resultados_5<-rbind(vector_resultados_5,c(Metrics::rmse(actual,fitted),Metrics::mape(actual,fitted)*100,Metrics::mae(actual,fitted),u,dr,e,d,i))
}
}
}
}
}
readr::write_rds(pan_5, here::here('entrenamiento_resultados_testeo_LSTM_5_d.RDS'))
readr::write_rds(vector_resultados_5, here::here('entrenamiento_validacion_cruzada_LSTM_5_d.RDS'))

resultados_cv_lstm_5<-data.frame(vector_resultados_5) %>% dplyr::rename(RMSE=X1,MAPE=X2,MAE=X3,UNITS=X4,LAYER_DROP=X5,EPOCHS=X6,SPLIT=X7,INDICE=X8)%>%arrange(RMSE,SPLIT,by_group=SPLIT)
  

g61<-ggplot(data = resultados_cv_lstm_5, aes(x = EPOCHS, y = RMSE)) +
  geom_point() +  facet_grid(rows = vars(LAYER_DROP), cols = vars(SPLIT)) +
  scale_x_discrete(limits=c(50,100) )+geom_smooth(se=FALSE)

readr::write_rds(g61, here::here('g61.RDS'))


g62<-ggplot(data = resultados_cv_lstm_5, aes(x = EPOCHS, y = MAPE)) +
  geom_point() +  facet_grid(rows = vars(LAYER_DROP), cols = vars(SPLIT)) +
  scale_x_discrete(limits=c(50,100) )+geom_smooth(se=FALSE)

readr::write_rds(g62, here::here('g62.RDS'))

g63<-ggplot(data = resultados_cv_lstm_5, aes(x = EPOCHS, y = MAE)) +
  geom_point() +  facet_grid(rows = vars(LAYER_DROP), cols = vars(SPLIT)) +
  scale_x_discrete(limits=c(50,100) )+geom_smooth(se=FALSE)

readr::write_rds(g63, here::here('g63.RDS'))



```

```{r,eval=F,include=F}
## VIEW SOBRE LOS PRIMEROS DATOS DEL DATASET

prueba2

prueba_LSTM<-as.tibble(prueba_LSTM_30)

set.seed(123)
serie <- prueba_LSTM$value
data.class(serie)

autoplot(ts(serie))

## DETERMINAR LOS FACTORES DE ESCALAMIENTO: MEDIA Y DESVIO DE LA SERIE
scale_factors <- c(mean(serie), sd(serie))
print(scale_factors)

## SELECCIONAR VARIABLE DE INTERÉS Y ESCALARLA
scaled_train <- prueba_LSTM %>% 
  dplyr::select(value) %>%
  dplyr::mutate(value = (value - scale_factors[1]) / scale_factors[2])

## DEFINIR HORIZONTE DE PREDICCIÓN
prediction <- 30
lag <- prediction

## DEFINIR COMO UNA MATRIZ AL CONJUNTO DE DATOS ESCALADOS
scaled_train <- as.matrix(scaled_train)

## RETRASAR LOS DATOS 11 VECES Y ORGANIZARLOS COMO COLUMNAS
x_train_data <- t(sapply(
  1:(length(scaled_train) - lag - prediction + 1),
  function(x) scaled_train[x:(x + lag - 1), 1]
))

## TRANSFORMAR EN FORMATO ARRAY
x_train_arr <- array(
  data = as.numeric(unlist(x_train_data)),
  dim = c(nrow(x_train_data),lag,1)
)

## SIMILAR TRANSFORMACIÓN PARA LOS VALORES DE Y
y_train_data <- t(sapply(
  (1 + lag):(length(scaled_train) - prediction + 1),
  function(x) scaled_train[x:(x + prediction - 1)]
))

y_train_arr <- array(
  data = as.numeric(unlist(y_train_data)),
  dim = c(nrow(y_train_data),prediction,1)
)

## PREPARAR DATOS PARA LA PREDICCIÓN
x_test <- prueba_LSTM$value[(nrow(scaled_train) - prediction + 1):nrow(scaled_train)]

## ESCALAR Y TRANSFORMAR
x_test_scaled <- (x_test - scale_factors[1])/scale_factors[2]

## LA MATRIZ TIENE UNA MUESTRA Y SE PRETENDE UNA PREDICCIÓN PARA LOS PRÓXIMOS 12 MESES
x_pred_arr <- array(
  data = x_test_scaled,
  dim = c(1,lag,1)
)

## CREAR EL MODELO
lstm_model <- keras_model_sequential() %>%
  layer_lstm(units = 272, # size of the layer
             batch_input_shape = c(1, 30, 1), # batch size, timesteps, features
             return_sequences = TRUE,
             stateful = FALSE) %>%
  # fraction of the units to drop for the linear transformation of the inputs
  layer_dropout(rate = 0.078) %>%
  layer_lstm(units = 272,
             return_sequences = TRUE,
             stateful = FALSE) %>%
  layer_dropout(rate = 0.078) %>%
  time_distributed(keras::layer_dense(units = 1))

## COMPILAR EL MODELO
lstm_model %>%
  compile(loss = 'logcosh', 
          optimizer = 'sgd', 
          metrics = list('mean_squared_error'))

## "binary_crossentropy"       ## PARA CLASIFICACIÓN BINARIO
## "categorical_crossentropy"  ## PARA CLASIFICACIÓN MULTICLASE

# Keras admite varios optimizadores: 
# optimizador de descenso de gradiente estocástico (sgd) 
# estimación de monumento adaptativo (adam)
# tasa de aprendizaje adaptativo (Adadelta), entre otros. 

# Mientras llama al optimizador, el usuario puede especificar la tasa de aprendizaje (lr)
# la tasa de aprendizaje decae en cada actualización (decadencia) 
# entre otros argumentos específicos de cada optimizador.

## MIRAR LA ESTRUCTURA DEL MODELO
summary(lstm_model)

## FIT EL MODELO
lstm_model %>% fit(
  x = x_train_arr,
  y = y_train_arr,
  batch_size = 1,
  epochs = 20,
  callbacks = callback_early_stopping(patience = 5, monitor = 'loss'),
  validation_split = 0.116
)

## verbose = 0,
## shuffle = F,
# epochs: el número de veces que el algoritmo 've' todos los datos de entrenamiento.
# batch_size: el tamaño de la muestra que se pasará a través del algoritmo en cada época. 

## https://keras.rstudio.com/reference/fit.html

## FORECASTING CON EL MODELO LSTM
lstm_forecast <- lstm_model %>%
  predict(x_pred_arr, batch_size = 1) %>%
  .[, , 1]

## SE REESCALA LAS PREDICCIONES A LOS NIVELES ORIGINALES
lstm_forecast <- lstm_forecast * scale_factors[2] + scale_factors[1]

## PREDICCIÓN CON LOS DATOS DE ENTRENAMIENTO
fitted <- predict(lstm_model, x_train_arr, batch_size = 1) %>%
  .[, , 1]

## SE NECESITA TRANSFORMAR LOS DATOS PARA OBTENER SOLO UNA PREDICCIÓN POR CADA FECHA
if(dim(fitted)[2] > 1){
  fit <- c(fitted[, 1], fitted[dim(fitted)[1], 2:dim(fitted)[2]])
} else {
  fit <- fitted[, 1]
}

## ADICIONALMENTE SE REESCALAN LOS DATOS
fitted <- fit * scale_factors[2] + scale_factors[1]

## ESPECIFICAR LOS PRIMEROS VALORES DE PRONOSTICO COMO NO DISPONIBLES
fitted <- c(rep(NA, lag), fitted)

## CONFIGURAR LAS PREDICCIONES COMO SERIE TEMPORAL
start_date <- prueba_LSTM$date[1827]
end_date <- prueba_LSTM$date[1856] # set the end date to one year after the start date

date_vector <- seq.Date(start_date, end_date, by = "day")
length(date_vector)


papa<-prueba_LSTM %>% select(date, value) %>% mutate(descripcion="Historia") %>% tsibble(index = date, key=descripcion)

mama<-tsibble(date = date_vector,
  value= lstm_forecast, descripcion="Pronóstico", index=date, key=descripcion)

mofle<-tsibble(date=prueba_LSTM$date, value=fitted, descripcion="Ajuste LSTM", index=date, key=descripcion) %>% na.omit()


abu<-bind_rows(papa,mama,mofle)

olvaldo<-autoplot(abu,mapping=(aes(x=date, y =value, color=descripcion)))
ggplotly(olvaldo)

actual<-tsibble(prueba_LSTM$value,date=date_vector)
estimado<-mama

Metrics::rmse(actual$`<dbl>`,estimado$value)

```

```{r,eval=F,include=F}
## TESTEO DEL PAQUETE TSLSTM##########

caca<-ts(prueba_LSTM$value, start=c(2017,1),frequency = 365)

tsLag=30
emilio<-list()

set.seed(123)
for (i in 1:100){

cacau<-ts.lstm(
            caca,
            xreg = NULL,
            tsLag,
            xregLag = 0,
            LSTMUnits=272,
            DropoutRate =  0.078,
            Epochs = 10,
            CompLoss = "mse",
            CompMetrics = "mae",
            ActivationFn = "tanh",
            SplitRatio = 0.9835668,
            ValidationSplit = 0.01616379
)

emilio<-c(emilio,cacau,i) 
}

readr::write_rds(emilio, here::here('emilio.RDS'))

tail(emilio[[1]])

seq(3,400,4)
c<-0
for (j in seq(3,400,4)){
  c<-c+emilio[[j]][2]
  print(emilio[[j]][2])
}
c/100

length(emilio[[1]])

ajustes_emilio<-data.frame(Ajuste=emilio[[1]],Iteracion=rep(1,length(emilio[[1]])))

ajustes_emilio_2<-rbind(ajustes_emilio, data.frame(Ajuste=emilio[[5]],Iteracion=rep(1,length(emilio[[5]]))))

date_vector

## CONFIGURAR LAS PREDICCIONES COMO SERIE TEMPORAL
start_date <- prueba_LSTM$date[1827]
end_date <- prueba_LSTM$date[1856] # set the end date to one year after the start date

date_vector <- seq.Date(start_date, end_date, by = "day")
length(date_vector)


papa<-prueba_LSTM %>% select(date, value) %>% mutate(descripcion="Historia") %>% tsibble(index = date, key=descripcion)


pronosticos_emilio<-data.frame()
for (q in seq(2,400,4)){
  pronosticos_emilio<-rbind(pronosticos_emilio,data.frame(date=date_vector,value=emilio[[q]],descripcion=as.character(rep(q, length(emilio[[q]])))))
}
pronosticos_emilio<-pronosticos_emilio %>% tsibble(index=date,key = descripcion)


df_pronostico_lstm_30<-bind_rows(papa,pronosticos_emilio)

my_colors <- rainbow(101)

g65<-autoplot(df_pronostico_lstm_30 %>% filter(date>"2021-01-01"),mapping=(aes(x=date, y =value, color=descripcion)))+
  theme(legend.position = "none")+
  scale_color_manual(values = my_colors)+
  geom_line(size=0.05)+
  xlab("Fecha")+
  ylab("Precio de cierre ETH/USD")

########### gráficos, tablas y archivo de calculo para 30 días ######
readr::write_rds(g65, here::here('g65.RDS'))

readr::write_rds(df_pronostico_lstm_30, here::here('df_pronostico_lstm_30.RDS'))

readr::write_rds(pronosticos_emilio, here::here('pronosticos_emilio.RDS'))

#####################################################################

g64<-autoplot(pronosticos_emilio)

readr::write_rds(g64, here::here('g64.RDS'))



caca_15<-ts(prueba_LSTM$value[1:1841], start=c(2017,1),frequency = 365)

1-15.5/1841

tsLag=15
emilio_15<-list()

set.seed(123)
for (i in 1:100){

cacau_15<-ts.lstm(
            caca_15,
            xreg = NULL,
            tsLag,
            xregLag = 0,
            LSTMUnits=272,
            DropoutRate =  0.078,
            Epochs = 10,
            CompLoss = "mse",
            CompMetrics = "mae",
            ActivationFn = "tanh",
            SplitRatio = 0.9915807,
            ValidationSplit = 0.008419337
)

emilio_15<-c(emilio_15,cacau_15,i) 
}

readr::write_rds(emilio_15, here::here('emilio_15.RDS'))





seq(3,400,4)
c<-0
for (j in seq(3,400,4)){
  c<-c+emilio_15[[j]][2]
  print(emilio_15[[j]][2])
}
c/100

#####################
## CONFIGURAR LAS PREDICCIONES COMO SERIE TEMPORAL
start_date_15 <- prueba_LSTM$date[1827]
end_date_15 <- prueba_LSTM$date[1841] # set the end date to one year after the start date

date_vector_15 <- seq.Date(start_date_15, end_date_15, by = "day")
length(date_vector_15)


papa<-prueba_LSTM %>% select(date, value) %>% mutate(descripcion="Historia") %>% tsibble(index = date, key=descripcion)

emilio_15[[2]]

pronosticos_emilio_15<-data.frame()
for (q in seq(2,400,4)){
  pronosticos_emilio_15<-rbind(pronosticos_emilio_15,data.frame(date=date_vector_15,value=emilio_15[[q]],descripcion=as.character(rep(q, length(emilio_15[[q]])))))
}
pronosticos_emilio_15<-pronosticos_emilio_15 %>% tsibble(index=date,key = descripcion)


df_pronostico_lstm_15<-bind_rows(papa,pronosticos_emilio_15)

my_colors <- rainbow(101)

g66<-autoplot(df_pronostico_lstm_15 %>% filter(date >="2021-01-01" & date <="2022-01-15"),mapping=(aes(x=date, y =value, color=descripcion)))+
  theme(legend.position = "none")+
  scale_color_manual(values = my_colors)+
  geom_line(size=0.05)+
  xlab("Fecha")+
  ylab("Precio de cierre ETH/USD")

########### gráficos, tablas y archivo de calculo para 30 días ######
readr::write_rds(g66, here::here('g66.RDS'))

readr::write_rds(df_pronostico_lstm_15, here::here('df_pronostico_lstm_15.RDS'))

readr::write_rds(pronosticos_emilio_15, here::here('pronosticos_emilio_15.RDS'))

#####################################################################


########################
emilio_15[[300]]

1-15.5/1856



plot(emilio[[1]],type='l')
lines(caca,type='l')

length(emilio[[1]])
length(emilio[[2]])


tsLag=5
emilio_5<-list()
caca_5<-ts(prueba_LSTM$value[1:1831], start=c(2017,1),frequency = 365)


set.seed(123)
for (i in 1:100){

cacau_5<-ts.lstm(
            caca_5,
            xreg = NULL,
            tsLag,
            xregLag = 0,
            LSTMUnits=272,
            DropoutRate =  0.078,
            Epochs = 10,
            CompLoss = "mse",
            CompMetrics = "mae",
            ActivationFn = "tanh",
            SplitRatio = 0.9971054,
            ValidationSplit =0.002894593
)
emilio_5<-c(emilio_5,cacau_5,i) 


}

1-5.3/1831

seq(3,400,4)
c<-0
for (j in seq(3,400,4)){
  c<-c+emilio_5[[j]][2]
  print(emilio_5[[j]][2])
}
c/100

readr::write_rds(emilio_5, here::here('emilio_5.RDS'))

#####################
## CONFIGURAR LAS PREDICCIONES COMO SERIE TEMPORAL
start_date_5 <- prueba_LSTM$date[1827]
end_date_5 <- prueba_LSTM$date[1831] # set the end date to one year after the start date

date_vector_5 <- seq.Date(start_date_5, end_date_5, by = "day")
length(date_vector_5)


papa<-prueba_LSTM %>% select(date, value) %>% mutate(descripcion="Historia") %>% tsibble(index = date, key=descripcion)


pronosticos_emilio_5<-data.frame()
for (q in seq(2,400,4)){
  pronosticos_emilio_5<-rbind(pronosticos_emilio_5,data.frame(date=date_vector_5,value=emilio_5[[q]],descripcion=as.character(rep(q, length(emilio_5[[q]])))))
}
pronosticos_emilio_5<-pronosticos_emilio_5 %>% tsibble(index=date,key = descripcion)


df_pronostico_lstm_5<-bind_rows(papa,pronosticos_emilio_5)

my_colors <- rainbow(101)

g67<-autoplot(df_pronostico_lstm_5 %>% filter(date >="2021-01-01" & date <="2022-01-5"),mapping=(aes(x=date, y =value, color=descripcion)))+
  theme(legend.position = "none")+
  scale_color_manual(values = my_colors)+
  geom_line(size=0.05)+
  xlab("Fecha")+
  ylab("Precio de cierre ETH/USD")

########### gráficos, tablas y archivo de calculo para 30 días ######
readr::write_rds(g67, here::here('g67.RDS'))

readr::write_rds(df_pronostico_lstm_5, here::here('df_pronostico_lstm_5.RDS'))

readr::write_rds(pronosticos_emilio_5, here::here('pronosticos_emilio_5.RDS'))

#####################################################################


#### Errores MAE, RMSE y MAPE  promedio para todos los horizontes con LSTM:
mae_15_lstm_prom<-0
rmse_15_lstm_prom<-0
mape_15_lstm_prom<-0

mae_5_lstm_prom<-0
rmse_5_lstm_prom<-0
mape_5_lstm_prom<-0

mae_30_lstm_prom<-0
rmse_30_lstm_prom<-0
mape_30_lstm_prom<-0


for (k in seq(2,400,4)){
  
mae_15_lstm_prom<-mae_15_lstm_prom+Metrics::mae(caca[1826:1840],emilio_15[[k]])
rmse_15_lstm_prom<-rmse_15_lstm_prom+Metrics::rmse(caca[1826:1840],emilio_15[[k]])
mape_15_lstm_prom<-mape_15_lstm_prom+Metrics::mape(caca[1826:1840],emilio_15[[k]])

mae_5_lstm_prom<-mae_5_lstm_prom+Metrics::mae(caca[1826:1830],emilio_5[[k]])
rmse_5_lstm_prom<-rmse_5_lstm_prom+Metrics::rmse(caca[1826:1830],emilio_5[[k]])
mape_5_lstm_prom<-mape_5_lstm_prom+Metrics::mape(caca[1826:1830],emilio_5[[k]])

mae_30_lstm_prom<-mae_30_lstm_prom+Metrics::mae(caca[1826:1855],emilio[[k]])
rmse_30_lstm_prom<-rmse_30_lstm_prom+Metrics::rmse(caca[1826:1855],emilio[[k]])
mape_30_lstm_prom<-mape_30_lstm_prom+Metrics::mape(caca[1826:1855],emilio[[k]])




}

mae_15_lstm_prom/100
rmse_15_lstm_prom/100
mape_15_lstm_prom/100

mae_5_lstm_prom/100
rmse_5_lstm_prom/100
mape_5_lstm_prom/100

mae_30_lstm_prom/100
rmse_30_lstm_prom/100
mape_30_lstm_prom/100

resultados_promedio_lstm<-data.frame(row.names = c("5 días","15 días","30 días"), MAE=c(mae_5_lstm_prom/100,mae_15_lstm_prom/100,mae_30_lstm_prom/100), RMSE=c(rmse_5_lstm_prom/100,rmse_15_lstm_prom/100,rmse_30_lstm_prom/100),MAPE=c(mape_5_lstm_prom,mape_15_lstm_prom,mape_30_lstm_prom))

readr::write_rds(resultados_promedio_lstm, here::here('resultados_promedio_lstm.RDS'))




emilio_5[[2]]


cacau_5$AccuracyTable





```

\newpage

# 7-Resultados y discusión:

Una vez evaluados los distintos componentes del marco empírico se procede a agrupar los resultados y a presentar los mismos de manera ordenada para poder realizar una evaluación de los métodos empleados. 

Como se menciona a lo largo del marco teórico y el marco empírico, para la evaluación de cada modelo de pronóstico se utilizan métricas relativas al error como el error cuadrático medio, el error absoluto medio y el error medio porcentual. A ello se suman los criterios de información y las pruebas de Mincer Zarnowitz en el caso de los modelos paramétricos. 

La naturaleza de los modelos empleados permite una comparación objetiva entre técnicas paramétricas y no paramétricas en el caso de la predicción de los precios de cierre de ETH a través de las métricas en el horizonte de predicción planteado. Además de las métricas calculadas, es necesario mencionar características particulares de cada método a fin de dar mayor información sobre las pruebas empíricas con las series de precios de cierre y retornos de ETH/USD.   

Por esto se decide realizar la evaluación entre los modelos paramétricos, luego la evaluación de las técnicas no paramétricas para predicción del precio de cierre mencionando las características relevantes de cada uno y finalmente comparar las métricas propuestas.

En el caso del estudio de la volatilidad se presentan los resultados, particularidades y comentarios al final de este capítulo. 

\newpage
## Modelos Paramétricos: 

Se presentan a continuación las métricas obtenidas para los modelos ETS, ARIMA y ARFIMA utilizados para la predicción del precio de cierre de ETH/USD. Se presentan las métricas obtenidas tanto en los horizontes objetivos del trabajo (5 días, 15 días y 30 días) tanto como durante la validación cruzada de los modelos.

En el cuadro a continuación se presentan las métricas para los pronósticos a 5,15 y 30 días en el mes de enero de 2022.

```{r,echo=F}
ra<-as.tibble(read_xlsx('~/tabla_metricas_parametricos.xlsx'))

knitr::kable(ra,
        caption = "Métricas modelos paramétricos en pronostico de precio de cierre de ETH/USD. Fuente: Elaboración Propia",caption.short="Métricas modelos paramétricos en pronostico de precio de cierre de ETH/USD."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)


```

En base al criterio de las métricas (RMSE, MAE y MAPE) en los horizontes previstos para el mes de enero de 2022 el modelo ARFIMA resulta más preciso en 15 y 30 días pero en el pronóstico de 5 días el modelo ETS resulta más preciso.

Al observar la tabla de métricas promedio de los modelos durante los distintos intervalos definidos para aplicar la validación cruzada se obtienen los resultados que se muestran en la tabla: 

```{r,echo=F}
ras<- data.frame(
  MODELO = c("ETS", "ARIMA", "ARFIMA", "ETS", "ARIMA", "ARFIMA", "ETS", "ARIMA", "ARFIMA"),
  RMSE = c('92.8000', '82.1100', '88.0600', '148.0100', '145.1000', '112.2200', '158.0100', '275.0300', '157.4900'),
  MAE = c('35.8000', '34.8900', '75.4400', '72.8100', '80.7100', '92.8500', '96.3100', '152.0200', '141.0200'),
  `MAPE %` = c('4.8600', '5.4500', '5.1700', '9.4900', '12.4200', '8.6200', '15.8100', '19.5000', '14.2300'),
  `Horizonte (días)` = c(5, 5, 5, 15, 15, 15, 30, 30, 30)
)

# Print the data frame


knitr::kable(ras,digits = 4,
        caption = "Métricas modelos paramétricos en pronostico con validación cruzada de precio de cierre de ETH/USD. Fuente: Elaboración Propia", caption.short="Métricas modelos paramétricos en pronostico con validación cruzada de precio de cierre de ETH/USD."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)


```

En el análisis de las métricas obtenidas de los conjuntos de validación cruzada no es posible detectar un método de pronóstico superior en ninguno de los horizontes planteados ya que no hay homogeneidad de resultados en base a las métricas y métodos empleados. 

En relación a los criterios de información AICc, AIC y BIC el método con los menores valores es el ARIMA con valores de AIC=5324, AICc=5224 y BIC=5291. ARFIMA con valores de los criterios de información AICc=21064.43, AIC=21064.63 y BIC=21103.20. ETS presenta valores de AICc=24683.53, AIC=24683.60.

Al realizar los tests de Diebold-Mariano para comparar de a pares la precisión de los métodos no se observan diferencias significativas a un nivel $\alpha=0.05$ para las comparaciones realizadas en ninguno de los horizontes de pronóstico.

Por otro lado, el test de Mincer-Zarnowsky evaluado en cada uno de los métodos entre los valores predichos (valores medios condicionados) y los actuales (valores reales de la serie temporal) para cada uno de los horizontes de pronóstico no presenta resultados significativos con un nivel $alpha=0.05$ en ningún caso planteando como intercepto el valor original de la serie en el cierre de cotización de 01/01/2022.

En relación a los residuos de los modelos el principal aspecto a destacar es la leptocurtosis de los mismos. El modelo ARFIMA además presenta leve autocorrelación en los residuos. Se presenta una cuestión frecuente en el análisis de series financieras como la leptocurtosis en los residuos de los modelos. 

## Modelos no paramétricos utilizados para predicción de precio de cierre:

En relación a los modelos no paramétricos la comparación sería entre NNETAR y LSTM. En ambos casos se desarrolla una calibración con configuración de los hiperparámetros de dos rondas a lo largo de un conjunto de entrenamiento y testeo con validación cruzada. Se registran las métricas tanto en los horizontes objetivo del trabajo (enero, 2022) como en la etapa de validación cruzada. 

A continuación se presentan las métricas (en los valores medios) observadas para los dos métodos.

```{r,echo=FALSE}
mingo<-as.tibble(read_xlsx('~/resumen_no_parametricos_precio.xlsx'))

knitr::kable(mingo,digits = 4,
        caption = "Métricas modelos no paramétricos utlizados en pronostico con validación cruzada de precio de cierre de ETH/USD. Fuente: Elaboración Propia", caption.short="Métricas modelos no paramétricos utlizados en pronostico con validación cruzada de precio de cierre de ETH/USD."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)


```

En relación a las métricas obtenidas se observa que NNETAR tuvo mejor performance en el pronóstico a 5 días y a 30 días mientras que LSTM tuvo mejores métricas en el pronóstico a 15 días. 

Un aspecto particular en NNETAR es la utilización de semillas aleatorias para establecer el estado inicial de la red. La utilización de diferentes semillas genera pronósticos distintos para los mismos hiper parámetros. Por ello se fijan las semillas para poder obtener los mismos valores en sucesivas corridas.

En relación a LSTM se realizan 100 corridas del modelo y se promedian los resultados como se menciona en el marco empírico del proyecto. 

Otro aspecto a considerar entre estos 2 métodos es el tiempo requerido para calibrar hiper parámetros y generar los pronósticos. En el caso de NNETAR el tiempo necesario para la calibración en el esquema de validación cruzada para cada horizonte es de 6 horas con una computadora personal (equipo AMD Ryzen 5 3400G con procesador gráfico Radeon Vega 3.70 GHz y 16 GB de memoria RAM) y aproximadamente 15 minutos para la calibración final y el pronóstico con los hiper parámetros ya configurados. En el caso de LSTM cada entrenamiento de la serie para cada una de las configuraciones de la grilla de hiperparámetros toma un tiempo aproximado de 2 horas con lo cual el tiempo total de calibración y pronóstico con LSTM ronda las 48 horas de cálculo. No se configura en el presente trabajo el procesamiento en paralelo (utilizando todos los núcleos disponibles del sistema) ni se dispone de una placa procesadora gráfica de la potencia suficiente para reducir el tiempo de las operaciones con los paquetes Keras y Tensorflow especialmente en LSTM. Este aspecto resulta importante a la hora de plantear la utilización sistemática de los métodos de pronóstico descriptos en relación con los recursos necesarios para instalar un servidor acorde o trabajar con computación en la nube.

## Comparación de métricas en horizonte objetivo para precio de cierre:

Aquí se presenta el resumen en conjunto de las métricas, de modelos paramétricos y no paramétricos en los horizontes de 5 , 15 y 30 días para el precio de cierre en la tabla a continuacion: 

```{r,echo=FALSE}
martin<-as.tibble(read_xlsx('~/tabla_metricas_precios_cierre.xlsx'))

knitr::kable(martin,digits = 4,
        caption = "Métricas en horizontes estudiados para pronóstico de precio de cierre de ETH/USD. Fuente: Elaboración Propia", caption.short="Métricas en horizontes estudiados para pronóstico de precio de cierre de ETH/USD."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)


```

En el horizonte de 5 días (desde el 01/01 al 05/01 de 2022) el método de pronóstico que presenta mejores métricas es el ETS de acuerdo a lo que se realiza en el marco empírico. Dentro de los no paramétricos el NNETAR es el más preciso. Si se considera el horizonte de 15 días (desde el 01/01 al 15/01 de 2022) el modelo ARFIMA tiene mejores métricas y el LSTM presenta mejores métricas que el NNETAR entre los no paramétricos. Para el horizonte de tiempo más largo, de 30 días (desde 01/01/2022 al 30/01/2022) los métodos no paramétricos estudiados tienen mejores métricas. 

## Métodos empleados para la evaluación de la volatilidad.

En este punto se presentan los resultados de los modelos estudiados para el pronóstico de la volatilidad de la serie. 

Como primera característica desarrollada tanto en el marco teórico como en las pruebas con los modelos la volatilidad se manifiesta como una variable no observable directamente. Tsay (2010) y Poon (2005), autores especializados en series de tiempo financieras entre otros, remarcan este hecho como un condicionante a la hora de la evaluación de los pronósticos por el ruido que las estimaciones de la volatilidad real presentan en los valores actuales contra los que se comparan los pronósticos. En el caso de los modelos ARCH(4),
GARCH(1,1) y EGARCH(1,1) se trabaja con los residuales de los retornos logarítmicos al cuadrado para el modelo de la volatilidad y es por esto que se debe tener cautela en la comparación. 

Una característica común desarrollada en el marco empírico es el uso de diferentes funciones de densidad condicional para las innovaciones. Se decide utilizar en los 3 modelos distribución Gaussiana. Se presentó en todos los casos residuos leptocúrticos en los modelos de varianza condicional, situación que resulta común entre series financieras de acuerdo a lo expuesto por Poon (2005).

En relación a los criterios de información de Akaike, Bayesiano, Shibata y Hannan-Quinn el modelo ARCH(4) resulta ser más apropiado que el GARCH(1,1) y el EGARCH(1,1) en relación al modelado de la volatilidad de la serie de retornos logarítmicos de ETH/USD. Los valores registrados de los criterios de información para los modelos se muestran en la siguiente tabla a continuación:

```{r,echo=F}
ramon<-as.tibble(read_xlsx('~/criterios_informacion_volatilidad.xlsx'))

knitr::kable(ramon,digits = 4,
        caption = "Criterios de información de modelos ARCH(4),GARCH(1,1) y EGARCH(1,1) aplicados al modelo de volatilidad de retornos logarítmicos de ETH/USD. Fuente: Elaboración Propia.",caption.short="Criterios de información de modelos ARCH(4),GARCH(1,1) y EGARCH(1,1) aplicados al modelo de volatilidad de retornos logarítmicos de ETH/USD."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)



```

En relación al test de Mincer-Zarnowitz el modelo ARCH(4) tiene una ecuación en el modelo lineal que presenta un coeficiente significativo a un nivel ($\alpha=0.05$) tanto para el intercepto como para la pendiente. GARCH(1,1) y EGARCH(1,1) no presentan un componente significativo al 0.05 para la pendiente. Los 3 modelos presentan un intercepto significativo distinto de cero, lo cual indica un sesgo en los modelos respecto a la volatilidad estimada mediante los residuos al cuadrado del modelo de medias. La pendiente significativa en el modelo ARCH tiene un valor absoluto de 0.57, un valor relativamente bajo comparado con el ideal de 1.

En relación al test de Diebold-Mariano las comparaciones de a pares entre los modelos no arrojan resultados significativos a un nivel ($\alpha=0.05$) tanto para las puebas a 2 colas (un modelo con distinta capacidad predictiva que el otro) o a una cola (un modelo es superior a otro) para los pronósticos estimados fuera de la muestra con 300 observaciones disponibles y horizonte deslizante de tiempo igual a un día.

En relación a las métricas obtenidas se presenta a continuación la tabla resumen:

```{r,echo=F}

kiko<-as.tibble(read_xlsx('~/metricas_resumen_volatilidad.xlsx'))

knitr::kable(kiko,digits = 4,
        caption = "Métricas de pronóstico de volatilidad  1 día fuera de la muestra (n=300). Fuente: Elaboración propia.", caption.short="Métricas de pronóstico de volatilidad  1 día fuera de la muestra (n=300)."
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 9)


```

En relación a las métricas calculadas se observa que el modelo GARCH(1,1) presenta mejores métricas en comparación con ARCH(4) y EGARCH(1,1).





```{r,include=FALSE,eval=FALSE}

actual_arima_30d
forecast_arfima_30_d$mean

mz_arfima_30_d<-lm(forecast_arfima_30_d$mean~actual_arima_30d)
summary(mz_arfima_30_d)

mz_arfima_15_d<-lm(forecast_arfima_15_d$mean~actual_arima_15d)
summary(mz_arfima_15_d)

mz_arfima_5_d<-lm(forecast_arfima_5d$mean~actual_arima)
summary(mz_arfima_5_d)
car::linearHypothesis(mz_arfima_30_d, c("(Intercept) = 3769", "actual_arima_30d = 1"))

Mincer-Zarnowsky:
predicted_arima_5<-forecast_arima_5_d$.mean
predicted_arima_15<-forecast_arima_15_d$.mean
predicted_arima_30<-forecast_arima_30_d$.mean
actual_arima_5<-ethTEST$close[1827:1831]
actual_arima_15<-ethTEST$close[1827:1841]
actual_arima_30<-ethTEST$close[1827:1856]

mz_arima_30<-lm(predicted_arima_30~actual_arima_30)
summary(mz_arima_30)

mz_arima_15<-lm(predicted_arima_15~actual_arima_15)
summary(mz_arima_15)

mz_arima_5<-lm(predicted_arima_5~actual_arima_5)
summary(mz_arima_5)
plot(mz_arima_30)

Mincer-Zarnowsky
mz_ets_30<-lm(pronostico_ets_30$.mean~actual_ets_30)
car::linearHypothesis(mz_ets_30, c("(Intercept) = 0", "actual_ets_30 = 1"))
summary(mz_ets_30)

mz_ets_15<-lm(pronostico_ets_15$.mean~actual_ets_15)
car::linearHypothesis(mz_ets_15, c("(Intercept) = 0", "actual_ets_15 = 1"))
summary(mz_ets_15)

mz_ets_5<-lm(pronostico_ets_5$.mean~actual_ets_5)
car::linearHypothesis(mz_ets_5, c("(Intercept) = 0", "actual_ets_5 = 1"))
summary(mz_ets_5)

error_ets_30<-pronostico_ets_30$.mean-actual_ets_30
error_arima_30<-predicted_arima_30-actual_arima_30
error_arfima_30<-as.vector(forecast_arfima_30_d$mean-actual_arima_30d)


dm_test_ets_arima<-forecast::dm.test(error_ets_30,error_arima_30,h=28)

dm_test_ets_arfima<-dm.test(error_ets_30,error_arfima_30,h=28,"greater")

dm_test_arima_arfima<-dm.test(error_arima_30,error_arfima_30,h=28)

dm.test(e1=error_arfima_30,e2=error_arima_30,h = 30)

error_ets_5<-pronostico_ets_5$.mean-actual_ets_5
error_arima_5<-predicted_arima_5-actual_arima_5
error_arfima_5<-as.vector(forecast_arfima_5d$mean)-actual_arima_5

dm.test(error_ets_5,error_arima_5,h=4,varestimator = "bartlett",power = 1)
dm.test(error_ets_5,error_arfima_5,h=4)
dm.test(error_arima_5,error_arfima_5,h=4)

error_ets_15<-pronostico_ets_15$.mean-actual_ets_15
error_arima_15<-predicted_arima_15-actual_ets_15
error_arfima_15<-as.vector(forecast_arfima_15_d$mean)-actual_ets_15

dm.test(error_arima_15,error_ets_15,h=13)
dm.test(error_arfima_15,error_ets_15,h=13)
dm.test(error_arima_15,error_arfima_15,h=13)



```


\newpage
# 8-Futuras Investigaciones:

El presente trabajo de tesis permite establecer potenciales investigaciones futuras que profundizan los aspectos estudiados y desarrollan nuevas oportunidades de aplicación de los pronósticos de precio de cierre y volatilidad en series de tiempo con marcada volatilidad. 

En referencia a la profundización de conocimientos teóricos la principal cuestión a abordar en futuras investigaciones es el supuesto de mercado ineficiente en las criptomonedas en general. Sobre este tema en el presente trabajo se asume que el mercado presenta oportunidades en los que se dan ineficiencias que permiten no considerar a los precios de cierre como un paseo aleatorio y desarrollar modelos de pronóstico para precios de cierre. 

Otra investigación futura se basa en la extensión a múltiples variables de los modelos aplicados, principalmente como instrumento de apoyo para el armado de carteras de inversión. 

En relación a los modelos aplicados, una evolución lógica es el planteo de ensambles de modelos que permitan mejorar la performance de predicción. En este aspecto resulta muy importante contar con un marco teórico adecuado para el desarrollo. Otra posible linea de investigación es el trabajo con variables externas para captar información adicional sobre eventos puntuales que tengan incidencia sobre los precios y la volatilidad. 

Tomando como punto de partida este trabajo puede plantearse una aplicación académica sobre series temporales con el tipo de características de extrema volatilidad que se tratan en esta tesis, como por ejemplo un curso o presentación de soporte en alguna materia específica. 

Con un sentido vinculado a la operatoria de mercado una potencial aplicación de parte del código realizado para los pronósticos tanto de precio de cierre como de volatilidad puede ser empleado en la confección de programas automáticos de compra y venta de criptomonedas, específicamente para la construcción de estrategias.





\newpage
# 9-Referencias:

Alizadeh, S., M.W. Brandt and F.X. Diebold (2002) Range-based estimation of stochastic
volatility models, Journal of Finance, 57, 3, 1047–1092.

Andersen, T.G., T. Bollerslev and S. Lange (1999) Forecasting financial market volatility:
Sample frequency vis-`a-vis forecast horizon, Journal of Empirical Finance, 6, 5,
457–477.

Beckers, S, (1993) Variances of security price returns based on high, low and closing
prices, Journal of Business, 56, 97–112.

Bollen, B., and B., Inder (2002) Estimating daily volatility in financial markets utilizing
intraday data, Journal of Empirical Finance, 9, 551–562.



Bollerslev, Tim (1986). Generalized Autoregressive Conditional Heteroskedasticity. Journal of Econometrics, 31, 307-327. Disponible en: https://public.econ.duke.edu/~boller/Published_Papers/joe_86.pdf


Box, G. E. P., G. M. Jenkins and G. C. Reinsel (1994). Time series analysis: forecasting and control. Prentice-Hall, Englewood Cliffs, New Jersey, 3rd ed. [6, 36, 69, 141, 142, 167, 168, 215, 231]


Brockwell, P. J. and R. A. Davis (1991). Time series: theory and methods. Springer, Berlin Heidelberg New York, 2nd ed. [37]


Bukhari et. al. (2020). Fractional Neuro-Sequential ARFIMA-LSTM for Financial Market Forecasting. IEEE.


Cheung, A. W. K., E. Roca, and J. J. Su (2015). Crypto-currency bubbles: an application of the Phillips, Shi and Yu (2013) methodology on Mt. Gox bitcoin prices. Applied Economics 47 (23), 2348–2358.


Corbet, Lucey, Yarovna (2017). Datestamping the Bitcoin and Ethereum Bubbles.Finance Research Letters.


Diba, B. T. and H. I. Grossman (1988). Explosive rational bubbles in stock prices? The American Economic Review 78 (3), 520–530.


Dickey, D. A., & Fuller, W. A. (1979). Distribution of the Estimators for Autoregressive Time Series with a Unit Root. Journal of the American Statistical Association, 74, 427-431.
https://doi.org/10.1080/01621459.1979.10482531

Diebold, F.X., and R.S. Mariano (1995) Comparing predictive accuracy, Journal of
Business and Economic Statistics, 13, 253–263.

Diebold, F.X., A. Hickman, A. Inoue and T. Schuermann (1998) Scale models, RISK
Magazine, 11, 104–107.


Ding, Z., C.W.J. Granger and R.F. Engle (1993) A long memory property of stock market
returns and a new model, Journal of Empirical Finance, 1, 83–106.


Donier, J. and J. P. Bouchaud (2015). Why do markets crash? Bitcoin data offers unprecedented insights. PLoS ONE 10 (10). 

Ederington, L.H., andW. Guan (2000a) Forecasting volatility,Working paper, University
of Oklahoma.

Ederington, L.H., and W. Guan (2000b) Measuring implied volatility: Is an average
better? Working paper, University of Oklahoma.

Enders, Walter (2015). Applied Econometric Time Series. John Wiley and Sons.

Engle R (1982).Autorregresive Conditional Heteroscedasticity With Estimates of the Variance of United Kingdom Inflations. Econometrica,50, 987-1007.

Fama E.F (1970). Efficient capital markets: A review of theory and empirical work. The Journal of Finance, 25, 383–417.

Fama,E.F.(1991). Efficient capital markets: II. The Journal of Finance, 46, 1575–1617.

Fondo Monetario Internacional (2022). Dinero Digital.
https://www.imf.org/es/Publications/fandd/issues/2022/09/Digital-Money-101-explainer


Fuller, W.A. (1996): Introduction to Statistical Time Series (2nd Ed.). New York: John Wiley 

Garman, M.B., and M.J. Klass (1980) On the estimation of security price volatilities
from historical data, Journal of Business, 53, 1, 67–78.

Geweke, J. y S. Porter-Hudak (1983) “Estimation and application of long memory time series models”, Journal of Time 
Series Analysis 4, pp. 221-238. 


Granger, C.W.J. (1980) “Long memory relationships and the aggregation of dynamic models”, Journal of Econometrics 14, pp. 227-238. 


Granger, C.W.J. y R. Joyeux (1980) “An introduction to long-memory time series models and fractional differencing”, Journal of Time Series Analysis 1, pp. 15-29. 


Gregoriu (2015). The Handbook of High Frequency Trading. Elsevier
Haubrich, J.G. y A.W. Lo (1989) “The sources and nature of long-term memory in the business cycle”, NBER Working Paper 2951. 

Hansen, L.P., and R.J. Hodrick (1980) Forward exchange rates as optimal predictors
of future spot rates: An econometric analysis, Journal of Political Economy, 88,
829–853.

Holt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted averages (O.N.R. Memorandum No. 52). Carnegie Institute of Technology, Pittsburgh USA.
 

Hosking, J.R.M. (1981) “Fractional differencing”, Biometrika 68, pp. 165-176.


Hyndman et al.(2008). Forecasting with Exponential Smoothing. Springer
Kroll, J., I. Davey, and E. Felten (2013). The Economics of Bitcoin Mining, or Bitcoin in the Presence of Adversaries. The Twelfth Workshop on the Economics of Information Security (WEIS 2013) (WEIS), 1–21.


Hyndman, R. J., & Khandakar, Y. (2008). Automatic time series forecasting: The forecast package for R. Journal of Statistical Software, 27(1), 1–22. https://doi.org/10.18637/jss.v027.i03 


Hyndman, R y Athanasopoulos, G (2018). Forecasting:principles and practice, 2nd edition.
OTexts: Melbourne, Australia,


Kwiatkowski, D., Phillips, P. C. B., Schmidt, P., & Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root: How sure are we that economic time series have a unit root? Journal of Econometrics, 54(1-3), 159–178. https://doi.org/10.1016/0304-4076(92)90104-Y


G. M. Ljung; G. E. P. Box (1978). "On a Measure of a Lack of Fit in Time Series Models". Biometrika. 65 (2): 297–303. doi:10.1093/biomet/65.2.297


Mensi Whalid, Khamis Hamed Al-Yahyaee , Sang Hoon Kang (2018). Structural breaks and double long memory of cryptocurrency prices: A comparative analysis from Bitcoin and Ethereum. Finance Research Letters.

Lo, A.W. (1991). “Long-term memory in stock market prices”, Econometrica 59, pp. 1279-1313. 

Lopez, J.A. (2001) Evaluating the predictive accuracy of volatility models, Journal of
Forecasting, 20, 2, 87–109.

McKenzieM.D. (1999) Power transformation and forecasting the magnitude of exchange
rate changes, International Journal of Forecasting, 15, 49–55.

Meghnad Desai, Yahia Said (2004). Global Governance and Financial Crises. Routledge

Nakamoto(2008) S., Bitcoin: A Peer-to-Peer Electronic Cash System.ReadLiberty.org

Narayan, P.K., Liu, R.(2013). New Evidence on the Weak-Form Efficient Market Hypothesis. Working Paper. Centre for Financial Econometrics, Deakin University

Padinger, G (25 de julio 2022).¿Por qué está en crisis la economía de Argentina y cómo se llegó a esta situación? https://cnnespanol.cnn.com/2022/07/25/explainer-crisis-economia-argentina-orix/

Parkinson, M. (1980) The extreme value method for estimating the variance of the rate
of return, Journal of Business, 53, 61–65

Peña, D (2010). Análisis de series temporales. Alianza Editorial.

Perez, A, Ruiz E (2001). Modelos de memoria larga para series económicas y financieras. Documento de Trabajo. Serie de Estadística y Econometría. https://www.researchgate.net/publication/26420360_Modelos_de_memoria_larga_para_series_economicas_y_financieras

Pichl Lucas, Cheoljun Eom, Enrico Scalas, and Taisei Kaizoji (2020). Financial Innovations and Blockchain Applications: New Digital Paradigms in Global Cybersociety. Springer.


Phillips, P. C., S. Shi, and J. Yu (2015). Testing for multiple bubbles: Historical episodes of exuberance and collapse in the S&P 500. International Economic Review 56 (4), 1043–1078.

Poon, S. H (2005). A practical guide to forecasting financial market volatility. Wiley 2005. 

Real Academia Española (2024). Diccionario de la lengua española (23.ª edición).

Rogers, L.C.G., and S.E. Satchell (1991) Estimating variance from high, low and closing
prices, Annals of Applied Probability, 1, 504–512

Rogers, L.C.G., S.E. Satchell and Y. Yoon (1994) Estimating the volatility of stock
prices: A comparison of methods that use high and low prices, Applied Financial
Economics, 4, 3, 241–248.

Sowell, F. (1990) “The fractional unit root distribution”, Econometrica 58, pp. 495-505.  


Taylor, J. W. (2003a). Exponential smoothing with a damped multiplicative trend. International Journal of Forecasting, 19, 715–725. [12, 64]

Taylor, Stephen (1986). Modelling financial time series. John Wiley & Sons, Chichester.

Tsay, Ruey S (2010). Analysis of Financial time series third edition. John Wiley & Sons.


Urquhart, Andrew, (August 24, 2016).The Inefficiency of Bitcoin. Available at SSRN: https://ssrn.com/abstract=2828745

West, K.D., (1996) Asymptotic inference about predictive ability, Econometrica, 64,
1067–1084.

West, K.D., and D. Cho (1995) The predictive ability of several models of exchange rate
volatility, Journal of Econometrics, 69, 2, 367–391.

West, K.D., H.J. Edison and D. Cho (1993) A utility based comparison of some
methods of exchange rate volatility, Journal of International Economics, 35, 1–2,
23–45.

West, K.D., and M. McCracken (1998) Regression based tests of predictive ability,
International Economic Review, 39, 817–840.

Wiggins, J.B. (1987) Option values under stochastic volatility: Theory and empirical
estimates, Journal of Financial Economics, 19, 351–372.

Winters, P. R. (1960). Forecasting sales by exponentially weighted moving averages. Management Science, 6(3), 324–342. 

Yuanyuan Zhang , Jeffrey Chu , Stephen Chan , Brandon Chan (2019). The generalised hyperbolic distribution and its subclass in the analysis of a new era of cryptocurrencies: Ethereum and its financial risk. Elsevier.

\newpage
# 10- Apéndice:

En esta sección se presentan los modelos aplicados y gráficas complementarias al marco empírico.

## ETS:

Modelo:

ETS(M,A,N) 
  Parámetros de suavizado:
    alpha = 0.9546 
    beta  = 0.0023 

  Estados Iniciales:
    l[0]      b[0]
 7.8559 0.2812

  $\sigma^2$:  0.0032




```{r,echo=F,message=F,warning=F}
forecast_ets_5_d<-fit %>%
  forecast(h = "5 days")

forecast_ets_15_d<-fit %>%
  forecast(h = "15 days")

forecast_ets_30_d<-fit %>%
  forecast(h = "30 days")

botija_5<-as.tibble(forecast_ets_5_d) 

botija_5$.mean<-sprintf("%.4f",botija_5$.mean)



knitr::kable(botija_5 %>% select(-.model),digits = 4,
        caption = "Pronóstico precio de Cierre ETH/USD modelo ETS (M,A,N) (h=5 días)",col.names = c("Fecha", "Valores","Media")
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)

botija_15<-as.tibble(forecast_ets_15_d)

botija_15$.mean<-sprintf("%.4f",botija_15$.mean)


knitr::kable(botija_15 %>% select(-.model),digits = 4,
        caption = "Pronóstico precio de Cierre ETH/USD modelo ETS (M,A,N) (h=15 días)",col.names = c("Fecha", "Valores","Media"),
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)

botija_30<-as.tibble(forecast_ets_30_d)

botija_30$.mean<-sprintf("%.4f",botija_30$.mean)


knitr::kable(botija_30 %>% select(-.model),digits = 4,
        caption = "Pronóstico precio de Cierre ETH/USD modelo ETS (M,A,N) (h=30 días)",col.names = c("Fecha", "Valores","Media"),
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)


ets_criterios_info<-data.frame(AIC=c("24683.5000"), AICc=c("24683.5300"), BIC=c("24711.0500"))

knitr::kable(ets_criterios_info,digits = 4,caption="Criterios de informacion modelo ETS alicado a la predicción de ETH/USD. Fuente: Elaboración Propia", caption.short="Criterios de informacion modelo ETS alicado a la predicción de ETH/USD.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)



```

\newpage
## ARIMA:

```{r,echo=FALSE}
seba<-as.tibble(model_arima_fit[[1]][[1]][["fit"]][["par"]])
knitr::kable(seba,digits = 4,
        caption = "Coeficientes modelo ARIMA(1,1,2)(1,0,0)(7). Fuente: Elaboración Propia",caption.short="Coeficientes modelo ARIMA(1,1,2)(1,0,0)(7)",col.names = c("Término", "Valor Estimado", "Error Std", "Estadístico", "Valor p")
          ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)

forecast_arima_5_d<-model_arima_fit  %>%
  forecast(h = "5 days")

forecast_arima_15_d<-model_arima_fit  %>%
  forecast(h = "15 days")

forecast_arima_30_d<-model_arima_fit  %>%
  forecast(h = "30 days")

chimo_5<-as.tibble(forecast_arima_5_d)

chimo_5$.mean<-sprintf("%.4f",chimo_5$.mean)


knitr::kable(chimo_5 %>% select(-.model),digits = 4,
        caption = "Pronóstico precio de Cierre ETH/USD modelo ARIMA(1,1,2)(1,0,0)(7) (h=5 días). Fuente: Elaboración Propia",caption.short="Pronóstico precio de Cierre ETH/USD modelo ARIMA(1,1,2)(1,0,0)(7) (h=5 días)",col.names = c("Fecha", "Valores (transformados)","Media"),
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)

chimo_15<-as.tibble(forecast_arima_15_d)
chimo_15$.mean<-sprintf("%.4f",chimo_15$.mean)


knitr::kable(chimo_15 %>% select(-.model),digits = 4,
        caption = "Pronóstico precio de Cierre ETH/USD modelo ARIMA(1,1,2)(1,0,0)(7) (h=15 días). Fuente: Elaboración Propia",caption.short="Pronóstico precio de Cierre ETH/USD modelo ARIMA(1,1,2)(1,0,0)(7) (h=15 días).",col.names = c("Fecha", "Valores (transformados)","Media"),
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)

chimo_30<-as.tibble(forecast_arima_30_d)
chimo_30$.mean<-sprintf("%.4f",chimo_30$.mean)

knitr::kable(chimo_30 %>% select(-.model),digits = 4,
        caption = "Pronóstico precio de Cierre ETH/USD modelo ARIMA(1,1,2)(1,0,0)(7) (h=30 días). Fuente: Elaboración Propia",caption.short="Pronóstico precio de Cierre ETH/USD modelo ARIMA(1,1,2)(1,0,0)(7) (h=30 días)",col.names = c("Fecha", "Valores (transformados)","Media"),
        ) %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)

```

\newpage
## ARFIMA:


```{r,echo=FALSE}
coef_arfima=read_xlsx('~/coeficientes_arfima.xlsx')

knitr::kable(coef_arfima,digits = 4,caption="Coeficientes modelo ARFIMA utilizado en la predicción de precio de cierre de ETH/USD. Fuente: Elaboración Propia", caption.short="Coeficientes modelo ARFIMA utilizado en la predicción de precio de cierre de ETH/USD.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)




```

\newpage
## ARCH:

```{r,echo=F,message=F,warning=F}
coef_arch_distribuciones=read_xlsx('~/resumen_arch_parametros_distribuciones.xlsx')

knitr::kable(coef_arch_distribuciones,digits = 4,caption = "Coeficientes de modelos ARCH(4) - distintas distribuciones. Fuente: Elaboración propia.", caption.short="Coeficientes de modelos ARCH(4) - distintas distribuciones.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)


```


```{r}

criterios_informacion_arch <- data.frame(
  Akaike = c("-3.0449", "-3.0416","22.4420","22.4410", "-3.2719", "-3.2707", "-3.2618","-3.2617"),
  Bayes = c("-3.0139", "-3.0072","22.4800", "22.4750", "-3.2307", "-3.2328", "-3.2274", "-3.2238"),
  Shibata = c("-3.0450", "-3.0417", "22.4420", "22.4410", "-3.2721", "-3.2708", "-3.2618", "-3.2618"),
  Hannan_Quinn = c("-3.0334", "-3.0288", "22.4560", "22.4530", "-3.2566", "-3.2566", "-3.2490", "-3.2476"),
  `Distribución` = c("normal", "normal sesgada", "error generalizado sesgado", "error generalizado", "hiperbólica generalizada", "normal inversa", "t-student", "t-student sesgada")
)


knitr::kable(criterios_informacion_arch,caption = "Criterios de información modelos ARCH(4) - distintas distribuciones. Fuente: Elaboración propia.", caption.short="Criterios de información modelos ARCH(4) - distintas distribuciones.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)


```

```{r}
test_ljung_arch_distribuciones<-read_xlsx('~/test_lung_residuos_arch_distribuciones.xlsx')

knitr::kable(test_ljung_arch_distribuciones,digits = 4,caption = "Test de Ljung-Box modelos para residuos estandarizados modelos  ARCH(4) - distintas distribuciones condicionales (Ho: No existe autocorrelación). Fuente: Elaboración propia.", caption.short="Test de Ljung-Box modelos para residuos estandarizados modelos  ARCH(4) - distintas distribuciones condicionales.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)

```

```{r}
test_ljung_arch_distribuciones_cuadrado<-read_xlsx('~/test_lung_residuos_cuadrado_arch_distribuciones.xlsx')

knitr::kable(test_ljung_arch_distribuciones_cuadrado,digits = 4,caption = "Test de Ljung-Box modelos para residuos estandarizados al cuadrado modelos  ARCH(4) - distintas distribuciones condicionales (Ho: No existe autocorrelación). Fuente: Elaboración propia.", caption.short="Test de Ljung-Box modelos para residuos estandarizados al cuadrado modelos  ARCH(4) - distintas distribuciones condicionales.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)


```

```{r}
test_arch_distribuciones<-read_xlsx('~/test_arch_distribuciones.xlsx')

test_arch_distribuciones$Forma<-sprintf("%.4f",test_arch_distribuciones$Forma)

test_arch_distribuciones$Scala<-sprintf("%.4f",test_arch_distribuciones$Scala)


knitr::kable(test_arch_distribuciones,digits = 4,caption = "Test ARCH para rezagos superiores, modelos ARCH(4) - distintas distribuciones condicionales (Ho: No Existe efecto ARCH). Fuente: Elaboración propia.", caption.short="Test ARCH para rezagos superiores, modelos  ARCH(4) - distintas distribuciones condicionales.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)



```


```{r}
test_bondad_ajuste_arch <- read_excel("~/tests_bondad_ajuste_arch.xlsx") 


test_bondad_ajuste_arch$Estadístico<-sprintf("%.4f",test_bondad_ajuste_arch$Estadístico)


knitr::kable(test_bondad_ajuste_arch,digits = 4,caption = "Test de bondad de ajuste de residuos, modelo ARCH(4) - dis-
tintas distribuciones condicionales (Ho: ajuste a la distribución de referencia). Fuente: Elaboración propia.", caption.short="Test de bondad de ajuste de residuos, modelo ARCH(4) - distintas distribuciones condicionales (Ho: ajuste a la distribución de referencia).") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)

```

```{r,fig.height=10,fig.width=10,fig.cap="Graficos de ajuste de modelo ARCH(4)",echo=F,warning=F,message=FALSE,include=F,eval=FALSE}

ug_spec<-ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(4, 0), 
submodel = NULL, external.regressors = NULL, variance.targeting = FALSE),mean.model=list(arfima=TRUE),distribution.model = "norm") # no se puede especificar bien el ARIMA o ARFIMA.

ugfit<-ugarchfit(spec=ug_spec,data=aldo1,out.sample = 300)

prueba_arch_30_df<-as.data.frame(prueba_arch_30)

library(xts)

# Convert the 'value' column to a numeric vector
values1 <- as.numeric(prueba_arch_30_df$value)

# Create an xts object with the 'values' vector and the 'date' column as the index
aldo1 <- xts(values1, order.by = as.Date(prueba_arch_30_df$date))

ugfit_GARCH_2<-ugarchfit(spec=ug_spec_GARCH,data=aldo1,out.sample = 300)

ugfit_EGARCH_2<-ugarchfit(spec=ug_spec_EGARCH_2,data=aldo1,out.sample = 300)
plot(ugfit_EGARCH_2,which="all")

ugfit@model

plot(ugfit,which=12)

# Generar el gráfico con plot(ugfit, which = 1)
plot(ugfit, which = "all")




names(ugfit@fit)
ugfit@model$modeldata$index


```

```{r, graffoto13,fig.align='center',fig.height=6,fig.width=6,fig.cap="Gráficos de ajuste modelo ARCH(4). Fuente: Elaboración propia.",fig.scap="Gráficos de ajuste modelo ARCH(4). ",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\graficos-ARCH(4).png")
```



\newpage
## GARCH:

```{r}

coef_garch_distribuciones=read_xlsx('~/resumen_garch_parametros_distribuciones.xlsx')

knitr::kable(coef_garch_distribuciones,digits = 4,caption = "Coeficientes de modelos GARCH(1,1) - distintas distribuciones. Fuente: Elaboración propia.", caption.short="Coeficientes de modelos GARCH(1,1) - distintas distribuciones.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)



```



```{r}

criterios_informacion_garch_distribuciones=read_xlsx('~/criterios_informacion_garch.xlsx')

knitr::kable(criterios_informacion_garch_distribuciones,digits = 4,caption = "Criterios de información modelos GARCH(1,1) - distintas distribuciones. Fuente: Elaboración propia.", caption.short="Criterios de información modelos GARCH(1,1) - distintas distribuciones.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)


```

```{r}
test_ljung_garch_distribuciones<-read_xlsx('~/test_lung_residuos_garch.xlsx')

knitr::kable(test_ljung_garch_distribuciones,digits = 4,caption = "Test de Ljung-Box modelos para residuos estandarizados modelos GARCH(1,1) - distintas distribuciones condicionales (Ho: No existe autocorrelación). Fuente: Elaboración propia.", caption.short="Test de Ljung-Box modelos para residuos estandarizados modelos  GARCH(1,1) - distintas distribuciones condicionales.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)

```

```{r}

test_ljung_garch_cuadrado_distribuciones<-read_xlsx('~/test_lung_residuos_cuadrado_garch.xlsx')

knitr::kable(test_ljung_garch_cuadrado_distribuciones,digits = 4,caption = "Test de Ljung-Box modelos para residuos estandarizados al cuadrado modelos GARCH(1,1) - distintas distribuciones condicionales (Ho: No existe autocorrelación). Fuente: Elaboración propia.", caption.short="Test de Ljung-Box modelos para residuos estandarizados al cuadrado modelos  GARCH(1,1) - distintas distribuciones condicionales.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)


```

```{r}
test_garch_distribuciones<-read_xlsx('~/tests_arch_modelo_garch.xlsx')

test_garch_distribuciones$Forma<-sprintf("%.4f",test_garch_distribuciones$Forma)

test_garch_distribuciones$Escala<-sprintf("%.4f",test_garch_distribuciones$Escala)



knitr::kable(test_garch_distribuciones,digits = 4,caption = "Test ARCH para rezagos superiores, modelos GARCH(1,1) - distintas distribuciones condicionales (Ho: No Existe efecto ARCH). Fuente: Elaboración propia.", caption.short="Test ARCH para rezagos superiores, modelos  GARCH(1,1) - distintas distribuciones condicionales.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)



```

```{r}
test_bondad_ajuste_garch <- read_excel("~/test_bondad_ajuste_garch.xlsx")


test_bondad_ajuste_garch$Estadístico<-sprintf("%.4f",test_bondad_ajuste_garch$Estadístico)


knitr::kable(test_bondad_ajuste_garch,digits = 4,caption = "Test de bondad de ajuste de residuos, modelo GARCH(1,1) - dis-
tintas distribuciones condicionales (Ho: ajuste a la distribución de referencia). Fuente: Elaboración propia.", caption.short="Test de bondad de ajuste de residuos, modelo GARCH(1,1) - distintas distribuciones condicionales (Ho: ajuste a la distribución de referencia).") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)



```


```{r, graffoto14,fig.align='center',fig.height=6,fig.width=6,fig.cap="Gráficos de ajuste modelo GARCH(1,1). Fuente: Elaboración propia.",fig.scap="Gráficos de ajuste modelo GARCH(1,1). ",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\grafico_resumen_garch.png")
```


\newpage
## EGARCH:

```{r}

coef_egarch_distribuciones=read_xlsx('~/resumen_egarch_parametros_distribuciones.xlsx')

knitr::kable(coef_egarch_distribuciones,digits = 4,caption = "Coeficientes de modelos EGARCH(1,1) - distintas distribuciones. Fuente: Elaboración propia.", caption.short="Coeficientes de modelos EGARCH(1,1) - distintas distribuciones.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)



```


```{r}
criterios_informacion_egarch_distribuciones=read_xlsx('~/criterios_informacion_egarch.xlsx')

knitr::kable(criterios_informacion_egarch_distribuciones,digits = 4,caption = "Criterios de información modelos EGARCH(1,1) - distintas distribuciones. Fuente: Elaboración propia.", caption.short="Criterios de información modelos EGARCH(1,1) - distintas distribuciones.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)
```

```{r}
test_ljung_egarch_distribuciones<-read_xlsx('~/test_lung_residuos_egarch.xlsx')

knitr::kable(test_ljung_egarch_distribuciones,digits = 4,caption = "Test de Ljung-Box modelos para residuos estandarizados modelos EGARCH(1,1) - distintas distribuciones condicionales (Ho: No existe autocorrelación). Fuente: Elaboración propia.", caption.short="Test de Ljung-Box modelos para residuos estandarizados modelos  EGARCH(1,1) - distintas distribuciones condicionales.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)

```

```{r}

test_ljung_egarch_cuadrado_distribuciones<-read_xlsx('~/test_lung_residuos_cuadrado_egarch.xlsx')

knitr::kable(test_ljung_egarch_cuadrado_distribuciones,digits = 4,caption = "Test de Ljung-Box modelos para residuos estandarizados al cuadrado modelos EGARCH(1,1) - distintas distribuciones condicionales (Ho: No existe autocorrelación). Fuente: Elaboración propia.", caption.short="Test de Ljung-Box modelos para residuos estandarizados al cuadrado modelos  EGARCH(1,1) - distintas distribuciones condicionales.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)




```

```{r}
test_egarch_distribuciones<-read_xlsx('~/tests_arch_modelo_egarch.xlsx')

test_egarch_distribuciones$Forma<-sprintf("%.4f",test_egarch_distribuciones$Forma)

test_egarch_distribuciones$Escala<-sprintf("%.4f",test_egarch_distribuciones$Escala)



knitr::kable(test_egarch_distribuciones,digits = 4,caption = "Test ARCH para rezagos superiores, modelos EGARCH(1,1) - distintas distribuciones condicionales (Ho: No Existe efecto ARCH). Fuente: Elaboración propia.", caption.short="Test ARCH para rezagos superiores, modelos  EGARCH(1,1) - distintas distribuciones condicionales.") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)



```

```{r}
test_bondad_ajuste_egarch <- read_excel("~/test_bondad_ajuste_egarch.xlsx")


test_bondad_ajuste_egarch$Estadístico<-sprintf("%.4f",test_bondad_ajuste_egarch$Estadístico)


knitr::kable(test_bondad_ajuste_egarch,digits = 4,caption = "Test de bondad de ajuste de residuos, modelo EGARCH(1,1) - dis-
tintas distribuciones condicionales (Ho: ajuste a la distribución de referencia). Fuente: Elaboración propia.", caption.short="Test de bondad de ajuste de residuos, modelo EGARCH(1,1) - distintas distribuciones condicionales (Ho: ajuste a la distribución de referencia).") %>%
  kableExtra::kable_styling(full_width = F,latex_options = c("hold_position"), font_size = 6)



```

```{r, graffoto15,fig.align='center',fig.height=6,fig.width=6,fig.cap="Gráficos de ajuste modelo EGARCH(1,1). Fuente: Elaboración propia.",fig.scap="Gráficos de ajuste modelo EGARCH(1,1). ",echo=F,message=FALSE}
library(knitr)
include_graphics("C:\\Users\\Sebastian\\Documents\\graficos_ajuste_EGARCH.png")
```





\newpage
## LSTM:

En la gráfica a continuación se presenta un ejemplo de una corrida de datos para LSTM en el pronóstico a 30 días. Se observa estabilidad tanto en la función pérdida como en la raíz del error cuadrático medio a partir de la época 4. 

\begin{center}\includegraphics[width=1\textwidth]{C:\\Users\\Sebastian\\Documents\\ejemplo_corrida_lstm.png}\end{center}
\begin{center}\text{Figura 47: Corrida LSTM para pronóstico a 30 días de ETH/USD. Fuente: Elaboración propia.}\end{center}

